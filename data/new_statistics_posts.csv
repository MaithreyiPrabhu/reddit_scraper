selftext,title,id,sorted_by,num_comments,score,ups,downs
"I'm a postgraduate biology student with somewhat weak knowledge of statistics (but I'm learning!). I'm working on human-wildlife conflict, currently analysing a dataset of responses to a questionnaire with columns as the various questions and each row as an individual respondent. I have demographic variables (age, gen, education level, household size, etc.), attitude as a single, ordinal variable, variables on fishing behaviour (lbs of fish caught, tools used, whether sold or not), and impacts of conflict with wildlife (whether participants had a net destroyed, the number of people participants knew who had been attacked, etc.). In the full survey, there are 252 respondents, however there are many NA's, some of which come from data collection errors (like an interviewer missing a question), from respondents declining to answer a question, or from respondents not being applicable to a question (such as someone who doesn't fish having to skip questions about fishing behaviour and success). 

My research questions are examining whether attitudes can be predicted by fishing behaviour, and whether fishing behaviour predicts attitudes, as per my hypothesis, that fishermen as a group should be targeted for future research into human-wildlife conflict mitigation. As an alternative, if fishing behaviour does not predict attitudes, then we wish to determine whether demographic variables by themselves do. As attitudes are ordinal, we will use ordinal regression. All other response variables are  For a quick summary, the models are as follows:

Attitudes \~ Fishing behaviour

Fishing behaviour \~ demographics

Alt: Attitudes \~ demographics

Removing all NA's leads to a final dataset of 156 variables, which is quite a lot smaller as we have lost 96 lines of data. We have been discussing two possible approaches. As the models I would like to develop utilize different variables, we are considering making subsets of the data without NA's only for the relevant questions, and using the results to inform our conclusion. Any models that need to be compared will use the same data, in order to use AIC. I'm uncertain about this approach, though, as I feel that the models should utilize the same dataset. What is your take on this?

I also wanted to ask about testing for multicolinearity in a dataset with mixed variables. So far I've seen that it's possible to do a chi square test for independence between categorical variables (is the appropriate cutoff value the standard alpha value for signficance testing?) and a correlation test for numerical variables (with a cutoff value of 0.7). However, what is the appropriate test to use between categorical and numerical variables? So far, I've used a quick glm to determine whether there was a significant effect of the categorical variable on the numerical, but I'm not sure this is appropriate and what the correct cutoff value should be. Also, if I decide to take the subset approach, should I test for multicolinearity separately for each subset or just in the global dataset?

Relevant to the first question: 

tl;dr: 

1. Removing all NA's removes a lot of rows of data. Is it better to just subset and remove NA's only for each specific model (if not comparing AIC values) or to just remove all NA's and use one global (but much smaller) dataset? 
2. How to test for multicolinearity in a dataset with mixed categorical and numeric responses? If in Q1, I decide to take the subset approach, should I test for multicolinearity separately for each subset or just in the global dataset?","Advice on the analysis (ordinal and logistic regression) of survey data with mix of categorical and numeric variables, with missing data",9ek6tf,new,0,1,1,0
"Plots in R about 111,303 earthquakes in Mexico since 1980

[https://medium.com/@obedm/43-gr%C3%A1ficas-estad%C3%ADsticas-de-los-sismos-en-m%C3%A9xico-desde-1980-b14b9fc6819](https://medium.com/@obedm/43-gr%C3%A1ficas-estad%C3%ADsticas-de-los-sismos-en-m%C3%A9xico-desde-1980-b14b9fc6819)",Visualizations about Earthquakes in Mexico since 1980,9eixxx,new,0,1,1,0
"I don't mind providing people with recommendations for books to read, online courses to check out, or things that they should know before starting their course. It just feels like we get the exact same requests for resources week in week out, hence why I think making at least some of the more general resources a sticked topic/sidebar link.

Not only would this mean we (hopefully) get fewer posts asking for the exact same thing, but the people who are looking for that information wouldn't have to look far to get what they need. I realise that the people who ask these questions clearly didn't use the search function to find previous posts, but I'm hoping that a quite prominently displayed container of this information would help.

I had a quick look and this has been requested a couple of times before - what's stopping this from being implemented?",Can we have a sidebar or sticked post with links to statistics resources to try and reduce the number of duplicate postings?,9eistt,new,0,2,2,0
"My current understanding is that the curvature (hessian matrix?) of the loss function (SSE) at the function minimum (LSE) is related to the standard deviation of the sampling distribution. For linear regression, this is done analytically, while nonlinear regression requires numerical approximation. However, I'm not sure if this is exactly correct, or how it is that loss function curvature is related back to the sampling distribution standard deviation.

It makes sense to me, because if the curvature is high, that means that the set of parameter estimates which minimizes loss is exceptionally better than the surrounding points in the parameter space, which means that it should have low uncertainty. 

Any help is appreciated, thanks.",Could somebody explain how parameter uncertainty is obtained in linear/nonlinear regression? Preferably in a linear algebra-ish context,9ehw4w,new,0,2,2,0
"**TL;DR:** new psychology study claims to use ML methods on MTurk sample as antidote to non-replicability of psych studies, but there are questionable analysis choices (such as dropping 15% of the data and discretizing their continuous outcome variable into 10 unordered classes), the result they get is a variable importance ranking of attributes driving predictive model fit, which they overinterpret and don't acknowledge a much more obvious driver of their finding. Read on if you want to hear more and discuss.

---

I learned about the recent paper [""Good Things for Those Who Wait: Predictive Modeling Highlights Importance of Delay Discounting for Income Attainment""](https://www.frontiersin.org/articles/10.3389/fpsyg.2018.01545/full) from the Marginal Revolution blog's Friday link round-up. It's an easy open-access read and I encourage you all to give it a skim. I have a lot of concerns about the methodology and interpretation in this paper and want to discuss this here. (Yes, it's a day ending in 'y', so of course there is a questionable social science study out in a high-impact journal which has garnered [a fair amount of media coverage](http://loop-impact.frontiersin.org/impact/article/359023#socialbuzz) and [over 13K views.](http://loop-impact.frontiersin.org/impact/article/359023#totalviews/views))

The authors tout their machine learning approach to data analysis as superior to traditional methods one might use instead. They motivate their work with concerns about multicollinearity that we experience with ""standard correlational and regression analytic approaches"". While that's fair, I am worried that psychology researchers may take away bad advice from this study when making good-faith efforts to address their field's very well-known issues around replication, which the authors specifically mention as motivating their approach to data collection and analysis.

This also provides an anecdote supporting a trend I've noticed: because of ML hype, there are an increasing number of data analysts who have learned about topics like cross-validation and random forests without having adequate statistical training to ground them. The authors write things like, ""we were able to model continuous, categorical, and dichotomous variables simultaneously, and subsequently compare their relative importance for predicting income; this would not have been possible using more traditional methods."" I don't know what strawman they have in mind, but there's nothing groundbreaking about modeling continuous and categorical features simultaneously. Additionally, I see lots of ""garden of forking paths"" analysis choices that would hinder replication, as many decisions are made on the whole data before the training/test splits, which makes the whole holdout/CV aspect of the paper seem like a lot of show for nothing.

The topic is ""a simple yet essential question: why do some individuals make more money than others?"" They cite prior work around some sociodemographic factors as well as height and the infamous Marshmallow Test around delay discounting (which I should note [has not held up well](https://www.theatlantic.com/family/archive/2018/06/marshmallow-test/561779/) in recent replications, which they do not cite). It's not totally clear what the authors' scientific questions or hypotheses are, but they seem to think it is interesting to figure out which of the basic sociodemographic and discount delay behavioral attributes they survey MTurkers about are most predictive of income and rank them.

Here's the setup:

* **Data collection:** the study's data come from an Amazon MTurk sample of 3000 Americans aged 25-65 who answered some questions about delayed gratification indifference points. Like: would you rather have $500 now or $1000 in 6 months? If you said $500 now, then would you rather have $250 now or $1000 in 6 months? If you said $1000 in 6 months, then would you rather have $375 now or $1000 in 6 months? etc. splitting the boundaries iterating until you have no preference. The indifference tasks were answered for time frames of 1 day, 1 week, 1 month, 6 months, and 1 year (variables of primary interest). The MTurkers also answered questions about income (the dependent outcome of interest), age, sex, race, ethnicity, height, education level, zip code, and occupational group.

* **Data cleaning:** the authors perform aggressive ""outlier"" handling that removes 15% of their data, resulting in n=2564 respondents for analysis. They drop all students and any participant who completed the delay discounting questions in under 2 SDs below the mean task time. The fast-completion removal rule is a red flag because subjects who chose the ""$1000 in the future"" option at the outset would have finished the task much faster than others, and so *the dropped outliers procedure is likely strongly associated with the delay discounting responses* and would bias the data. The authors also say they applied ""extreme value detection and distribution-based inspections"" to other continuous covariates without clarifying further. To me, these look like forking path decisions that may substantially affect the results, and all of this is done before holding out data.

* **Outcome discretization:** This part is the biggest eyebrow raiser: they take the continuous self-reported income outcome variable (ranges from $10K to $235K) and discretize it into 10 buckets containing the same number of (non-outlier) subjects. This converts their analysis into a 10-level classification task with performance measured by AUC. In discretizing, these income groups become *unordered* labels and thus the authors get much less information out of their data than if they handled this as a regression problem by leaving income as a numeric variable. They claim: ""This conversion also yielded a more compact representation, and thus, less complexity"", to which I say *absolutely NOT.* The loss functions for their ML models treat mis-classifying someone who actually makes $23K a year in the $24.5K-$35.2K group equally as erroneously as mis-classifying them in the $158.4K-$235K group! This transformation is not only statistically wasteful, it leaves their models uninterpretable as a side effect: they can't describe the direction of the relationship between discounting and income or speak to model fit in an understandable way (like RMSE). It's likely that their predictive models would not be robust to different choices for number of outcome levels or cut points.

* **Model fitting:** They messed with income because they are motivated by trying to cram the data into a particular ML framework without being aware of the trade-offs. The authors justify this with: ""Some of the criteria that we used in our feature selection method are more compatible with categorical features. Further, reported incomes were not evenly distributed."" This is the method driving the transformation at the expense of the science and they do not say *why* unevenly distributed incomes would be an issue (hint: they aren't). They run SVMs, neural networks, and random forests on a 90% subset of the data with 10-fold CV, and as part of this process, they calculate feature importance by removing variables one-by-one to rank their contribution in predicting the income labels.

* **Results:** The primary output is [a ranking of which variables they considered in terms of feature importance,](https://www.frontiersin.org/files/Articles/359023/fpsyg-09-01545-HTML/image_m/fpsyg-09-01545-t002.jpg) and the underwhelming conclusion: ""Interestingly, delay discounting was more predictive than age, race, ethnicity, and height"" (but that's just 1 year delay discounting, and occupation, education, zip code, and gender are more important). Instead of reporting effect sizes or showing a marginal GAM plot, they have just moved the target to something more stable (importance ranks) but less interesting. To me, this isn't a solution to multicollinearity or non-linearity, it's just replacing a thing we care about with something much less useful. They can't even speak to *how* delayed discounting predicts income to assess whether the models even make scientific sense. For all we know from these results, preferring $1000 in the future over $X now could be *negatively* associated with income after accounting for other attributes.

* **Causality:** They make a brief disclaimer that results are associational and not causal, but they don't mention what seems to me like a simple and obvious explanation for their finding that delayed discounting helps predict income, which is that *income causes delayed discounting* rather than *delayed discounting causes income.* The authors write: ""we speculate that this relationship [aside: whose sign they haven't established!] may be a consequence of the correlation between higher discounting and other undesirable life choices. ... In this way, one possibility is that delay discounting signals a cascade of negative behaviors that derail individuals from pursuing education and may ultimately preclude entry into certain lucrative occupational niches."" I'm no psychologist, but it seems really obvious that someone who makes $20K probably *is* more likely to prefer an immediate windfall of $500 compared to someone who makes $150K who can afford to wait a year to see the full $1000...because they're poor and $500 now may go far in paying for today's expenses.",you can't fix bad psych methods with bad ML methods: comments on a recent paper,9egem6,new,20,84,84,0
"Say you have two logistic regression models 1 and 2. 2 performs better than 1, but you have no idea why, because you don't know the exact relationship its variables have with the outcome.

1 on the other hand you know perfectly well how the variables relate to the outcome, but it doesn't have as great of accuracy.

I understand why people would be very hesitant to choose 2, because if it suddenly stops performing on new data you would have no explanation as to why. Whereas 1 you might be able to investigate what's going on and figure it out. But are there any more specific theoretical / philosophical reasons I'm missing?",Why do variables in a model need theoretical justification?,9efr35,new,7,3,3,0
,Is it possible to find a career in statistics after a psychology degree? ( Would I need a masters degree or any extra qualification ),9ef67p,new,5,1,1,0
"Hi, sorry  for the noob question.  I  want  to know  for  what  bootstrap  is  useful for.  I  have  understood  that  it gives  you a better understanding of some  stimators  (such as  the mean, the  variance, and so on)

Let's  suppose  I have an  aritmetic  mean calculated  by a dataset, then  why should I  use bootstrap method?  Where is the need  to  use it?

I have read  a little  bit  about it.  In this  example

[https://www.statmethods.net/advstats/bootstrapping.html](https://www.statmethods.net/advstats/bootstrapping.html)

generates the bootstrapped 95% confidence interval for R-squared in the  linear regression of miles per gallon (mpg) on car weight (wt) and  displacement (disp). The data source is mtcars. The bootstrapped  confidence interval is based on 1000 replications

Here  the  code:

&#x200B;

# Bootstrap 95% CI for R-Squared  
library(boot)  
# function to obtain R-Squared from the data   
rsq <- function(formula, data, indices)  		          {  
  d <- data\[indices,\] # allows boot to select sample   
  fit <- lm(formula, data=d)  
  return(summary(fit)$r.square)  
}  		            
# bootstrapping with 1000 replications   
results <- boot(data=mtcars, statistic=rsq,   
R=1000, formula=mpg\~wt+disp)  
 

  # view results  
  results    
  plot(results)  
 

 # get 95% confidence interval   
 boot.ci(results, type=""bca"")

&#x200B;

Then  the  plot:

[https://www.statmethods.net/advstats/images/bootstrap.png](https://www.statmethods.net/advstats/images/bootstrap.png)

&#x200B;

My question is:  What  does this  plot  mean?  Does  it mean  that the true  value, the 95% confidence  values  of the R-squared  over 1000  resamplings  made  with the  bootstrap do they  fall in the 0,77 - 080  values?

&#x200B;

So are these  ( 0,77 - 080 )  the  truest  values  of the  R-squared  with a  confidence of 95% ?

&#x200B;

Am  I correct?

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;",Could you help me with bootstrapping?,9edsi0,new,2,3,3,0
"Hey everyone!

I'm a Politics major who is interested in learning how to use Stata and R. However, as a complete beginner, I wanted to ask around and see what is the best way to start. Therefore, what would you recommend for a beginner who is eager to learn?

Also, do you have any useful tips/hints/ideas as how should I go about this and what common mistakes to avoid?",Newcomer interested in learning Stata and R,9ecwmo,new,9,5,5,0
"Hello All,

&#x200B;

I am not going to lie, help is urgently needed as depending on a couple questions is my masters application. 

I've attempted solving it for ages now (below you'll find my method which I also found in a textbook)

&#x200B;

I am currently working on this question:

&#x200B;

*""While a particle detector is recording the passage of a particle, it is “dead” — i.e., insensitive to the passage of further particles. This can become a problem when rates are high. Assume that a counter has a dead time of 200 ns, and is exposed to a beam of 1 × 10\^6 particles per second, so the mean number of particles hitting the detector in a 200 ns window is $\\mu$ = 0.2. Find the average number of particles recorded in a 200-ns period (hint: no particles are recorded if zero pass through; otherwise one particle is recorded), and hence the efficiency of the counter.""*

&#x200B;

I've solved using the following method:

&#x200B;

[http://www.pa.uky.edu/\~kwng/phy335/hw/P2\_15.pdf](http://www.pa.uky.edu/~kwng/phy335/hw/P2_15.pdf)

&#x200B;

However my professor said it was incorrect.

&#x200B;

Now I have another method that I think might be correct, however I am no longer sure as I was certain the original method was correct

&#x200B;

[https://imgur.com/a/x4aDhnu](https://imgur.com/a/x4aDhnu)

&#x200B;

What do you guys think? 

&#x200B;

&#x200B;

&#x200B;",Poisson Distribution - particle detector,9eciwa,new,1,2,2,0
"Hello, sorry for  the noob  question.  If   I have  a  dataset   in which  there are some dependent variables  I want to  outcome,  and  I want to know  the relationship  of the  dependent variable  with the idipendent variables, and also the relationship  between both the dependent  variables.   Would it be  better  to use   a   MANOVA, or   to make  multiple singular  regressions  based  on my need?

&#x200B;

Basically my question is:   when   and   why do you decide  to use a  Manova instead  of  running singular separated  multiple regressions in order to  find  the outcome of  many dependent variables?",MANOVA or multiple regressions?,9ec9ms,new,2,1,1,0
"Hello,

&#x200B;

I am a fresh graduate in statistics, in particular in statistics for sample surveys. However, what attracts me the most is the part of data modelling, in particular the branch of statistical learning (I read ""An Introduction to Statistical Learning"" by Springer Texts). Unfortunately my degree program has provided me with no purely computer science skills (only one course about R and one about VBA), so the advice I ask you is the following: what basic computer science concepts should a data analyst know? And again, what texts can you advise me to learn these notions and improve my algorithmic/coding thinking?

&#x200B;

Thank you a lot!",What are the basics of computer science for a recent graduate in statistics?,9ec69o,new,21,22,22,0
"Hello,

I'm trying to write a program to do some analysis on some poker data I have.  One interesting statistic is a player's ""Aggression Factor,"" (AF) which is defined as ""The number of times a player Raises divided by the number of times the player Calls.""  Alternatively, the player can Check or Fold, neither of which affect the statistic.

So AF = Number of Raises / Number of Calls

I would also like to have a meaningful value for the error on AF, given that it's a random sampling of events.  The more hands of data we have from the player, the more accurate our value for AF becomes.  However, in trying to derive the formula for standard deviation of the AF statistic, I seem to have made a critical error (I used this as a starting point:  http://webpages.ursinus.edu/lriley/ref/unc/unc.html)

Here are the values we know:

* X = Number of times player raises
* Y = Number of times player calls
* Z = Number of times player folds or checks
* N = X + Y + Z = Number of times player takes any action
* AF  = X / Y

What is a meaningful statistical uncertainty value for AF?  Clearly it should approach 0 as N approaches infinity.  And though AF does not depend on Z, it's possible that an uncertainty value for AF would (hence my inclusion of Z).

My derivation, which is clearly wrong, is:

Error = Sigma(AF) = (X / Y) * SQRT( (N/X) + (N/Y) - 2) / (N * (N - 1)).

I definitely know this is wrong because the value in the SQRT function is possibly negative.

Can anyone provide me with some help?  Also, let me know if this should go somewhere else (perhaps somewhere less technical - perhaps this qualifies as ""Homework Help"", though this is not homework as it's purely for my own edification - school was more than a decade ago for me now).

Thanks.",Determine Statistical Uncertainty on Ratio of Events,9eb0xc,new,4,0,0,0
"I’m running some pre-post questionnaires on groups of students to evaluate their experiences of a class. I’m in a spot where while I have received 22 returned pre tests and 20 post tests, I only have 9 peoples who completed both. Is there another way to run significance testing that doesn’t rely on paired testing and therefore throwing away half of my data? ",Noob question - overlapping samples and significance testing,9eazf2,new,2,0,0,0
"My apologies for the long winded explanation. Would be a great help if anyone could teach me a thing or two. Currently just a statistic noob in university.

&#x200B;

I'm currently doing a regression of effect of sanitation and water on mortality of children under the age of five. 

The variables I'll be using are as follow:

u5mort: number of death of children under the age of 5, per 1000 (dependent variable)

gdppc: gross domestic product per capita

sanitation: % of population in the nation that have access to basic sanitation

water: % of population in the nation that have access to basic drinking water

&#x200B;

My general regression equation would be:

ln(u5mort) = c + ln(gdppc) + sanitation + water  (n=39) (rsquared=0.8012)

&#x200B;

My problem is that I know water and sanitation will have an effect on u5mort, but the hypothesis testing for water was that water was not significant (p-value = 0.4638) and the coefficient of water was positive. Meaning a nation with higher % of population with access to water, it'll cause the mortality rate to increase. Note that ln(gdppc) and sanitation are significant, their p-values were less than 0.05.

In this case, I ran a regression of :

sanitation = c + ln(gdppc) + water 

and proved that water and ln(gdppc) are significant upon sanitation and they have an effect on sanitation. My main focus was the effect of water on sanitation.

The reasoning I came up with was that water might carry waterborne diseases so the coefficient from the general equation was positive, but not significant. At the same time, I'm trying to reason that water does not have a direct effect on mortality, but it'll affect sanitation which will affect mortality. Is there a test or a way to prove this? I've tried to input a variable of sanitation\*water to show the interaction of the two upon u5mort using latent effect. However, the new regression that I've got gave me positive coefficients for water and sanitation and negative coefficient for ln(gdppc) and sanitation\*water. All independent variables are significant with hypothesis testing. But obviously, I can't use this model because of how the coefficients work against what I'm trying to prove.",Problem with significance,9ea4fr,new,1,1,1,0
"A friend rolled 32d6 (32 rolls of a 6-sided die) in a game, and only 4 outcomes were 4, 5, or 6. I'm simplifying this as a coin flip where 1-3=heads, 4-6=tails. 

My question is as follows: is 4 tails of 32 coin flips the same as 1/8 or 12.5%, or would Pascal's Triangle apply here resulting in 33 permutations, only one of which produces 4 tails ( (H+T)^32) ?

Only have minor background with stats so I'm genuinely curious as to how to approach this situation. Thanks!","32 coin flips, only 4 tails?",9e9kz7,new,2,0,0,0
"My intents are to analyze the results with Excel, Power BI, and/or R and help analytics job seekers (those in college and those who already graduated) have a better time finding work.

I had a very turbulent job search before finding my first analytics role, so hopefully my analyses will help alleviate that for someone else!

Survey link. It should take around 5 minutes:

[https://docs.google.com/forms/d/e/1FAIpQLSd8K9K6CJZMk2cDnXmXzJZhqjFVHjZHmo-kxKlhXUFNRIL6kw/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSd8K9K6CJZMk2cDnXmXzJZhqjFVHjZHmo-kxKlhXUFNRIL6kw/viewform?usp=sf_link)

&#x200B;

\[EDIT\] I've only received 10 responses so far, so please respond to the survey if you haven't already!",Please Take This Survey if You're a College Grad and Working In or Pursuing a Career in Analytics!,9e8qrl,new,4,0,0,0
"Hi,

&#x200B;

I'm a student looking into correction for a project and just wanted to question if you think my idea would yield an noteworthy results. 

&#x200B;

I have some data for prices of an asset and want to view to link between the prices and my other variable which essentially an indexed value of often people google it. I have a value for each day of the asset and an indexed score for each day as well over a 1 year period. Would I need to de-trend the data before run the Pearson correlation for it to work? and most importantly would the results be of any value? From what I understand and expect the results should indicate a link between the amount people google it and the actually day by day value but i'm not 100% sure.

&#x200B;

It may seem really trivial but this is my first experience trying to do anything like this so any help or advice would be tremendous. 

&#x200B;

&#x200B;",Basic correlation,9e8kr2,new,0,1,1,0
"I’ve been looking for a job since I’ve graduate with a masters in biostatistics and have yet to find a job in the Bay Area, CA (over a year of searching). I got my bachelors in public health and have 5-7 years as a healthcare provider (data analyst for heart pump, EMT, EKG, etc..)

I want to get into research Data analysis in the health field (UCSF, public health department, etc ....), but have been applying to any position that is relatable. Even internships aren’t getting back to me. 

I’m starting to regret my decision to get a masters in lieu of just grinding it out in the work force. I have a job currently as a cardiac tech but  the pay is limiting. 

Has anyone had a similar story? Looking for some advice and/or stories to relate to from others in the statistics field. 

How did you get your first break?

Could you have been where you’ve gotten to without an advance degree? ",Need some input for some one,9e809x,new,2,0,0,0
"So im trying to learn statistics, and was wondering a few things. So i have a dataset, of employee scheduling from 2011 to today, and was wondering what i should use to portray that data. I have Date,	Start Time, End Time, Name, Location, whether they are Still Employee, and Shift Duration. ",Would like some help with figuring out what statistics to use.,9e7jq6,new,5,2,2,0
"I currently work in health care, but I have a strong interest in politics. Also, it seems like ""data"" is becoming extremely important in politics. Does anyone work in this field and if so, how did you go about securing your job?",Anyone work in politics?,9e725q,new,2,1,1,0
Inspired in the cheat sheets for RStudio and my notes in Evernote for storing and sharing chunks of code I´ve built one cheat sheet for easy “copy-paste” code lines for plotting with ggplot(). [https://medium.com/@obedm/cheat-sheet-snippets-for-plotting-with-ggplot-5a5403c5eac](https://medium.com/@obedm/cheat-sheet-snippets-for-plotting-with-ggplot-5a5403c5eac),[Cheat Sheet] Snippets for Plotting With ggplot,9e6yg5,new,2,32,32,0
"Hey,

Adrian Torchiana (creator of the brilliant app [Probability Puzzles](https://play.google.com/store/apps/details?id=atorch.statspuzzles&hl=en_GB&showAllReviews=true)) gave a great puzzle to Black Swans. Here it is:

**Five foxes and seven hounds run into a foxhole. While they're inside, they get all jumbled up, so that all orderings are equally likely.**

**The foxes and the hounds run out of the hole in a neat line. On average, how many foxes are immediately followed by a hound?**

Post solutions below!

For more puzzles, go to [www.blackswans.io](https://www.blackswans.io).",Foxes and Hounds Probability Puzzle,9e6lna,new,6,5,5,0
"Hi!

I'm currently starting 3rd year with my first expected coop placement to be this coming summer (as in \~8 months from now). What are some viable career options for a bachelor of statistics degree and how would I go about preparing for a co-op in said field? (as in for data science, I know that I need to become familiar with R and SQL, but where would I start with learning these?)

&#x200B;

Any help is extremely appreciated!",Questions from a Statistics Major,9e6490,new,3,2,2,0
"If I have a 2D stationary random process, I can express its covariance as C(i,j) = E[ X(n,m)X(n-i,m-j) ], with i, j lags. If we assume that the covariance is separable, that means we can express C(i,j) = A(i)B(j) where A and B are covariance sequences in the individual dimensions. I can understand that mathematically, but I'm having trouble building an intuition for it. What is A(i)? Is it A(i) = E[X(n)X(n-i)]? That doesn't make much sense since X(n) is not well defined to me. What are some examples of separable covariance processes? Does anyone have some nice introductory papers or textbooks?",Intuition behind separable covariance model,9e5mt3,new,5,8,8,0
"Full disclosure I'm not really well informed on stats. I am analyzing 3 groups of rats with surgery/treatment (surgery+drug, surgery+saline, naive (sham surgery)+drug): their behavior at 3 time points -baseline before any treatment/drug, and then 2 other times some days after the procedure. Additionally I'm measuring MRI parameters measured at 3 time points all after the procedures. My adviser recommended

repeated measures anova to compare each group individually to itself over time for all measurements, and then

some other higher level test to compare each group together at each time point.

Using JMP, I used MANOVA repeated measures for (1) but I'm not sure how to interpret the results (what do the results of ""between subjects"" and ""within subjects"" actually tell me?). I put the scores they had (3 sets b/c 3 days) as the Y response and put the group (surgery/treatment) as the model effect. Does this test or the way I do it seem right?

And for the second point (2), I'm not totally sure what to run - an anova with t test or something because it's comparing groups at individual time points? God stats are so confusing to me... any help would be appreciated!",Can someone help me decide what stats to do and how to interpret them?,9e5i0h,new,3,1,1,0
"I’m not sure if this is the appropriate place to ask, but I’m seriously considering going into statistics as a career and am wondering if anyone could give me any insight as to what it’s like.  I’m a senior in high school and I really fell in love with it after taking AP stat, but since it’s still just a high school course I’d like to know more about the field.  Any additional resources would be appreciated, too.",What’s your favorite and least favorite thing about the field?,9e4jwb,new,10,7,7,0
"I have no idea where to start with some data that I'd like to compare any help that you might be able to give would be hugely appreciated!

I have an average from a group of 44 that I would like to compare to the national average of 10,000 in order to to see if there is a significant difference between my group and the national average. I have individual vales for each of my 44, but not for the national average.

How do I go about comparing the data?",Comparisons of group average against national data.,9e37ja,new,7,0,0,0
"I was talking with a  trader  about the  average  returns  generated  by a stock during  a  time frame (10 years)

He asked  how could he  calculate the yearly  average gains that  stock  made over  that  time frame , 10 years.

I suggested  him to make  a  simple  average  mean, but he told me: ''Average mean  it's  not the only mean  you can use, it would  fit better  in this  case a geometrical mean''

So  as seen  that we are dealing with  time series 8the price of a  stock)  and  we  want to  know  the  average gains made  yearly by that  stock, which  kind of  mean  would  be  better  to compute?  An  average  mean,  a geometrical mean, or an armonic mean?","Armonic mean, Geometric mean or average mean?",9e1h4b,new,9,14,14,0
"Hi, I’m confused on how to calculate the expected value: 
Given a mean of 5.6 and following a Poisson distribution how do I find the expected value of:
X = 20 -2(Y) - 0.01(Y)^2 
",Expected Value Question,9e0zkc,new,12,1,1,0
"With the permission from moderators, let me invite you to join the new AI subreddit: [r/MachinesLearn](https://www.reddit.com/r/MachinesLearn).

The community is oriented on practitioners in the AI field, so tutorials, reviews, and news on practically useful machine learning algorithms, tools, frameworks, libraries and datasets are welcome.

Join us!","An invitation to join r/MachinesLearn, a new Artificial Intelligence community",9e0oj0,new,7,0,0,0
"Perhaps this is an all too hasty general rule of thumb but I'm thinking that if you have to ask whether or not you can infer causation then the answer is no, you can't.

Sorry to be silly.",As hoc rule on inferring causation,9dxyq1,new,3,0,0,0
"Any R -> R+ function would work, but I haven't seen anything other than logarithms being used. I suppose it's the easiest choice and normally works, but does anyone know of a justification for this or know of alternative links that should be considered?",Are there any specific reasons to use log links for regressions on a variance parameter?,9dwog0,new,3,2,2,0
"Hello

I've wrote an abstract that is going for publication in conference procedure. The reviewers asked for 95% Confidence Intervals to our results. We are only doing descriptive statistics on a sample and are not generalizing it to the whole population. We are a little lost on how to do it

>Materials and Methods: Prior to data collecting approval by the ethics committee of the
*** was obtained. A random sample on 100 individuals
from a total of 350 patients were selected.

>These were evaluated and characterized by: age, sex and school preference (I,II,III,IV). A descriptive statistical analysis with crosstabs procedures was
applied to check frequencies using the SPSS Statistics 20.0 software.

>Results: The average age was 18,9. 60% of the sample were women. For women a tie between school II and III (25% each)
was found making both the most common. For men school III (40%) was the
most common while school IV was not found in the studied population. 

Can anyone help? thanks",Noob question on Confidence Intervals,9dvd6y,new,11,15,15,0
"Hi everyone! I'm considering writing an economics master thesis in which cross-validation might become central to my argument. As such, I would like to prepare well in advance with regards to understanding the theory behind the method.

**Does anyone here know of a thorough theoretical exposition of the cross-validation method?**

Specifically, I want to test multiple hypotheses on the same data. I want to be able to do statistical inference without raising the t-values used more than absolutely necessary.

Any reference or pointers would be greatly appreciated!",Theory on cross validation,9dv5xx,new,15,8,8,0
"Hello, I study bacteria.  A common way to look at coexistence of two different species is to see whether each species can increase its proportion of the population when rare.  

So my experiments look something like this:

Species A and B.  Initial total population size = 10^4.  

Three treatments.  Treatment 1, A is 1% of the initial population, treatment 2, A is 5% of the population, treatment 3, A is 10% of the population.

I then measure the population of species A and B after a period of growth.  I would say A can invade a population of B if it increases in proportion at least in treatment 1.  It cannot invade population B if it decreases in proportion in treatment 1.

I also have 3 reps for each treatment.

A wrong test to do would be to take the proportions themselves and do a t.test, for each treatment.  For example, t.test(c(0.01, 0.01,0.01), c(0.013,0.02,0.011)) in R. But this doesn't take into account the population sizes. 

A second idea is to take the CHANGE in proportions for all the treatments, run a linear regression on these numbers, and see whether the line crosses y=0 or not, and if not, which side it is on.  This, again, does not take into account the raw numbers.

Any suggestions on the right approach?

Thanks!","Analysis of change in proportions question. Want to determine statistically whether change in proportion, from a number of initial proportions, is always in the same direction.",9dv1ze,new,3,3,3,0
"Hi all,
I have a data set with:
Independent variable: numerically coded teacher actions 3 ratings per teacher, 300+ teachers
Dependent variable: 3 measures of outcomes per student, about 20 students per teacher

What statistical test can I use to show the relationships between these variables? Some ideas I have are ANCOVA and t test. Will these tests prove correlation or causation?
Thank you!",What statistical test should I use? First year doctoral student in education research,9dqu7o,new,3,1,1,0
"Hello everyone, I am a student in Applied Statistics at my university. It is my sophomore year, and I obtained a 4.0 in calc 1 and 2 freshman year. Because of this, I came into this semester confident in my ability to learn mathematics, until yesterday. I have to take a professor for Linear Algebra who is known for his heavy use of proofs, and it only took one look at the homework problems to freak me out. I have no background in mathematical proofs, and it seems almost all of his problems ask you to prove something. In his textbook, he says that he expects that students have taken an intro class in discrete math or logic, and I have not because it's not a prerequisite. I am getting extremely worked up and wondering if I have the ability to succeed in a class where I have to learn proofs AND new content from a professor known for being inaccessible to students. If anyone has any advice on learning proofs from the beginning,  or doing homework that you do not know the answer to, I would really appreciate it. On top of this, is this a common feeling stats majors have? I have the option of switching my Multivariable calculus and linear algebra course's semesters, although the multivariable professor is supposed to be hard to understand  (accent) and learn from as well (at least that course has SI). Should I consider switching courses? Thanks for any input you can provide, I really need it.",Questioning my Abilities,9dpsfd,new,7,3,3,0
"In 2016 a company with 1000 salespeople made $5mil in sales. In 2017 a policy change was enacted and the same salespeople made $5.5mil in sales. How do I prove that this increase is statistically significant? Seems like such a simple question but I cannot find this online. 

P.S. I do know the individual salespeople's figures for both years. ",[Help] How to determine if annual sales increase was statistically significant?,9dpjd0,new,42,11,11,0
"Hello! I hope I'm in the right place. But here's the situation that I could really use a fresh perspective on. 

I'm helping run this HUGE softball tournament of sorts and the games are all super quick. It's a two day event with 20 teams participating all resulting in a single elimination championship game on the second day. The best bracket setup my tournament director has come up with requires 4 pools of 5 teams to play each other once in round-robin. After that happens isn't really a concern. But what if (by Murphy's Law) every team in the tournament goes 2-2 in their bracket? We're trying to figure out what to do in that particular situation. A 2-2 result for too many teams in either one or multiple brackets could wreak havoc with our schedule. Establishing scale for criteria like head-to-head results, followed by least runs allowed, followed by most runs scored, could work...but the goal is to minimize the amount of team captains we upset by eliminating teams who had a 2-2 record (but weak secondary critera).

&#x200B;

Perhaps this system will work out and there won't be any 2-2 tiebreaker situations, but I'd like to know all of my options and hear plenty of  opinions if at all possible. 

&#x200B;

So...could anyone help me? Share an opinion or point me towards a more appropriate sub for Tournament admins? 

&#x200B;

Thank you,

&#x200B;

&#x200B;

&#x200B;",Bracketology Question - In Need Of A Statistician's Opinion,9dpbzd,new,9,0,0,0
"I have a txt file with time recorded in four-digit numbers. For example, 12:45 is written as 1245. I want to calculate time difference between two time variables, but directly subtracting will cause an issue because time is base-60 instead of base-10. Is there any way to convert this 1245 into 12:45 in SAS?",How to read time into SAS,9do6o7,new,6,1,1,0
"Hi!

I am currently describing a real data set and wonder what to make of its shape. The data is continous, but the sample size is around 130 so it's not surprising that the density plot is not perfectly bell shaped but it does suggest a Gaussian distribution. The KS test did not reject H0: ""the data is normally distributed"", but then again that test has low power. Which brings me to my question: When can I actually speak of normally distributed data? I simulated some data of the same dimension according to a Gaussian distribution but of course, it does deviate from the expected shape. Is that particular data set normally distributed or not? While I specifically designed it to be Gaussian distributed but it doesn't show with that sample size. Again, that is of no surprise but I'm not quite sure how I am to report such data. I feel quite silly for reporting heterogeneities in such a small data set when simulated data start to actually look like a Gaussian distribution at n= 10 000. 

&#x200B;

I appreciate any input, thanks a lot!",Do simulated data actually follow the specified distribution? When does a data set follow a certain distribution?,9dny26,new,10,2,2,0
Are there any good resources specifically tailored to analyzing sales and/or pipeline data?,Sales Pipeline Analytics Resources,9dnokl,new,0,1,1,0
"Ok so, in this game (Granblue Fantasy) the player can hit the opponent with critical hits. This critical hits are determined on every attack, and the stat itself of ""being able to critical hit"" comes from different sources. One can get ""critical hits"" by having special equipment, or use a skill that makes player critical hit.

&#x200B;

The way critical hit works, is with a simple percentage. On every attack, the player gets X percentage of doing more damage.

So, for example, after using a powerfull skill, the player gets a buff that gives 70% chances to critical hit on every attack.

This is all fine, but here's the deal. IF the player, after having used said skill, then uses ANOTHER skill that gives the exact same percentage of critical hitting, what the game does is NOT to add 70%+70=140%, but instead go into every crit source separatedly

&#x200B;

Sooo, let's say, for example, that the player gets ""70% chance to critical hit"" from 3 different sources. The game would check every instance one by one, something like this:

1 Game rolls a dice with 100 faces. If number that gets rolled is <= 70, the player critical hits

then

2 Game rolls second dice with same number of face. If number that gets rolled is <=70, the player critical hits

Then

3 Game rolls second dice with same number etc etc etc.

The reason the game does this, is because the game wants to check if the player can critical hit...a critical hit! So if I already critted on 1st time, and i get the crit on the second, the second crit damage gets added to the first so that the critical becomes a critical-critical hit, in a sense it becomes a super sayan level 2 critical hit. 

&#x200B;

HAVING SAID all of this, and I ask sorry in advance because, not knowing statistics, I don't even know if this Whole info was even important in the first place, my question is:

If I do have two separate 70% chances to critical hit, what are my odds of critical hitting AT LEAST ONCE?

I do understand that ofcourse they are higher than 70%, but how much?",Help me with basic statistics to understand the mechanism of famous game,9dnc51,new,2,2,2,0
"Hey r/Statistics,

Hopefully this is a pretty quick question. I'll try to keep it brief: 

I started working my way towards fulfilling the prerequisites for a MS Biostatistics program. I've been taking the classes at a community college. I'm a good student (Straight A's in bio, chem, calculus so far, a 3.8 GPA from my art history undergrad), but I signed up for the one statistics class the college offers this semester and it's concerning me. The textbook is literally from 1975 and the first few weeks of work look like they're going to be tedious busywork, drawing histograms by hand from data sets with six elements and the like. Things everyone does in elementary school. I'm concerned the class is going to be so outdated as to be useless.

I've been teaching myself R over the summer in the evenings after work and it's been--um, I'll just say--considerably more stimulating. I'm tempted to drop this statistics course and continue teaching myself. The class has only met one time, but I'll still have a Withdrawal on my transcript. Is this the right move? Or is this how all basic stats classes start?

I've spent the whole afternoon drawing dot plots on paper and ticking off values in data tables, and am beginning to feel resentful. Not sure if I can handle this for the next 14 weeks.",Is this normal?,9dm3wt,new,11,1,1,0
"I am a statistics major and I also work part time as a computer support assistant. I love my math and statistics coursework and can definitely see myself working as a Statistician or working in another statistics related career. I also enjoy my part time job, which involves solving computer related issues and learning new things about networking, operating systems, etc. every day. I would like to have a career which would combine these two interests. Which career paths would allow me to do statistics related work in the information technology domain (besides data science)?

Edit: I would like to hear about other careers besides data science because data science is the most obvious choice and I think everyone knows about it. I want to know about other careers that are use statistics and information technology that aren't mentioned as much as data science.",Information Technology Statistical Careers,9dlzwy,new,7,6,6,0
"  The overall objective for my project is to estimate population size of three demographic groups using a capture-recapture method. We used a formative assessment (meeting with local gov officials, community gatekeepers, etc) to identify potential hotspots for each of the 3 groups, and came up with a list of about 8,000.

We then dispatched enumerators for hotspot mapping and validation; essentially they seek out the pre-listed potential hotspots and either confirm or invalidate them. We programmed tablets that contained the list (select hotspot name from a drop-down menu), and a survey to be completed for each hotspot. Unfortunately, we had a number of issues that have produced a large amount of duplicates during data collection for hotspot mapping and listing. Some were technical (drop-down menus of hotspot names were switched for two of the provinces, so all of the names had to be manually entered; a programming error in the tablets that caused multiple entries if data wasn't refreshed immediately after finishing a hotspot), and some were human (enumerators didn't recognize a name in the drop-down menu, and manually entered a name that we can't validate; different enumerators visiting the same hotspot incidentally).

So currently I have a list of 20,000 hotspots that are probably 50% duplicates, and we need to begin capture-recapture in 24 hours. Apart from entries that are identical in all fields, I can't be 100% which are duplicates and which are not. If we initiate capture-recapture with a duplicate-riddled dataset, how will this affect estimates of population size?

Thanks and happy to clarify anything.",Hotspot Mapping for Capture-Recapture and Population Size Estimation,9dl1s1,new,5,6,6,0
,How do you implement a hold out group when a product feature is 100% launched?,9dkycx,new,1,2,2,0
"I have three product lines, each with different amounts of products in their lines. I need to know how many combinations are possible, taking order into account so we arent getting repeats of the combinations just with the same products in a different place. I was reading up on combinations and permutations, here [https://stattrek.com/online-calculator/combinations-permutations.aspx](https://stattrek.com/online-calculator/combinations-permutations.aspx)  and here[https://www.calculator.net/permutation-and-combination-calculator.html](https://www.calculator.net/permutation-and-combination-calculator.html) and I think I am headed down the right path, but I don't see how to set this up. Any guidance you can direct me to to either use or read up on is appreciated.

&#x200B;

&#x200B;",[Help] Need to know what to look for to learn to solve a work problem,9dkgel,new,2,3,3,0
"I am thinking of taking statistics next year in university, and my question is: Do you know any useful books or perhaps online courses in statistics? I really like the subject and want to increase my knowledge in order to have a higher chance of being accepted into a university. Maybe you know a youtube channel or a website that covers the subject?",Statistics Degree questions,9dk84j,new,11,20,20,0
"So I have a dataset of 36 participants. Each participant were asked to do a specific task under 3 different conditions. During each task, heart rate, right and left pupil diameters were collected. The dependent variable is a continuous variable measuring their performance. I would like to find the correlation between collected data (HR, right and left PD) and the performance measure. 

&#x200B;

What is the right statistical approach to use to assess the correlation between those measures and my dependent variable?

&#x200B;

&#x200B;",[SPSS] Using a Linear Mixed Model to analyze within subjects under different conditions,9djynl,new,4,4,4,0
"I hope this post won't get too long, I'll try to be as concise as possible. I apologize if this is not the place for this post.

I'll give the bad first, I'm taking a 6-sigma black belt training.

Now, before I'm permabanned, let me explain myself.

This is a course my company is paying for me, on the expectations that I become the mythological black belt that cuts costs and increase efficiency through the mystical power of statistics. I'm self-aware enough to know that what is taught during the course barely scratches the knowledge needed to effectively use statistical analysis as a tool for quality improvement and process control. I had some knowledge of the field prior to the course, mostly thanks to my curious nature, but I prefer to see myself as an ignorant on the matter. What I know is only enough to make me doubt the preaching given by the lecturers.

Nevertheless, I already have one project in my hand (and soon enough, I'll have a handful). I want to be able to do a good job. If I have to use statistics as part of my project (and I *do* want to use it), then I want to make sure I get it right and that my findings are legitimate.

On the other hand, I lack the formal training (and the available time to take one now), but not the will to learn. I have a good grasp of some basic concepts (probability density, confidence interval, t-test, anova, what really is p-value and why it's not a definitive test, etc). As I said, I know this is a beginner-level understanding of the field. Besides, the more I read, the more paranoid I become that I’m unwittingly making mistakes.

What I’m looking for is a direction on what to learn. I don’t have a trouble studying more technical or advanced concepts, but due to time constraints, I could really use some sort of guidance and learning what is really necessary to ~~survive~~ complete the project. I’m asking this because I already have a data set, but since it is not anything like the normal-numeric-continuous friendly cases we see in the course, I’m struggling a bit to learn how I should analyze it. What methods should I use? Is the data structured correctly in order to answer my questions? Does it even make sense to ask these questions for my project?

Any help here would be much appreciated. Advices, websites to read (maybe books), forums to ask for help (if this is not the place) and so on.

&#x200B;

tl;dr: Self-aware 6-sigma black belt is looking for help to do statistics right.

&#x200B;

Sorry for the english, not my first language,",[Help] Looking for education in statistics,9djwab,new,1,1,1,0
"Hello! I am developing my research thesis for my masters and am having a tough time finding a good online survey generator to draw correlations from. It will be a survey held in Europe sent to over 4000 participants. It needs a function that the user can choose their language as we have it in multiple languages and we need the data to be imported into excel, sheets, or directly to R. Our trial run proved that simple generators like Survey monkey makes us manually input the data into an excel which with hundreds of results will be brutal. What are some good online survey programs I could look into? Thanks! ","Best Survey Generator that can import data into excel, sheets or R?",9djl6b,new,17,7,7,0
"I want to do a trend analysis of various storages in a water quality model. This is my approach.

1. Only conduct trend analysis if timeseries is longer than 5 years.
1. Calculate a linear regression where X values are the days since the start of the simulation, and Y values are the storages (for e.g. water volume in a reach, bed-sediment in a reach, total phosphorus in a reach etc.)
1. Analyze the slope of the resultant linear regression.
1. If the slope is 0, then there is no change in the storage over time. If the slope is +ve then the storage is increasing and if the storage is -ve then the storage is decreasing. Come up with a range beyond which the program will generate warnings in the model QA/QC report.

Please suggest any issues with my approach .

Thanks in advance for your help.",Trend Analysis in Water Quality Modeling!,9dctg8,new,2,6,6,0
"No luck in  /r/AskStatistics, thought i might try here with the broader audience.

Situation: market price analysis for various competitors. 

similar situations:
looking for patterns in neuron firing rates, where I am reasonably sure analysis has been done but not sure how. When this part of the neuron rate changes is there other parts that do too, and how?

another could be stocks. Price of stock over time.  


Aim: 
investigate how the prices change together or separately over time.  Does one competitor follow another directly, maybe a lag? or... 

So in this situation there is no x/predictor variable except maybe time.  All the variables are outcome variables and the aim is to find out how they change together.   

The linear, non-linear and lag aspects may all require separate modeling. 

Considered methods:

AMOS modeling.  The version I did before treated the variables as separate and thus might not work in this situation. 

 * iteratively run, run standard correlation pearsons/find a non-linear correlation methods, for all seasons. 

Wikipedia lists theses three possible sources for non linear:

      Croxton, Frederick Emory; Cowden, Dudley Johnstone; Klein, Sidney (1968) Applied General Statistics, Pitman. ISBN 9780273403159 (page 625)
      Dietrich, Cornelius Frank (1991) Uncertainty, Calibration and Probability: The Statistics of Scientific and Industrial Measurement 2nd Edition, A. Higler. ISBN 9780750300605 (Page 331)
      Aitken, Alexander Craig (1957) Statistical Mathematics 8th Edition. Oliver & Boyd. ISBN 9780050013007 (Page 95)

 * Some variation of factor analysis but for time based situation - I.e. is competitor 1 so similar in pricing over time to competitor 2, based on linear/non linear/lag analysis that they could be considered one entity. 

 * Confidence interval analysis of the slop/differentiation/correlation of the data over time. 
 
The last one is similar to:

 * Create a market mean and then modeling that as ""0"" and looking at the different competitors relative to this. Kind of like a GLM but takes into account the change in mean overtime. 



Thoughts?
 
",[ex post askstats] looking for advanced correlation analysis,9dcm0m,new,9,7,7,0
"Hi all,

I'm in the middle of running an HLM with 1 DV, 2 IV's, and a co-variate.

&#x200B;

My question is:

Should the co-variate be nested within the first IV, and then the second? Or should everything be nested within the co-variate?

&#x200B;

I understand the theory in the ordering within the two Iv's, but have no clue about how a co-variate should be ordered.

&#x200B;

Any help is welcomed.

&#x200B;

Thank you",Hierarchical regression question,9dcl9f,new,4,1,1,0
"People are known to be bad at interpreting what probabilities mean. For example, when FiveThirtyEight gave Clinton a 70% chance to win the 2016 election, to many that looks like certainty.

Now FiveThirtyEight is making projections about the midterm election and using rounder numbers to try and communicate the uncertainty in a way people will properly interpret, and those numbers are generally around 70%-80% chance of Dem win, around the same odds Clinton had of winning.

I was looking at the projection and I got the idea that the visualization FiveThirtyEight shows seems to better convey at an intuitive level what that number means. That is, while ~70% feels like good odds for Democrats, the visualization makes Dem odds ""feel"" worse than the number itself. That is, it better conveys the sense that Democrats are the favorite but a Republican win would not be a huge upset and is certainly a possibility.

When I teach statistics I often tell my students to use their intuition about area to think about probability, since many of the same rules apply (both are particular cases of measures). I'm wondering if area is also a good sense to convey what a particular probability means or how it should be interpreted. Human intuition about probability is bad but our intuition about length or area is pretty good.

Any thoughts on this? Research?",Communicating Probabilities as Areas,9dc3r7,new,5,17,17,0
"We owe a lot to him in the realm of computation but everyone only seems concerned with calling him a climate change denier, which by the way, he isn't. ",Why is Ed Wegman shit on so much?,9d9vm4,new,3,0,0,0
"I've seen it used in the context of mixed models and with missing data problems. I feel like it's just shorthand for ""this familiar likelihood with something extra""/""the actual likelihood we'll use"".

Wondering if using the ""augmented"" qualifier gives any actually useful information.","Is there a rigorous definition of ""augmented likelihood""?",9d85kt,new,0,1,1,0
"Title says it all :).  I'd like to see data that shows us, of all the tax revenue the US Government earns year over year, what percentage of that revenue comes from the ""middle class""?","I'm trying to find data that supports this claim: ""The rich are not taxed. It's the middle class, especially the educated upper-income middle class, who pays for the poor."" - Do we have Federal revenue statistics that show us a pie chart of ""total tax revenue"" split into socioeconomic tiers?",9d7n9j,new,5,2,2,0
"Greetings everyone,
I'm an undergraduate student and would like to learn about probability theory. I have read and worked through statistics by David friedman and have a solid foundation with mathematics so I'm familiar with most of the basics.right now I'm considering 2 textbooks but am not sure which textbook will be better.
1.Probability Theory: The Logic Of Science By E. T. Jaynes
2.Introduction to Probability
Book by Dimitri Bertsekas and John Tsitsiklis

I need something that is suited for self learning as this is not part of my program.also, I'm studying economics so one that provides context related to field would be nice but it shouldn't just limit itself to application in economic problems.

If you have any suggestions Besides these 2, then feel free to recommend them.
Thanks

EDIT: thank you everyone for all your replies. you have really helped me out and for that i am most grateful",Textbook recommendation for learning probability theory,9d7bhu,new,21,28,28,0
Edit: I emailed the Census and they confirmed Poverty Rate is the same as Percent in Poverty. ,Is there a difference between poverty rate and poverty percent? Thanks!,9d6rnj,new,3,0,0,0
"I am currently working with copulas to predict some kind of bivariate count data. I have yet to understand the calculation from [Sklar's theorem](https://en.wikipedia.org/wiki/Copula_%28probability_theory%29#Sklar's_theorem) with the copula-function C and a possible differential c.

For an example let's assume a Gumbel copula from 'copula' in R.

    library(copula)
    # parameter arbitrary
    lambda1 <- 1.2
    lambda2 <- 1,4
    theta <- 1.2
    Cop <- gumbelCopula(theta)
    Cop <- mvdc(Cop, margins = c(""pois"",""pois""),
                paramMargins = list(lambda=lambda1,lambda=lambda2))

Let's now assume I want to calculate the probability for (2/2) by substracting and adding parts of the copula (distribution) function.

    pMvdc(c(2,2), Cop) - pMvdc(c(1,2), Cop) - pMvdc(c(2,1), Cop) + pMvdc(c(1,1), Cop)
    [1] 0.06039884

I did the math via the poisson margins and Sklar's theorem and it checks out. The question here is: Why can't I use

    dMvdc(c(2,2), Cop)
    [1] 0.07344485

which is supposed to be the density at (2/2)? Another thing is, that if I cumulate over dMvdc(c(i,j), Cop) for i,j >0 it won't add up to 1. My guess is that this has something to do with copulas with discrete margins not being unique, but I can't put my finger on it yet. A continuous copula with both discrete margins seems like a very odd concept. Could somebody point me to some good literature on this? I couldn't find anything in standard copula literature. Any help would be appreciated.",Question about continuous copulas with discrete margins,9d5ud8,new,2,9,9,0
"As far as time series analysis is concerned, moving averages can be calculated using only previous values or both previous and following values. At least in textbooks, it seems to me that the latter are more widely used. Since they sound almost equivalent to me, I was wondering why there is a preference for moving averages calculated on both previous and following data. As far as I know, both moving averages are equally able to detect changes in the trend component of the time series. As soon as one moving average changes direction, the other changes direction too. Is there something wrong with moving averages calculated using only previous values? Are they less popular just because they cannot be superimposed on raw data?",Why are moving averages calculated only on past values so unpopular?,9d4o0e,new,5,2,2,0
"Let's say A=qualitative variable and B= quantitative variable.

If the question is to add an interaction between B  and each option of A, should I create dummy variables for A and just do A1xB, A2xB, etc? 

Or is there another way. I am doing this in R. Sorry if I am not clear enough.",If I do interactions (for multiple regression) between quantitative and qualitative variables should I use dummy variables for the qualitative variables?,9d4f3x,new,4,5,5,0
"Hi, I am doing an assignment where I have to answer some questions appraising an RCT. 

One question is ""How precise was the estimate of the treatment effect? Consider: what are the confidence limits?"" 

The mean difference on the outcome of interest (which could have scores from 0-10) was 2.2 (CI 1.51-2.86). The confidence level is 95%. The effect size was 1.32. 

How would I interpret the precision of the estimate of the treatment effect? Do you think I'm supposed to just eyeball it and say yeah it's pretty precise because the confidence interval is only 1.35 points wide (CI 1.51-2.86)?

Thank you.",Interpreting precision of treatment effect based on the confidence limits?,9d3w3x,new,1,4,4,0
"I understand that estimates of the standard deviation using a GEE/marginal model framework are consistent even in the presence of covariance structure misspecification, thanks to the 'sandwich' variance, but this isn't the case for linear mixed-models where misspecification can lead to biased estimates of the random effects.

What I want to know is how much of an issue this is when it comes to log-linear mixed-models (I'm working specifically with count data, so I assume a Poisson/negative-binomial distribution). For example's sake, let's say we have data on

1. Students (individual level)
2. Schools (cluster level)

And we want to investigate the association between student level characteristics and the **rate** of student smoking, controlling for school. What sort of considerations do I need to be aware of when it comes to specifying the covariance structure? I assume the same issues can arise from misspecification, but I haven't had to do this specific type of modelling before so I am seeking some advice.

Any explanations or pointers to papers would be greatly appreciated.",How much of an issue is covariance structure misspecification in log-linear mixed-models (count data),9d3azr,new,0,4,4,0
"Hello,

I graduated with a bachelor's in Economics & Public Health in 2017. Since then, I've mostly been figuring out my career roadmap. After being involved in a summer internship program in a health system and engaging in data wrangling and some stats analysis (Survival Analysis mainly - Cox and KM) and learning the foundations of Python and SAS, I learned I want a quantitative role and be a future data scientist / Clinical Informatics/ etc. My main concern in applying to a lot of MS in Statistics programs is I lack a strong quantitative background. I want to take additional courses at a good Uni in Prob and Stats, Linear Algebra, Mutivariable Calc, etc. I know many schools have uni extension programs but I would like to gain academic units to signal that I am capable of statistics work. Any advice on how to be a stronger applicant for a Stats masters for a guy like me? Thank you guys in advance :) ",Advice for Academic Unit Certificate Program to signal Quant Ability,9d2cao,new,0,1,1,0
"Hello All,

&#x200B;

Kinda stuck on a question, and browsing the internet without being able to discuss what they're saying has left me even more lost. 

Basically, I have 4 percentages (7%, 22%, 26%, and 40%) taken from different population and I would like to know if there is a statistical difference between them. 

My question is: what test do I use and what is a good website to do that test? 

&#x200B;

I appreciate any help, Thanks!",Beginner question,9czoq6,new,7,1,1,0
&#x200B;,can someone help me understand big data and the hype around it?,9czf8e,new,17,26,26,0
"**Background**

I'm a Master's Student of CS. I'm recently working on a project with an LSTM network to forecast time series data because I want to go into quantitative finance. I have decided that even if I don't make money from it, it might serve as a good talking point and shows strong motivation. 

&#x200B;

It's a fairly large project and I've been getting stuck quite often. On one hand getting stuck is where the learning happens (this happened to me in my undergrad classes) but, on the other hand it is overwhelming sometimes and I find myself wasting time and, as my undergrad PI used to say ""trying to re-invent the wheel."" This is not ideal, as I'm beginning to realize the importance of efficiency while undertaking coding projects. 

&#x200B;

I've had some success planning things out more on paper before jumping into the code (I'm trying to spend at minimum half my time on paper and reading), and breaking down the project into very, very small goals. I've also started limiting my daily work on it to 1-2 hours, as I'm going for quality over quantity.

&#x200B;

It feels quite liberating to do this and I will certainly keep at it. I suspect successful statisticians and researchers may follow similar logic. But I wanted to ask the community here if you have any organized methods for tackling personal projects that you've found success with in the past. Feel free to discuss a project you've applied it to!

&#x200B;

Super excited to hear from you guys.",What is YOUR method for tackling an individual project?,9cxrep,new,0,1,1,0
"I'm wondering about the best way to go about reporting within-subjects contrasts for a two-way repeated measures ANOVA when there aren't necessarily significant main effects?  


I tried googling how to generally report contrasts, but I couldn't find a straight forward answer. ",Reporting within-subjects contrasts for repeated measures ANOVA when within subject effects are not significant?,9cwv3o,new,1,5,5,0
"I want to check if a value of a variable (Var1) at the beginning of my study is the same as at the end to check if the manipulation persisted (B=before, E=end). Furthermore, my data has 3 Groups: high, low, control (H, L, C), which makes it a mixed design for this calculation.

I calculated new Variables for each group and each measurement. For example HVar1B, HVar1B (Highvariable1Before and Highvariabale1End)

In each group, there should be no difference between the before measurement and the end measurement (Var1B and Var1E). The data is not normally distributed, why I wanted to use a non parametric test.

In SPSS, I tried to do a Wilcoxon test (Analyze-nonparametric-2paired). The issue is, that I'm not certain if my workflow was the right one? I attatched the output to this post and it shows significant p-values. Yet, the means between the before and after measurements are very similar, yet for each group significant (which should be not).

If I worked correctly, the manipulation check in my thesis would fail... So I just want to be sure I did everything right?

&#x200B;

[https://imgur.com/a/Kd7vB5b](https://imgur.com/a/Kd7vB5b)",Method for Before / After in 3 Groups,9cwqd9,new,0,2,2,0
" 

Hello people,

​

I am a medical student and I would like to learn some statistics and how to implement it in some programing language. I have some basic knowledge of statistics (mean, variance, standard deviation, power analysis, t-tests, ANOVA) but I would like to learn more.

​

For the last couple of weeks I have been doing the ""Statistics and R"" course on [edx.com](https://edx.com/). The first quarter of the course was pretty easy and basic stuff so I had no problem. Then they start talking about t-distributions and central limit theorem and confidence intervals and I got lost. The course has a textbook and some videos, I went through the textbook twice and still couldnt understand what they were talking about or how to solve most of the exercises. After some googling many I realized that many people have the same problem, complaining that the course is not structured well enough. So I decided to search for something else.

​

So as I said I am searching something about the basics of statistics, it would be nice if it was a program so I can take more advanced courses later on. About the programing languages I know a bit of matlab and R but I am willing to start something new if the course is worth it.

​

Any ideas?

​

Thanks! :)",Searching for online course. Help,9cwaqp,new,17,12,12,0
"I have to write a project in C++ CUDA for a course. Any ideas of what I might do? I am looking for something interesting, that is also simple to do so I don't have to spend too much time on it. Thanks.",Interesting idea for a simple CUDA course project?,9cw81j,new,4,1,1,0
,Is normality of the data required when working with panel data?,9cvxej,new,6,3,3,0
"Hello,

​

after finishing all the calculations, my professor asked me to do a CFA (Confirmatory factor analysis) for my thesis.

The issue is, that the computers in my university only have SPSS 25, but not AMOS.

I just downloaded PSPP on my own computer but it seems it can only do PCA.

Is there an easy way to do a CFA withouth having to learn R?

&#x200B;

edit: I'm very sorry about the misspelled title :(",Who to do a CFA?,9cuytd,new,14,1,1,0
"Hopefully this question is not as dumb as I think it is.

I'm using sklearn's `confusion_matrix` and `classification_report` methods to compute the confusion matrix and F1-Score of a simple multiclass classification project I'm doing.

For some classes the F1-Score that I'm getting is higher than the accuracy and this seems strange to me. Is it possible or am I doing something wrong?

Thanks",Can F1-Score be higher than accuracy?,9cuof8,new,4,1,1,0
"I have two variables predicting my outcome. My first hypothesis is the the main effect of the first variable, my second hypothesis is the second main effect and my third hypothesis is the interaction effect.
Now, for the first two hypothesis: do I take the coefficients and p-values of the full regression model with the other variable and the interaction? Or do I take the values of a seperate linear model without other influences?

Thank you very much in advance!",One multiple regression or two linear models?,9cuetj,new,3,0,0,0
"Please assume I have a Gaussian distribution developed over several years, different manufacturing dates & hundreds of data points; this distribution has been used to set a specification.

The process continues to run and data records continue to observe the normal distribution.

I want to compare one production campaign with another; typically, each campaign is always well within distribution & usually the data is within +/- 2 standard deviations of the original distribution.

Please stay with me ...

The last production campaign started in the +2sd region and shifted to -3sd during the run.

The product is still ‘within specification’ but clearly something changed (The test has been verified & results are repeatable)

How can I express the probability of a process that is normally within 2sd of the mean shifting to -3sd because something has changed & I would like to be able to say ‘there is a (low) probability of such a shift and we should not accept the product as ‘in specification’ when this step change occurs’

But I don’t know how & just because this product meets the specification (on this one test), I don’t think it represents the production history.

Can someone explain the normal distribution to me?  Is a value jumping from +2sd to -3sd as probable as any other shift?

Put plainly - I’m used to seeing a campaign jump up to 3 standard deviations from start to finish; but never seen a shift of 5 sd before; and I find the conclusion ‘it’s in specification ‘ unsatisfactory & I think it warrants investigation.  It’s gut instinct because I lack the smarts but I’m hoping someone can point me to something that will help me demonstrate a process investigation is warranted.",Normal Distribution,9cq4t8,new,3,2,2,0
"This is a new problem for me and I'm not sure what to do.

I have 32 binary input variables and a continuous output with 4k observations. All are ""yes"" for 2 or more inputs, over half are yes for 3+ inputs and ~20% are yes for 5+ inputs.

The goal as told to me was to find combinations of ""yes"" which result in very high output.

Idea 1: What I could do is just run a linear regression and just tell them the top few coefficients, and say that the combinations of those will result in the highest outputs, but I don't think that will yield anything interesting, since it's already known which inputs alone are correlated with the highest output. 

Idea 2: I've had a weekend-long brain fart over this but...would re-creating the dataset as a bunch of pair-wise combinations of the original 32 variables be worth doing? (as in, a ""1"" indicates a yes for some pair of inputs). This means I'd have p=496, so I'd use elastic net regularization. However, I'm thinking this would also give the same results as the previous idea, where the inputs that individually correlate with high output will perfectly predict the pairs with the highest coefficients, and thus not really provide anything interesting.

Idea 3: What I really thought would be interesting is adding pair-wise interaction effects between all the variables. This would mean adding 496 interaction effects so again I would need regularized regression. However, I've asked about this idea previously and it was pointed out this may just create a ""junk dataset"" and I would just be modelling noise. (may also be a problem with idea 2)

So, any ideas on how to approach this?",How do I find combinatons of variables with the strongest effect?,9cpgvr,new,6,1,1,0
"Hi,

I’m a sophomore studying statistics and was talking with some grad students / upperclassmen from other departments today. They were talking about their capstone projects and then asked me what type of projects people do as statistics majors. I’m not sure of the entire scope of what I’ll have learned by then so I couldn’t think of any applications. Now I’m curious because it’s something I’d like to start thinking about. What did you guys/gals do for your senior capstone project? 

My core projects that I’ve done on my own time are factor models in finance, mean variance optimization in finance, Kaggle classification projects using different sklearn algorithms (KNN, RF) and a lot of solving calc problems computationally via Monte Carlo simulation or more geometry through brute force. Think creating function to find 1000s of distances to a plane from one point then finding the minimum. <—- My most recent as of today for multi variable.

Thanks!",Undergrad capstone projects?,9cp270,new,0,4,4,0
"This is a somewhat silly question, but as a stats grad student both working through problems and helping teach lower level courses, I cite this identity all the time. Does this have a specific name, so I can tell students what to look up? It's just a basic tenant of the sum of squares relation, but that can refer to many things. In Latex form, it goes (where \bar{X}=1/n*\sum X_i, the sample mean). Note \mu is of course usually referring to the true mean, but that's irrelevant to this identity. 

$$
\sum (X_i - \mu)^2 = n(\bar{X} - \mu)^2 + \sum (X_i - \bar{X})^2
$$

Basically, the sum of the squared difference between the X_i's and a constant is the sum of squares o the difference of between X_i and their sample mean, plus n times the squared difference between that sample mean and the previous constant.",Does this commonly used identity have a name? (Basic sum of squares relation),9con0j,new,5,2,2,0
"Hi,

I recently interviewed Gian-Carlo Pascutto, creator of LeelaZero, the Go Engine based on AlphaZero. I thought I would share it here for those who were interested: [https://blackswans.io/post/14/](https://blackswans.io/post/14/).

Enjoy,

Jack",How a Firefox Engineer is Using A Deep Convolutional Neural Network to Replicate AlphaZero,9cnxhb,new,4,24,24,0
"Hi all! I'm new to this subreddit. I just picked up a copy of Think Stats by Allen B. Downey. The author assumes some basic mathematics knowledge:

>I assume that the reader knows basic mathematics, including logarithms, for example, and summations. I refer to calculus concepts in a few places, but you don’t have to do any calculus.

Can anyone recommend some resource(s) that cover this math knowledge?

&#x200B;",Math requirements for Think Stats book by Allen B. Downey,9cnaez,new,1,1,1,0
"I dont know if this is the right spot to ask this question, but I am in desperate need for career advice. I apologize in advance because this might come out as a rant/venting.

I just graduate in April with a B.A in International Relations and a minor in Data Science. I realized too late in my education that I love statistics to switch to it as my major, so I decided to just go with what I got and try to find ways to make my education work in the job market. Well that hasn't gone well at all and right now im stuck in my current job as a Foreman on a crew installing ponds and waterfalls. While I dont mind the job, this is not what I want to do with my life.

Ive been playing with the idea of going back to my school to get a Masters in Applied Statistics for this winter, but I haven't had a chance to study at all for it (working 50-60 weeks in the heat kinda kills any motivation). 

So I guess I have two different types of questions I want to ask you guys.

If I choose to go the Masters route, do you guys know any good online resources to study so I can properly prepare myself for it? (even if I have to go much later then I originally planned on)

If I choose to NOT get my Masters, is there anyway I can do stuff from home to build my Resume up so I can actually land some sort of Statistics job? (like online certifications, projects, competitions)?

I appreciate any and all advice, sorry for the rambling. ",Need some career advice.,9cm64b,new,8,1,1,0
"My job gives me a professional development stipend, and I'd like to use it to take statistics course. My ideal course would be something that something that's a part of some university's extension program, so I wouldn't have to do any special registration beyond signing up. I'm open to a lot of potential subjects within statistics or probability as long as they're not 101-level. ",Recommended online statistics courses?,9clkh2,new,2,3,3,0
"- [Harvard](https://www.edx.org/professional-certificate/harvardx-data-science): this one is a professional certificate program

- [MIT](https://www.edx.org/micromasters/mitx-statistics-and-data-science): this one is a micromasters program


Which one do you think would be better? Im open to any other recommendation aswell.",Would you recommend Harvard's or MIT's data science programs on edX?,9cl3gx,new,11,25,25,0
,When to add ɛ when doing regression.,9ckwkn,new,4,0,0,0
"Hey there,


Following situation: I have 5 different campaigns on Facebook, and I have the following results:

A: Views: 120 Clicks: 7
B: Views: 110 Clicks: 8
C: Views: 90 Clicks: 5
D: Views: 115 Clicks 7
E: Views: 67 Clicks 4

Now I can use the Clicks/View ratio to determine the most effective campaign. But how can I check if it’s statically relevant?

And the more important questions, how do I calculate the amount of clicks and Views needed to a get significant result?",Sample Size: Determine the most effective advertising,9cjnp0,new,13,6,6,0
"Consider this thought experiment:

A standard hd monitor has 1920x1080 = 2073600 pixels. Each pixel utilizes 1 unsigned byte for 3 color channels (Red, Blue and Green). An unsigned byte ranges from 0 - 255, so it can take 256 unique values. So, we have: (256^3)^ 2073600 possible configurations of your monitor. To make future comparisons easier: 256^3 ~ 1.7e7 so (256^3)^ 2073600 ~ 1.7e 14515200 This is astronomical!!!!!!!

If we assume that the monitor has a refresh rate of 60fps, and every frame is unique, in a 100 years, the monitor takes roughly  60*60*60*24*7*52*100 ~ 1.9e11 unique configurations. This is astronomically smaller than my previous number (1.9e11 / 1.7e 14515200 is unbelievably close to 0).

Why did I do all of this? This is an estimate of what you has a human could perceive in your entire lifetime. In your entire lifetime, you won't perceive even 1% of the possibility space of what you could perceive.

All information that we has humans deal with is like this, audio, vision, to even language. All of these sensory data is non-stationary. Machine learning doesn't take my previous observation into account. Nor does it take my next crucial observation (which in my opinion is a key insight to develop something new into probability theory). Classical statistics and probability assume a stationary world. 

I have many intuitions that I have stumbled upon about how can you make sense of a non-stationary world and instantly adapt? My above observation is a key insight: in a non-stationary world the possibility space is enormously larger than the outcome space. This is key observation #1.

Key observation #2. In a stationary world, when your model generates an expectation, and in reality this doesn't happen, this event is special. It ""challenged"" your models belief about the world, and this is something you need record, and somehow update your model. In essence, this event is ""something you should learn a lot from"" and ""pay attention to"". If on the other hand you had a strong expectation about something and it comes to pass, then it should strengthen you, and again ""pay attention to this"". Everything in between in not noteworthy, and it's ignored. It's neither confirming nor discrediting your beliefs, a mundane event.

The problem with reinforcement learning, and statistical inference is that they can't make a) general rules and b) make exceptions to these rules. To get an intuition about what I mean, we know that all past tense verbs end in -ed (walked, talked, regretted etc.). But there are exceptions to the rule like sat, saw,threw etc. The goal of non-stationary statistics should be to find a balance between models that generalize and make special exceptions can find an optimal ration. Theoretically I could make a special exception to every event (this is what GOFAI was, at a high level). But this is useless, you will spend an eternity collecting rules to describe the most basic things (GOFAI is this, so are ontologies). Similarly if you try to have a single model generalize to all cases (this is what ML and reinforcement learning does) you will spend an eternity playing whack-a-mole. How can you strike a balance between these two extremes? In my view, use observation #2 to collect mis matches, and record them permanently. By observation #1, it's not going to be a massive set. This type of learning is philosophically different from what's currently being done. I am not able to rigorously describe what I mean by learning. It's not reducing the MAE. This is a different paradigm from correcting error (where it's by least squares, back propagation, or reinforcement learning, whatever, the philosophy is the same). I will eventually get it. The essence is striking a balance between beliefs (ie. statistical models) and exceptions to the rules (ie. strict rules from GOFAI). You want to minimize the size of both sets and fit the entire data (different philosophy from current ML!). This is the essence of learning, and you want to constantly find this balance in real time as you experience the world.

I think observation #1 is key, evolution noticed this and that's why our brains are exquisitely designed to handle an enormous amount of sensory data, and react to it in real time. There is a fallacy that this is because the brain is highly parallelized. This isn't true (My reference is ""On Intelligence"" by Jeff Hawkins. In that book, he cites more experimental sources to key experiments, but this is a well know fact in neuroscience). Neurons are unbelievably inefficient. Each one operates on 10Hz, and the entire brain utilizes 20 watts of power. Your brain ""does"" ""NLP"", vision, audition, motor movement and it hosts your entire personality and it does it effortlessly. You can do abstract statistics, and deep theories in this field and walk, and talk and and and. Compare this to the AI that Google or Apple siri are working on. Their models are trained on football fields of data centers non-stop. They are consuming unimaginable amounts of power, and they can't automate even the most basic things we can do. There is a fundamental gap between where the we are in probability and statistics. New ideas are required. A paradigm shift is required. I think my ideas are the stepping stones, and I'm looking for more. I don't think it is wise to go into neuroscience, this will be a black hole information that I won't be able to filter out. I just wanted to share some of my ideas, maybe someone smarter can take it further.



I didn't rigorously formulate everything yet. I'm still trying to conceptualize this. This process/algorithm is how to deal with disparate sensory information in a non-stationary world in real time, efficiently.","Why is vision learning (and by corollary audio, language etc.) so difficult and computationally expensive when the possibility space is so narrow? Advancing non-stationary statistics is a foundational problem of this century.",9ch7p2,new,5,1,1,0
"If my chance of success is 0.007, and i'm doing 100 trials, with replacement, what is my chance of at least one success?

​

I was able to find the probability of exactly one success: (100!/99!)(0.007)(.993\^99), but there has to be a better way to find at least one success than doing it over and over adding each up. (Is that what the CDF represents?)

&#x200B;

EDIT: 

SOLVED by u/conmanau thank you r/statistics",[Help] Binomial Distribution,9cgrs5,new,3,1,1,0
"Hi!

I am new to stats and needed help understanding these models. What is their purpose what do they tell us? I am workin on the problem below and could use help:


Billy wants to know if while number of hours slept increases, will stress significantly decrease. Participants report how many hours they slept ranging from 0-24 hours. They also report current stress level on Likert-scale from 1-7 (1= no stress, 7= extreme stress)

1. Write a hypothesized A to C comparion to test substantive question of interest.
2. Write null hypothesis in symbols
3. Write estimated model A given b0= 7 and b1 = -1

&#x200B;

&#x200B;

&#x200B;",Augments vs Comparison Models,9cg841,new,0,0,0,0
"I'm trying to compute the CDF for the multivariate distribution for high dimensions (N > 1000). All known algorithms are exponential in complexity, and the alternative is Monte Carlo methods. Monte Carlo is not suitable, since you can't really trust the convergence, and can't   quantify asymptotically what the error is. I've read through all the literature there is, and can't find a reasonable way to compute the CDF in high dimension at a known precision. 

Does anyone know of any approximation technique that can compute this accurately in high dimension with reasonable runtime and error?","Computing cumulative multivariate distribution in high dimensions accurately, in reasonable time.",9ceks6,new,18,6,6,0
"Terrible title, but I'm not 100% sure how to word this. I have a bunch of values, and I'm wondering how I would calculate the mean total value of a random sample of 8 of these known values. I could just have a computer simulate a large number of samples, and then take the mean of those sample totals, but I'm wondering if there's some formula that would give me a more exact answer.

&#x200B;

E: Forgot the part where you sum the values in the sample.",How to calculate average of random sample from known population?,9ceca4,new,8,1,1,0
"I recently completed a bachelor's of science in neuroscience with a minor in chemistry. My math background does not include multivariable calculus or linear algebra; however, it includes Calc I and II as well as introductory and intermediate statistics courses, mostly covering inferential statistics and hypothesis testing. Beyond that, I have extensive research experience that includes a bit of bioinformatics as well as programming (R, Unix, and Python) and data analysis.

​

I have only found one master's in applied statistics - Loyola Chicago - which does not explicitly require LA and calc III. I am wondering if there are others out there, not counting online degrees and certificates. Maybe there is a website or database where I can search? For what it's worth, I realize these subjects are valuable and I'm working through a graded open course in linear algebra offered by Austin Texas with the intention to do the same for Calc III. However, that may not be enough. I have found varied requirements for master's programs in data science: some schools require Calc III and LA to be completed at the time of admission; others at the time of enrollment; still others will allow an aptitude test (but I have not discovered a Statistics program with this option). Thoughts?",Are there Master's programs in Applied Statistics for those who may not have taken Calc III and Linear Algebra?,9cdxug,new,46,19,19,0
"Hey guys, I am a highschool teacher and the year 8's of my school have just filled out a google form deciding on who they would like to be their representatives for the year. They were asked to choose their first preference, then their 2nd, 3rd and 4th from 23 students (ridiculous i know). I was wondering how would I decide who were the 4 winners? I was thinking of 4 points for 1st, 3 for 2nd, 2 for 3rd, 1 for 4th. Top 4 highest scores are the winners  What do you think? Should this even be here or should I post this some where else?",Picking 4 Winners from 23 Candidates,9cbmwv,new,16,16,16,0
"Hello, sorry if this is a stupid question.

I am doing two peer reviews for some papers from the Computer Science department. I would like some advice or tips for what to look for in these papers. And what kind of comments should I be making.
Sometimes I don't know if the paper is truly unclear in some areas or if it's just me not having the knowledge.
",[Help] How to make comments when doing a peer review for papers that are not pure statistics?,9caora,new,4,6,6,0
"I am trying to find a way to measure the following scenario:

I have two computer algorithms which perform the same task, let's call them operation A and operation B. My goal is to find out which operation is faster and by about how much. I perform a bunch of timing tests on both and the results are pretty spread out (They appear to follow an exponential distribution if it matters). Since the actual amount of time each operation takes to run is dependent on a number of factors outside of my control and ability to measure, such as the other programs running on the computer and would vary from machine to machine, I was hoping that I could use this data to estimate the mean ratio of the mean time it takes to complete operation A to the mean time it takes to complete operation B and see if that is a useful statistic. But I'm not sure how I would go about calculating a confidence interval as my measurements of operation A and operation B do not pair.

Let me know if you need more information. Thanks.",Calculating a confidence interval for the ratio of sample means?,9c6rjs,new,13,1,1,0
"I can create a boxplot, but how do I make SPSS show the values for the median, 1st quartile, 3rd quartile, etc.?",Stupid question - how to make SPSS show boxplot value labels,9c67vz,new,0,1,1,0
"I am working on teaching myself gaussian processes. The plan is to eventually use either Scikit-Learn or another (mature) toolbox in Python but I want to make sure I understand it first.

Anyway, I have been searching the literature and not finding much on dealing with multi-dimensional data at different length scales.

For example, let's say I am working in 2D and x1 is in [0,1] but x2 is in [-1000,1000].

I imagine one way to handle this is in the kernel hyper-parameters but, ~~as far as I can tell, all of the ones seem to be radial and not account for the spread~~ (Turns out Scikit-Learn can do it but not sure if this is the best approach). Alternatively, I can manually scale the inputs by some a priori length scale (and then still fit the data scale in the kernel).

Thoughts? I've looked through most of the major references and didn't see anything about this (though I may have missed it)",Gaussian Process / Kriging with different length scales on the input,9c61g1,new,2,1,1,0
"TL;DR How do you estimate the population standard error of a regression coefficient based on random samples from that population?

I have a big data problem, specifically, my data is too big, and my PC can't load it all at once, so I'm trying to come up with a workaround. My first thought on how to solve this was to draw repeated random samples from the dataset, run regressions on each of those datasets, and then use that information to make my inference.

Correct me if I'm wrong, but I'm reasonably confident that regression coefficients are consistent, so I can just average the coefficients across the models and get a point estimate for what the coefficient would be, had I run the regression on the full dataset. However, I'm not sure how I would get an interval estimate on the coefficient, because standard error is a function of sample size, and obviously the random samples are intentionally a lot smaller than the full dataset. So I have no idea how to estimate what the standard error would be, were I to run the regression on the full dataset.

And, I'm not sure what to do about that. I can get an accurate point estimate for the effects of my variables, but I'm not sure how I'd say anything about significance. Any thoughts? I'd have assumed there was an easy solution to this since repeated random sampling is the core of frequentism, so hopefully you can set me straight, but Google hasn't been much help.",Random Sampling and Regression Coefficients,9c54a1,new,11,13,13,0
"Hello,

I'm not so good in statistics (I wish I were), but I've a statistic problem and don't have an idea about how to approach it. I've some local measures (15x21 points) of local velocities. By local velocities I mean velocity in each cell (represented by the magnitude :  [https://imgur.com/a/wNGLVea](https://imgur.com/a/wNGLVea)). With this data, I can compute the discharge (flux across the section). As we all know, the more data we have the more we're confident, and here we have enough data. The problem is this is a lot of measures (each case is a measure) and I want to measure (in next steps) in fewer points without affecting the result (the discharge) very much. So I thought that (maybe) there are two parameter I should optimize: the number of points and the position. In brief: What's the minimum points and their position that can give us the better discharge. I have no idea about how to approach the problem in a statistic way (I guess it's an optimization problem?). I could do it manually (test all the possibilities: remove one point, test all the combination, remove 2 pts test all the combination, etc) but I guess there is an other statistical (so faster) way to apply ?

Thank you!",Stat problem,9c42l2,new,6,1,1,0
"Hey everyone,

For a little practice using R and statistics, I want to calculate my softball team's odds of winning an upcoming mini-round robin tournament. I have the outcomes for all the games each team in the tournament has played this season. From that I have been able to calculate a distribution for points-for and point-against.

To calculate our odds of winning, my plan was to simulate the tournament a million times (or so). I would simulate each game by randomly drawing a number from each team's points-for distribution and which ever team had the higher number would be awarded the win.

    # PF = points-for
    # PA = points-against

    team_1 <- rnorm(1, mean=mean(df_team_1$PF), sd=sd(df_team_1$PF))
    team_2 <- rnorm(1, mean=mean(df_team_2$PF), sd=sd(df_team_2$PF))
  
    if (team_1 > team_2) {
    ## Team 1 gets a win
    } else if (team_2 > team_1) {
    ## Team 2 gets a win
    } else {
    ## Redraw in case of tie
    }

But then I realized I would be overlooking each team's points-against. I figured that I could address this by randomly drawing a number from each team's points-against and then subtracting that number against the opponent's points-for.

    # Calculate offense
    team_1_off <- rnorm(1, mean=mean(df_team_1$PF), sd=sd(df_team_1$PF))
    team_2_off <- rnorm(1, mean=mean(df_team_2$PF), sd=sd(df_team_2$PF))
    # Calculate defense
    team_2_def <- rnorm(1, mean=mean(df_team_2$PA), sd=sd(df_team_2$PA))
    team_2_def <- rnorm(1, mean=mean(df_team_2$PA), sd=sd(df_team_2$PA))
   
    team_1_score <- team_1_off - team_2_def
    team_2_score <- team_2_off - team_1_def
    
    if (team_1_score > team_2_score) {
    ## Team 1 gets a win
    } else if (team_2_score > team_1_score) {
    ## Team 2 gets a win
    } else {
    ## Redraw in case of tie
    }

Does this method make any sense?  I don't have great expectations for this little project but I would like to know if I am thinking along the right lines. Is there another method to calculate win-probs that I am overlooking?",How to forecast my softball team's win-probability using our (and our opponents) points-for and points-against?,9c3s4m,new,14,2,2,0
"Hi!

So I am applying for jobs right now as I'm on the edge of finishing my master's in biostatistics. I realize that a lot of offers apply to PhD holders. Right now, my university is offering a PhD programm in epidemiology but I wonder if that would help me? Is that a proper PhD program for a  biostatistician or would I be tied to epi as people want to employ biostatisticians with a PhD in biostats as well? Maybe they don't think working on non-interventional studies qualifies for clinical trials.

I don't think I would want to stay to stay in academia afterwards, so I'm certainly looking for something that would qualify me in the industry.

​

What are your opiniions and experiences?",Can I return to clinical trials with a PhD in epidemiology?,9c1h39,new,8,1,1,0
"Hi everyone! I am a rising senior in Statistics, and planning to do research with a professor who studies Functional Data Analysis (FDA) extensively. This subject is quite new to me, so I am concerned if I have the right prerequisites. I read a little bit, and I believe I need Calculus, but I wonder if I need Real Analysis as well. So far, for Stats, I've had one year of introductory course, Linear Regression, Probability Theory, Statistical Computing (R). Also, this semester, I am in a small research team working on Cluster Analysis (mixture model), I think I may get an idea of what it takes to do research. The professor I mentioned knows me quite well, and there is a high chance she would allow me to work with her. But, at the same, I want to make sure I can have meaningful contributions. I hope you guys can help me. It is good if you can tell me some of your impressions about FDA as well as its applications; or recommend your topics related to FDA I can look into.

Thank you so much. Any input is strongly valued.",What are some prerequisites to do research on Functional Data Analysis?,9c0mw4,new,5,5,5,0
"The wife and I are debating whether the  more you shuffle a deck of cards (by hand!) whether or not the deck becomes “more random” or “more shuffled”. One of us believes that if you shuffle over and over again you are increasing the true randomness of a given deal while the other believes that there is no such thing as “more random” and that shuffling once gives you the same level of randomness every time you shuffle. So who is right???

Edit: thanks for all the replies folks! I’m very lucky to have a spouse who can admit when she is wrong ;)",Improve my marriage by answering our stats question!,9c0675,new,18,21,21,0
"Hi, I know there are several links to look for programs but I have a weird situation. I'm graduating in May 2019 with a degree in Chemistry but I absolutely hate it. 

Last year I found biostats and fell in love with it. The problem is that most programs require a 3.5 gpa, calc 3 and linear algebra. I have a 3.3 gpa and only have calc 1 and 2, stats and probability with a strong background in programming (R, Python). 

Do you guys know of any good but not top programs that I might have a chance of getting in? Looking both for the US and Europe.","Help! I'm looking for good but not top 10 Biostats Ms/MPH Programs (US, Europe)",9bzr4f,new,3,3,3,0
"I am taking a calculus-based probability class, but I'm a tiny afraid my professor isn't great at explaining things. I like short, descriptive videos like PatrickJMT over the textbook (which is also not great imo). Has anyone found some useful videos? Thanks!",Video Tutorials for Intro to Probability Theory?,9bwjxv,new,5,16,16,0
"Hi all,

I know there's a link to this on the sidebar but I thought I'd post again seeing as that link is seven years old and not particularly helpful.

My background is that I have a BSc in Psychology and an MSc in Neuroscience, and I'm working as a Data Analyst in the UK. I'm currently learning linear algebra and I intend to pick up calculus and then probability theory once I'm comfortable with LA. I intend to apply to a masters' program next year so I'll hopefully have picked up enough by then.

I have a few questions about applying to a master's program- Firstly, how can I demonstrate that I meet the requirements for the course? I don't have an A-level in maths but I'm willing to do independent projects if that will help and I already know enough R that I don't think I'll have to invest more time learning it.

Secondly, is this a crazy idea? I already have an MSc and I'd be forfeiting a year of relevant work experience, pay, plus tuition fees for another one, so I don't know if it's worth it. Like many others I'm looking to break into data science but almost all of the job postings want a degree in maths, stats or physics, and I think having one would seriously help my chances.

Thanks, and sorry if this has been posted before.",Getting into a grad program,9bvm2x,new,12,7,7,0
"Hey all,

&#x200B;

I am attempting to fit a multivariate function z = f(x,y). I basically know that the function varies logarithmicaly with x, but have no idea how it relates to y. I've done a ton of guessing, from power series polynomials to complementary error functions. Nothing seems to give me better results than the basic f(x) relationship which, while good, isn't good enough. Is there a more analytic way to approach model fitting? 

&#x200B;

Thank you!",Model Fitting Inquiry,9bvg94,new,8,1,1,0
"I have 32 binary predictors for whether a certain thing is true for an individual with a highly skewed continuous outcome (range $300-$630,000 with mean 11,000 and median 2,000), n=4,300. The goal is to find important interactions between the predictors. I'm doing repeated (5x) 10-fold cross-validation on a 70% test set. 

I allowed for all pairwise interactions with regularization, testing values of alpha from 0 -> 1 and lambda from 0 -> 1. Alpha=0 provided the best fit, with values of lambda never making any difference for any value of alpha, which already concerns me. 

The final model only shrunk maybe 25 coefficients to zero out of 528 interactions and 32 main effects, which also concerns me as I expected more.

Anyway, what I am confused about is why the best MSE is 12,400 for the model with interactions whereas the one without is 10,000 (performance on test set gives the same trend). I would've thought the best lambda and alpha would've been in such a way to at the very worst, shrink all the interaction effects to zero and at least bring the model back to the main effects only model with a superior MSE.

Any idea what's going on? Perhaps my outcome variable is just too skewed or maybe the data just isn't good?",How is it possible that including interactions in a ridge regression make the model fit worse?,9bv6bd,new,8,8,8,0
"I am a software developer in my 40’s.  Overall I am pretty happy with my career, and the job market for my skills.

I have lots of experience working with data, and over the past two years have started to work with machine learning.  What I quickly found was that my math/statistical skills are a limiting factor for me.

I am at a stage of life where I have some extra time, and my employer has tuition reimbursement, so I just started going for a masters in statistics degree at a local university.

My question is, this is a untypical career/education path.  I got a bachelors in business 20 years ago, and have since spent 18 years doing software development.  Is adding a masters in stats at this point going to seem strange to employers and be a turn off?

Thanks for your responses.",Master in Statistics good idea?,9bv5mq,new,8,3,3,0
"I had a question about recommended higher level mathematics courses in preparation for a Statistics MS.

I'm currently a Software Engineering BS going into my last year. I'm planning on two minors (one in Stats and one in Math).

I've already taken...

\- Calc I/II, Discrete Math, Linear Algebra.

\- Applied Statistics (intro course for engineers), Design of Experiments, Math Stats, Non-Parametric Stats.

I am currently taking...

\- Multivariable and Vector Calculus, Differential Equations

\- Regression Analysis

I have two higher level math courses to take in my last semester. I was leaning towards Stochastic Processes and Advanced Linear Algebra. Any recommendations based upon what I've already taken? I'm currently looking into Spatial Data Analysis as a research area. I'd assume more Linear Algebra couldn't hurt?",Another Math Course Recommendation Question,9bupa9,new,7,9,9,0
"Hello everyone,

I have a question regarding the goodness-of-fit when using random effects estimator. What determines the goodness-of-fit in these models? I know that for (normal) linear regression you have to look at the R-squared.
I tried to determine the log-likelihood in the regression, but STATA  couldn't determine it because of group clustering I used. I tried to look at papers discussing this, but that wasn't really helpful either. 
There is the overall R-squared and the ratio of individual specific error variance to the composite error variance which I could look at.

Any ideas? ",Goodness-of-fit random effects panel data estimator,9bunmi,new,0,3,3,0
"I have a data set of a doctors office. I want to get the rate each doctor see patients. I have as the response column ""Total Patients"" seen. Each row is another day. I have made an indicator matrix of each doctor and whether he was there that day or not. 

&#x200B;

I did a linear model, but it poorly fit the data, even if it did fit the data well, I am not sure how I would get rates out of it.

&#x200B;

Any suggestions? 

&#x200B;",Inferring the rate doctors see patients.,9bu3qq,new,2,0,0,0
"Hello !

I'm not sure this is the right sub to ask this, but I'm looking for a community who could help me so...

There's the story: I would like to know if it's possible to make a statistical curve of a duration with this type of conditions.

Let's say I choose a basic duration of 3 days, once per day I roll a 4 faces die and the result can add or remove a day from the total:

1 = +1 day

2 = +2 days

3 = +0 days

4 = -1 day

When I reach the last day the test is over and I get the final duration.

​

Is it possible to make a statistical curve of this ? If so, how can I do that ?

Thanks a lot !

​

EDIT :

To make it clearer I would say you are in a waiting line, a really rubbish waiting line.

3 persons are waiting before you. Everytime it's the turn of a new person you roll the die. The result indicate how many guy leave or come in the queue before you (a rubbish waiting line haha).

In this case the question would be how many people would  I have waited for it to be my turn ?",How can I make a statistical curve with this sort of equation ?,9bti52,new,6,6,6,0
"[Screenshot of SPSS document.](https://imgur.com/aMSzSWT) I need to identify what type of variable CaOx and Gravity are and justify my reasoning but am unsure how to determine the type of variable. I also have to create some graphs but don't know what type of graph to use since they have 2 variables. 

Graph 1 needs to describe the relationship between specific gravity and osmolarity. 

Graph 2 needs to describe the relationship between calcium concentration (calc) and the presence or absence of calcium oxalate crystals (CaOx). 

I would appreciate any help, thank you. ",[Help] identifying types of variables in SPSS and descriptive statistics,9bsnin,new,6,1,1,0
"Hi everyone,

I’m new to time series analysis, so please go easy on me. 

I scraped social media posts from Reddit and coded the individual posts for several attributes (such as, for example, anger words). Now I want to test whether these attributes change over time.

I set the time of the first post of each individual as 0, so that the time since the first post is my independent variable and the attribute (i.e. anger) is my dependent variable.

Now if my independent variable was anything else than time, I could simply use least-squares regression. The way I understand it though, I won’t be able to reliably use a simple regression analysis because error terms are gonna be autocorrelated. 

Every solution for this problem I’ve found so far assumes that measurements are either evenly spaced out over time or that there is the same amount of measurements (posts) per individual (these vary between 1 and several thousand though!). 

Could anyone here point me towards an algorithm or correction for regression that works well with this kind of data?

Thanks in advance
","Need help in correcting for autocorrelation in uninterrupted, non-seasonal timeline analysis",9bjzd6,new,2,3,3,0
"I'm fairly new to SEM, and I'm trying to compare several models.  Three of the models are identical except for the paths- in Model 1 has a direct path from latent variables A and B to C; in Model 2 C has direct paths to A and B; and in Model 3 A, B, and C all covary.

I am running this with the lavaan package in r.  I keep getting identical model fit for the three models (CFI, TLI, RMSEA, AIC, BIC).  The other results- regressions and covariances- look like the models are running as intended.

I tried creating fake variables and running different versions of models with different causal pathways, and I'm always getting the same model fit results if the only difference between the models is the direction of the paths, so I know it isn't just the particular models I'm running.  I've been looking for answers; there are definitely examples in the literature of these types of model comparisons yielding different results

Is this normal/expected?  Is there something I am misunderstanding?",SEM Model comparison,9bj6ev,new,13,0,0,0
"Hi all,

&#x200B;

I have a dataset with 573 cases, split across 4 groups (N = 149, 144, 141, 139). I ran an ANCOVA with 4 different dependent measures, controlling with 2 covariate variables (so, 4 ANCOVA analyses in total).

&#x200B;

All data for each dependent measure, for each group, was normally distributed for ANOVA (within Skew = |1.5|; Kurt. = |3.0|). Homogeneity of regression slopes were not violated, when testing via an interaction hypothesis, however, my Levene's test was violated for each dependent measure.  


Where do I go from here? I understand that you have to do another analysis, but that is only if you also violate the regressions slopes, which I haven't.  


Any help would be fantastic, either advice or a link to an external source.  


Cheers in advance :)

&#x200B;",ANCOVA and a violation of Levene's.,9bhv4p,new,2,4,4,0
"Hey everyone,

I am doing an assignment which asks for me to create a histogram and ogive using some numbers for the observations my tutor has given us (there are 420 numbers - lowest is 28 and the highest is 92).

I've tried coming up with classes (I've come up with 9); 0-28, 29-37, 38-46, 47-55, 56-64, 65-73, 74-82, 83-91, 92-100. I'm now trying to come up with frequencies but I can't remember how to do it for this formula. I'm not exactly sure whether you need more information but please let me know.

I've only started Uni this year and this type of statistics I only learnt a couple of weeks ago. 

If you could help me out it would be greatly appreciate (I want to learn how to do this so I can do it next time).

Thanks!",[University Stats] Help!,9bgtwl,new,3,0,0,0
"This is something I have been thinking about, but got a C in college Statistics, so am not sure of the correct probability. I would appreciate help!

&#x200B;

Let's say there are two forks and a dignitary eats two meals with either of those forks as an option, with the other fork used by his/her counterpart. We can't be sure he/she used both of the forks--it could have been just one of them.

&#x200B;

What is the probibility on any subsequent meal that one of the same two forks was used by that dignitary?

&#x200B;

I was thinking it would be addition of probability, so 1/2 \* 1/2 + 1/2 = 3/4. Is this correct? Thanks!

&#x200B;","Two Forks, One Dignitary Problem",9bgnq6,new,9,3,3,0
"Hi experts,

I'm looking forward to predicting the XX-day customer value of current month with some accuracy.

The business model is simple: Customers register a trial account for a very small amount of money, and can then purchase weekly/monthly/yearly membership.

The XX-day customer value of August is calculated as:

Take all customers registered in August (from 0801-0831), and track their spending for XX-day after registration (e.g. James register in Aug 15th, track from Aug 15 - Sep 30 for 45-day customer value), and calculate the sum and divide by number of registrations in August.

I have tried a few methods:

1. Multi regression: It's easy to see that it is revenue / joins, so I tried to use a number of factors (related to revenue and joins, e.g. Do we have a promotion this month?) and do a multi regression. So far it's barely successful.

​

2) Time series: I tried exponential moving average but it's very bad, it doesn't have much seasonality.

​

Other info: We have about 4 years of data, and we are focused on monthly 30-day customer value at the moment. Another piece of info is that most products actually don't have a weekly option, so the only spending we are looking at for the first 30 days after registration is: either CX purchases a monthly subscription or a yearly, or nothing, and if they do purchase something they won't renew for the first 30 days (because you only renew at expiration, that is after at least one month/year)

Can you please give me more ideas about this one? Thank you\~\~",Help with predicting XX-day customer value,9bg4cl,new,3,7,7,0
"Is it fair to conclude that as significant p values get close to .05, the chances of the null hypothesis being true increases ([Sellke et al., 2001](https://faculty.washington.edu/jonno/SISG-2011/lectures/sellkeetal01.pdf))?

&#x200B;

If so, when repeated replications of an effect obtain p values close to .05 (i.e., hovering around .045), is this reason to increase or decrease one's confidence in the effect?",Should replications of the same effect that obtain p values around .045 increase or decrease confidence in the effect?,9bet41,new,4,1,1,0
"As Im learning more about statistics, I see that textbooks like to print problems where perhaps M is known but sigma isn't or visa versa. What I'm trying to better understand is how can that be possible in the real world? From my understanding, both of those values come from the population and so if you can compute a mean, then why couldn't you compute a standard deviation and know what it is. I do understand there are things like population and sample statistics, this isn't about that, it's more about how can a population mean be known but the population sigma is unknown, or you know the population standard deviation, but aren't actually sure of the true mean. How would you have known the population standard deviation ahead of time, but not the population mean. ",How can M be known and sigma be unknown?,9be1vt,new,9,6,6,0
"Hello there. So my research concerned looking at how proximity to a ""site"" (in this case a pesticide-sprayed field) affected the diversity of beetles found (# of species). I.e. samples were taken at 100m increasing intervals away from the pesticide field for a total of 1km.

My question regards which statistical analysis would be most ideal for my study. My hypothesis is that with increasing distance from the pesticides the number of species found will change. How do i assess whether my results are significant, and is there any other specific analyses that might be beneficial to conduct? 

Thanks for reading :)
",Measuring proximity to a site and the effect on another variable,9bdjs3,new,5,2,2,0
"The book is called **An Introduction to Data Analysis Using MINITAB 17 5th Ed**. by *Kathleen McLaughlin and Dorothy Wakefield*.  My school has the wrong book listed online, the syllabus reads an older edition that I can't find any where online either.  All of the Amazon pictures look the same and I don't trust those sources.

&#x200B;

Thank you in advance!",Looking for a statistics book for one of my college classes by Mclaughlin and Wakefield (Tri-State NY/NJ/CT),9bc2l8,new,2,6,6,0
"Hello all,

I come bearing a statistical question. First come context, I am quantifying proteins in using various conditions. I want to know if the quantification reproducibility is different between one condition and the others. What I have done so far, is to take the relative standard deviations (RSD) of each protein quantified in a condition over 3 replicates. My plan was to do a T-test or an anova test to look at the differences; however, the distribution of RSD's is right skewed. Thus, I cannot use a typical t-test or anova test. 

&#x200B;

What are my options?  

Log transformations seem to overcompensate and generate a slightly left skewed (although much more normal) distribution. Would a box-cox or other transformation be better?

Which parametric tests work best for this situation? Is there a major difference between one way, 2 way and 3 way anova?

&#x200B;

Any resources would be welcome as well. I need to learn more about data statistics for the future. ",Statistical test to compare Relative Standard Deviations between two conditions (Right Skewed distribution),9bb68y,new,10,3,3,0
"I'm looking at several independent variables (~20+) impact on two different, unrelated dependent variables. My goal is to see if there is any correlation between the independents and dependents. Ideally I would look at th independents as stand alone variables AND also as a whole.

Essentially impact of things like age, weight, etc. on a specific medication dose. I would like to be able to say something like (age x 1.3) + (weight in kg x 0.87) + (etc if other factors are correlated) = estimated dose in mg.

What's the approach here? Any example studies to follow? Can this be run on Vassarstats or is there a better website?",Help with picking correct test and where to run test,9bawrb,new,2,3,3,0
"I'm taking a class on DOE and Bayesian statistics, I'm missing some pre-requisites like probability theory and statistical inference.

I'm currently trying to catch up on those from online sources like MIT opencourseware.

Are there any recommended online sources for learning DOE?",Recommended sources for learning Design of Experiments and Analysis,9b9kph,new,12,20,20,0
Title says all. I am looking for an introduction to probability theory which also teaches the necessary measure theory,Looking for introduction to probability theory and just-enough measure theory,9b9gkr,new,7,7,7,0
Having trouble distinguishing these clearly so if somebody could try and explain it as simply as possible that would be fantastic - very new to statistics ,"What is the difference between the test statistic, the critical value and the p-value?",9b8y3k,new,13,3,3,0
"Hey Guys,

I've taken classes in Linear Algebra, Calculus and Probability Theory, and am fairly familiar with these subjects. However, I'm working through Elements of Stat Learning, and one of the first exercises is deriving the Matrix Formula from RSS. I've never taken a derivative of a matrix before and I've never seen Lin Alg and Calculus integrated in this way. I was also talking to someone else about how the dot product is related to covariance/correlation. Once again, I've never heard of Lin Alg being integrated with Prob Theory/Statistics like that.

I'm wondering what resources you guys would recommend to specifically learn about these connections (it's fine if they re-teach lin alg or something but with these connections as a focus)? Any coursera courses, textbooks, lectures? Thanks guys!","Books integrating Linear Algebra, Calculus and Probability Theory?",9b58bl,new,6,31,31,0
"For self-experimentation purposes, I'm interested in trying to quantify 'personal productivity' in some sense. I thought it would be a latent variable, but a large personal dataset failed to yield any trace, so I'm backtracking. What if 'productivity' is more like 'MP' or 'mana' in a game, where it can be larger or smaller but must be spent on various things in a zero-sum way? This would not yield a simple general factor where 'a rising tide lifts all boats' or necessarily any kind of hierarchical factor model either. I've been unable to find a model which corresponds to this. Does anyone recognize this setup?

In your standard latent variable like a measurement error model, an unobserved hidden variable is postulated which increases or decreases a set of observed variables (or another latent variable) to variable-specific degrees. So the variables tend to move together, in what you might call a positive-sum way. Intelligence is the classic example: scores on all tests of cognitive ability tend to increase or decrease together to some degree.

One alternative to a latent variable model is an index variable, where variables are simply added up and it is the sum which affects other things. Index variables aren't discussed much, so it's hard to find how to work with them, but the summing up appears to be done by fiat based on theoretical expectations about the relevant scales & weights, and then used in the rest of the model as simply another variable. It's unclear to me how you would *infer* an index variable if you didn't already know the weights.
Further, the variables might affect each other by 'using up' in some stochastic fashion.
Is there any way to infer the distribution of a latent index variable, or whether it exists, or how it's allocated among multiple outputs?

---

In Quantified Self, when we run experiments or analyses, the question of measurements is often a big one. You can use something like Cogmind or dual n-back to measure effects, but this is rarely what you want and delivers a precise answer to the wrong question. What you want is something like 'personal productivity'. My feeling has always been that it seems like productivity (let's call it P) waxes and wanes on a roughly daily basis. Some days P is high and you get a lot done, and some days it feels like it's all you can do to watch cat videos on YouTube. So, P looks a lot like _g_: it's a latent variable which additively influences a variety of measurements, perhaps to different degrees (different P-loadings), but you should be able to measure it by simply collecting a bunch of variables for a year or so, extract the biggest factor, and now you have a reliable global measure of your functioning which will be *much* more efficient than ad hoc experimenting on loosely-connected narrow variables. The lack of any P-factor might even explain the often unimpressive effects of things like stimulants, where the objective measures on things like reaction-time are so much less impressive than the subjective impressions.

So, I collected a bunch of data: number of emails sent/received, window-tracking logs split into 10 categories like IRC vs Emacs vs videos, number of patches in Git repos (a proxy for both coding & writing for me), commandline activity, daily self-ratings, number of Wikipedia edits, entries in a daily checklist of things to do, etc. I cleaned missing data as much as possible, imputed where relevant, transformed everything into normality for stabler fitting, deleted variables of low quality, checked for oddness, and plugged it into lavaan/blavaan and... Nothing. There was no general factor. There were a few factors which picked up clusters of specific related variables (often measuring the same thing, eg time spent in IRC client/# of IRC lines), but I couldn't find anything like the P-factor I expected to account for half or more of variance.

After thinking more about this and introspecting, it occurred to me. Often on the best days, activity is skewed. On a good day, I might get a bunch of everything done, but on, say, a good writing day, I might well simply skip going to the gym entirely because I'm in flow; I probably won't be answering too many emails; and I may or may not spend time reading a book. This didn't look so much like a P-factor at all. There might even be *negative* intercorrelations, simply because there's only so much time/energy in a day and time spent doing one thing is time spent not doing another thing. The metaphor which occurs to me is that of 'mana' or 'MP' in video games like RPGs, where you get a total which you can then spend on each ability: there is a single variable which constrains all of the abilities, sure, but you might not allocate them evenly, you might deliberately dump almost all of it into a specific ability, and another player might focus on a different ability or subset of abilities, and so on, and if you were to naively factor analyze a bunch of such characters, one would conclude that there is an negative correlation between strength and magical ability and that there is no general factor of mana and so on, all of which would be wrong. So in this model, each day you get a certain random amount of mana, and this mana gets spent on several possible outcomes in an uneven way.

Unfortunately, I can't find anything which really corresponds to this. It's *somewhat* like index variables in SEMs, but such indexes seem to be assumed by fiat, not inferred or modeled. One can simply standardize & sum up the variables and proceed regardless, but that provides no apparent way to check that the index is meaningful or corresponds to any latent mana in the first place. From a decision-theory perspective, since the end-purpose is optimization, constructing an index variable is optimal since by definition if your experiments don't increase the things you value, it is of no value that it might be increasing some hypothetical P-factor, but it's hard to assign utilities to indirect measurements like these, the results may be of little value to third-parties, and the question of 'what is productivity, why does it vary so much day-to-day, and how can we increase it?' is of interest in its own right. The uneven allocation reminds me of nonparametrics, but of course latent Dirichlet analysis doesn't work here; the Indian & Chinese buffet processes sound more similar, but still not what I need.

I can probably write something like the below toy model in Stan (all of it, including the stickbreaking or Dirichlet simplex, should be doable in a differentiable form), and then do Bayesian model comparison to a simple linear model with the observed intercorrelations, which doesn't really prove the existence of a mana variable but would at least suggest it's not worse than a baseline model with no latent variables, but that feels very ad hoc and I'd rather not. So, any ideas?

---

Below I present a simple generative toy model of how such a zero-sum latent variable model could work. For each day (row of _k_ observations, _k_ defaulting to 10 for easy viewing), defaulting to 3 years in my example:

1. we draw a latent variable, 'mana', which determines how much total we have to 'spend' (arbitrarily set to `N(100,15)`)
2. we do a stickbreaking-like process to decide what fraction of mana each of the _k_ categories of observations get that day: the first one gets ~1/5th of the mana on average, the next gets ~1/5th of whatever is left. (1/5th simply gives a roughly plausible feel to how many categories are active each day and a nice imbalance; this could also be done differently, with uniform distributions or on a simplex or whatever, without affecting anything important, I think)
3. the weights are permuted
4. each _k_ observation gets its fraction of the mana
5. for some realism, we add some `N(0,5)` noise to each observation (a fraction of the original mana variance)
6. for some more realism, most productivity measurements are inherently positive (you can't send -1 emails or go to the gym -1 times), so finally we constrain everything to be non-negative

Code below; as expected, it shows minimal intercorrelations between the 10 variables, and the factor analysis yields nonsensical results which essentially pick up on all the individual variables or one variable at random (depending on which of the disagreeing criterion for number of factors you pick):

    set.seed(2018-08-23)
    zsSim <- function(k=10, manaMean=100, manaSD=15, verbose=FALSE) {
    
        mana <- rnorm(1, manaMean, manaSD)
        manaWeights <- rbeta(k, 1, 2) # peak at ~20%
        ## stickbreaking-like process: each Kth variable gets a fraction of whatever is left over from the previous K-1th variables
        manaFractions <- numeric(k)
        for (i in 1:k) { if (i==1) { manaFractions[i] <- 1 * manaWeights[i] } else { manaFractions[i] <- (1-sum(manaFractions[1:i])) * manaWeights[i] } }
        ## and shuffle so every variable index has a chance of being the winner:
        manaFractions <- sample(manaFractions)
        manas <- mana * manaFractions
        noise <- rnorm(k, mean=0, sd=5)
        observedK <- pmax(0, noise + manas)
        if (verbose) { print(cat(""mana fraction sum = "", sum(manaFractions))); print(data.frame(mana, manaFractions, manas, noise, observedK)) }
        return(c(mana, observedK)) }
    zsSim(verbose=TRUE)
    # mana fraction sum =  0.9822376563NULL
    #           mana  manaFractions         manas         noise     observedK
    # 1  80.67062075 0.244110724694 19.6925636922 -1.8044969274 17.8880667648
    # 2  80.67062075 0.337299009511 27.2101204748 -2.8833943254 24.3267261494
    # 3  80.67062075 0.106969587558  8.6293030294  2.4519340066 11.0812370360
    # 4  80.67062075 0.038686279033  3.1208461440 -1.8644193958  1.2564267482
    # 5  80.67062075 0.009046399910  0.7297786962 -1.2547776674  0.0000000000
    # 6  80.67062075 0.008968339185  0.7234814891 -3.5647215933  0.0000000000
    # 7  80.67062075 0.011288962516  0.9106876138  5.2570430302  6.1677306440
    # 8  80.67062075 0.076767488915  6.1928809840  1.4026349717  7.5955159557
    # 9  80.67062075 0.043309055577  3.4937683974 -0.7777781983  2.7159901991
    # 10 80.67062075 0.105791809402  8.5342909345 -7.8818836301  0.6524073044
    #  [1] 80.6706207475 17.8880667648 24.3267261494 11.0812370360  1.2564267482  0.0000000000  0.0000000000  6.1677306440  7.5955159557  2.7159901991  0.6524073044
    
    zsSamples <- function(n=3*365) {
       df <- as.data.frame(t(as.matrix(replicate(n, zsSim()))))
       df$Total <- rowSums(df[,2:11])
       colnames(df)[1] <- ""Mana""
       return(df) }
    
    df <- zsSamples(); head(df); library(skimr); skim(df)
    #           Mana           V2           V3            V4           V5          V6           V7           V8           V9          V10          V11        Total
    # 1  85.95998192 10.217891405 21.302955523  5.7183615417 24.482058824 0.000000000  4.934269306  0.000000000  9.056144993  5.475358537  8.895966577  90.08300671
    # 2  97.31998793  4.696202516 28.118715139  0.0000000000  0.000000000 0.000000000  2.831360574  4.646360925  0.000000000 46.966954472  8.419743164  95.67933679
    # 3  99.09208823 55.838817955  5.716622158 15.7976175665  1.981887659 7.218705793  0.000000000  0.000000000 13.161102725  0.000000000 19.131188352 118.84594221
    # 4 115.84031989  2.692868221  0.000000000 15.4021610995 12.252356943 0.000000000  1.335282060  6.082977984 70.962209257  9.029501884  1.251178728 119.00853618
    # 5 102.82656990  5.654245118  0.000000000  0.4575808017 17.677875061 0.000000000 13.739800959  0.000000000 29.584056032  0.000000000  7.379633772  74.49319174
    # 6 111.87671714  0.000000000  0.000000000 46.0825241332  2.072741551 3.010552616  0.000000000 51.449334396  8.368490798  3.275699184  6.760038518 121.01938119
    # Skim summary statistics
    #  n obs: 1095
    #  n variables: 12
    #
    # Variable type: numeric
    #  variable missing complete    n   mean    sd    p0    p25    p50    p75   p100     hist
    #      Mana       0     1095 1095  99.72 15.23 52.56 89.98   99.68 109.96 148.42 ▁▂▅▇▇▅▁▁
    #     Total       0     1095 1095 106.87 19.29 43.27 93.67  107.11 120.38 159.87 ▁▁▃▆▇▆▂▁
    #       V10       0     1095 1095  11.56 16.11  0     0.082   5.54  14.36  94.81 ▇▂▁▁▁▁▁▁
    #       V11       0     1095 1095  10.53 15.87  0     0       4.67  12.71 100.88 ▇▂▁▁▁▁▁▁
    #        V2       0     1095 1095  10.77 15.95  0     0.1     5.37  12.97 107.02 ▇▂▁▁▁▁▁▁
    #        V3       0     1095 1095  10.78 15.44  0     0.27    5.12  13.15 113.54 ▇▁▁▁▁▁▁▁
    #        V4       0     1095 1095  11.42 16.28  0     0.33    5.15  15    107.75 ▇▂▁▁▁▁▁▁
    #        V5       0     1095 1095   9.97 14.84  0     0       4.82  12.12  98.24 ▇▁▁▁▁▁▁▁
    #        V6       0     1095 1095   9.98 15.64  0     0       4.63  11.77 116.35 ▇▁▁▁▁▁▁▁
    #        V7       0     1095 1095   9.77 14.21  0     0       4.55  12.26  92.47 ▇▂▁▁▁▁▁▁
    #        V8       0     1095 1095  10.9  15.88  0     0       4.84  13.3   94.37 ▇▂▁▁▁▁▁▁
    #        V9       0     1095 1095  11.2  15.98  0     0.06    5.08  13.53  96.24 ▇▁▁▁▁▁▁▁
    
    round(digits=2, cor(df))
    #       Mana    V2    V3    V4    V5    V6    V7    V8    V9   V10   V11 Total
    # Mana  1.00  0.09  0.10  0.10  0.07  0.07  0.12  0.09  0.08  0.05  0.13  0.75
    # V2    0.09  1.00 -0.08 -0.13 -0.07 -0.11 -0.11 -0.08 -0.12 -0.07 -0.12  0.10
    # V3    0.10 -0.08  1.00 -0.12 -0.11 -0.10 -0.05 -0.09 -0.09 -0.13 -0.08  0.11
    # V4    0.10 -0.13 -0.12  1.00 -0.07 -0.10 -0.06 -0.10 -0.12 -0.10 -0.05  0.16
    # V5    0.07 -0.07 -0.11 -0.07  1.00 -0.11 -0.07 -0.12 -0.07 -0.09 -0.09  0.12
    # V6    0.07 -0.11 -0.10 -0.10 -0.11  1.00 -0.10 -0.08 -0.10 -0.06 -0.09  0.13
    # V7    0.12 -0.11 -0.05 -0.06 -0.07 -0.10  1.00 -0.07 -0.12 -0.10 -0.05  0.15
    # V8    0.09 -0.08 -0.09 -0.10 -0.12 -0.08 -0.07  1.00 -0.10 -0.12 -0.12  0.10
    # V9    0.08 -0.12 -0.09 -0.12 -0.07 -0.10 -0.12 -0.10  1.00 -0.09 -0.09  0.10
    # V10   0.05 -0.07 -0.13 -0.10 -0.09 -0.06 -0.10 -0.12 -0.09  1.00 -0.12  0.11
    # V11   0.13 -0.12 -0.08 -0.05 -0.09 -0.09 -0.05 -0.12 -0.09 -0.12  1.00  0.16
    # Total 0.75  0.10  0.11  0.16  0.12  0.13  0.15  0.10  0.10  0.11  0.16  1.00
    
    library(psych)
    nfactors(df[,2:11])
    # VSS complexity 1 achieves a maximimum of 0.81  with  10  factors
    # VSS complexity 2 achieves a maximimum of 0.84  with  10  factors
    # The Velicer MAP achieves a minimum of 0.02  with  1  factors
    # Empirical BIC achieves a minimum of  294.33  with  5  factors
    # Sample Size adjusted BIC achieves a minimum of  661.74  with  5  factors
    # ...
    
    fa(nfactors=1, df[,2:11])
    #  Warning: A Heywood case was detected.
    # Standardized loadings (pattern matrix) based upon correlation matrix
    #       MR1      h2     u2 com
    # V2  -0.08 7.1e-03  0.993   1
    # V3  -0.04 1.3e-03  0.999   1
    # V4   0.00 9.6e-06  1.000   1
    # V5  -0.05 2.4e-03  0.998   1
    # V6  -0.05 3.0e-03  0.997   1
    # V7  -0.01 2.2e-04  1.000   1
    # V8  -0.08 6.9e-03  0.993   1
    # V9  -0.05 2.3e-03  0.998   1
    # V10 -0.08 7.1e-03  0.993   1
    # V11  1.01 1.0e+00 -0.012   1
    #
    #                 MR1
    # SS loadings    1.04
    # Proportion Var 0.10",[Question] Models for estimating latent index variables with zero-sum measurements?,9b4lxf,new,5,8,8,0
I have been reading the Wikipedia explanations over and over and I don't get it. Please help me understand how these work. ,ELI5: bray curtis dissimilarity matrix and UPGMA clustering,9b4jqi,new,2,1,1,0
"Hello all.

​

The survey: Our survey asks people about their experience with a single product, and then we compare the difference between each product.

​

The sample: An online panel of folks who take surveys for points/small monetary incentives.

​

The concern: We are considering weighting, but we worry that each product actually has a different demographic profile. We are worried that by weighting our survey to control for sampling bias, we invite additional error as one company may actual have a user base of a particular demographic. On the other hand, I worry that we introduce a lot of bias if we do not weight, as it ruins the generalizability of the data.

​

My question: When you're not sure what demographic distribution each group *should* have, is it still reasonable to weight to something like the census? What we know for certain is that our distribution of users does not match census distributions (to be clear, our actual customer distribution, not our sample distribution). I worry that online surveys themselves are more likely to get responses from specific demographic groups than others as well, and weighting helps fight this effect.

​

I'd love to hear how other people have solved for this.

​",Weighting an online survey with a lot of unknowns,9b3ulu,new,0,1,1,0
"Hey all,

I am an engineering student with limited background in statistics. I would be hyper-appreciative of any help I could get on this subject! I am well enough versed in linear algebra to understand singular values, orthogonal matricies, SVD, etc.. I am doing this project on an internship, so unfortunately I don't have any academic support.

I'm currently working on a problem where I am given three outputs based on four variables with 2064 experiments, so I have seven columns yielding a 2064x7 matrix. I have performed PCA in MATLAB which yielded seven principle components. I recognize by the cumulative ratios of singular values to the sum of singular values that 81% of the variance is captured in the first two principal component vectors.

Taking PC1 for example:

PC1 = \[-0.0005,  -0.3375,  -0.5084,  -0.3578,  -0.0016,  -0.0095,  0.7068\]

which corresponds to variables x1, x2, x3, and x4, with outputs y1, y2, y3.

&#x200B;

My interpretation of this is that output y3 is most strongly affected by inputs x2, x3, and x4, while output y1 and y1 are only weakly affected and variable x1 is essentially irrelevant. Is this the right idea?

&#x200B;

Thank you!",Interpretation of PCA,9b3g2u,new,10,3,3,0
"I've been doing some MLE on some data in order to find the best fit for 3 parameters of a probit model (binary outcome). Basically I've done it the brute force way, which means I've gone through a large grid of possible parameter value sets and calculated the log-likelihood for each set. So in this particular instance the grid is 100x 100x1000. My end result is a list of 100x100x1000 log-likelihood values, where the idea is then to find the largest value, and backtrack that to get the parameters.

As far as that goes it seems to be the right way to do it (at least one way), but I'm having some trouble defining the confidence intervals for the parameter set I actually find.

I have read about profile likelihood, but I am really not entirely sure how to perform it. As far as I understand the idea is to take the MLE parameter set that one found, hold two of the parameters fixed, and the change the last parameter with the same range as for the grid. Then at some point the log-likelihood will be some value less that the optimal log-likelihood value, and that is supposed the be either the upper or lower bound of that particular parameter. And this is done for all 3 parameters. However, I am not sure what this ""threshold value"" should be, and how to calculate it.

For example, in one article (https://sci-hub.tw/10.1088/0031-9155/53/3/014 paragraph 2.3) I found it stated:

> The 95% lower and upper confidence bounds were determined as parameter values that reduce the optimal likelihood by χ2(0.05,1)/2 = 1.92

But I am unsure if that applies to everyone that wants to use this, or if the 1.92 is something only for their data ?

This was also one I found:

> This involves finding the maximum log-likelihood and then varying each parameter until the log-likelihood is decreased by an amount equal to half the critical value of the χ2(1) distribution at the desired significance level.

Basically, is the chi squared distribution something that is general for all, or is it something that needs to be calculated for each data set ?",Maximum Likelihood Estimation (MLE) and confidence intervals,9b27ts,new,24,7,7,0
"I am  using R,  and  I have two time  series,   and I want  to know  if they are correlated, which is  the pearson index  (which  is the strenghtness of the correlation)  and if they are invers correlated  or  directly correlated

How  can I do it in R?  

The  command   for the correlation?  ",How to find out if two time series are inversley correlated?,9b1c2u,new,13,5,5,0
"Hello all. I'm a grad school student who ended up in a program that heavily uses the SPSS program. While I have a decent knowledge of stats, and I've used SPSS before, I've never had to write or use syntax in any capacity. I'm working on a research project now and it's expected that I be able to run all the data and such using syntax and I have absolutely no idea where to begin. Are there any good resources or books that I can take to in my spare time to master this a quickly as possible! Thanks! I look forward to hearing from you!",Need to Learn How to Use SPSS Syntax ASAP,9b167c,new,7,4,4,0
I should preface this by saying I know very little about stats beyond the basics. I am looking for help with normalizing some data for a research project. I am trying to report on number of a certain kind of lawsuit per state and want to normalize by state population. The issue is that the data is over a 30 year period of time over which state populations have of course changed. Can anyone suggest a good way to do this?,Help with normalization of data,9b0idd,new,1,1,1,0
"I'm outsourced on an automotive company on data management, they've sent me some (43) CP and CPK reports for some processes on Excel, even the whole tests keep on the limits, none on these is with a CP or CPK acceptable (1, 1.33).

Due this, I reviewed the data and used formulas which are okay, then I transferred this data to MiniTab and on a ""normal analyze"" of the data, the CP and CPK's are acceptable on more than the half.

How could this happen? Is the formula used on MiniTab different? Does minitab have a data limit (my process had 50 tests on groups of 1, someone said that it has a limit of 20)?",Minitab vs Excel,9b0d37,new,8,14,14,0
"Hey r/statistics, I need some advice on how to analyze a disease dataset I've acquired. The dataset is ten years of Dengue Fever cases organized by year (column) and location (row). Along with the cases, there are two additional columns for the number of mosquitoes of one species and another species found in that year (column) and location (row). Thus for each year there are three columns: dengue cases, mosquito counts for species 1, and mosquito counts for species 2. I want to determine if the dengue cases are related to the mosquito counts, but want to somehow pair each year's cases with that year's mosquito counts, to understand the trends in time, rather than running separate tests for each year. 

I'm using Stata, so any advice would be greatly appreciated!",Advice on an epidemiology dataset,9b04y9,new,4,1,1,0
"Hello everyone, I'm looking for books which tackle estimators, hypothesis testing, simple and multivariate linear regressions,... 

In both Python and R (doesn't have to be the same book).

Ideally something with good mathematical explanations of the models and hypotheses and exercises.

Can you help me out?

Thank you",Resources for undergrad material in Python & R.,9azwrf,new,9,3,3,0
"I'm facing 3 problems in my current analysis (method: ANOVA)

&#x200B;

First of all, the groupsize is different. The groups have 57, 55 and 52 people in it. Is there a way to analyze (in spss?) if it is ""equal enough""?

Second, the genders are unqual. I have over 90% men in every group. Do I need to change something in my analysis or do I just have to write something about it in the discussion?

Lastly, is there a way to find out if my sample size is big enough? I heard the word g power some time ago but I'm not sure if thats the right thing... or how to calculate if my sample size is big enough","Groupsize differences, unequal genders and g power?",9aziam,new,1,1,1,0
How do you apply the Bonferroni correction if you are looking at running 2 ANOVAs each with 3 groups - would you have to adjust the alpha level of each ANOVA to .05/2=.025? Would the alpha level for each of the 6 comparisons need to be adjust to .05/6=.0083? Or would the new alpha level for everything just become .05/8=.00625?,Bonferroni corrections,9ayxc8,new,8,3,3,0
"Hi everyone. I'm curious whether anyone knows of any textbooks (in any subfield of statistics, on any level) which have great, theoretically-focused problem sets? Ideally they should be asking me to prove things more often than not, though some computational exercises are okay. I'm a master's student in mathematical statistics so I do know some stuff already, but I'm curious even if the book is entry-level -- I'm really thirsty for some cool problems.",Textbooks in statistics with great problem sets,9axspv,new,4,12,12,0
"I am analyzing dyadic data in a multilevel model using the gls() function in R. I pulled up the residuals plot and it's something I've never seen before. The plot seems to show a pattern of diagonal lines with a slope of about -2. My best guess is that this could be autocorrelation, but it does not look like the other plots I've seen online. Does anyone know what this is showing me? [https://imgur.com/a/HDNsnhu](https://imgur.com/a/HDNsnhu)

Thanks!",Residuals plot: Is this autocorrelation?,9axskn,new,7,10,10,0
"Hi there! I was hoping someone may be able to point me in the right direction as I’ve hit a statistical wall.

I’m wondering what a good alternative to a 2-way (potentially 3-way) ANOVA would be. Unfortunately, my data does not meet the assumptions as it lacks homogeneity of variance and is not normally distributed.

IVs:
-child gender (categorical)
-parent values (categorical)
-experience condition (possible variable/categorical)

DVs:
-Coordination (ratio)
-Success (ratio)
-Latency (ratio)

Essentially, I am wondering if parent values could predict any of my DVs.

Any help would be greatly appreciated :)!",Unsure which test to use,9axb37,new,5,5,5,0
"A group of students takes a PRE test with 50 questions. Each question is ranked by difficulty on a scale of 1 to 5. After the weighting is applied, let's say the PRE test has a weighted value of 250 ""points"". 

The students go through a curriculum. Then, they take a POST test with 50 questions and the same difficulty rating system of 1 to 5 is applied to each question. However, this time, the POST test's weighted value is 300. 

How would you compare whether or not students improved using a t-test? In other words, how would you adjust the scales to compare these data?","Repeat measures t-test on exam data, but pre and post exams have different scales. Ideas?",9auc2d,new,4,0,0,0
"I am working with panel data with n=30 and t=7.   I have 12 predictors.    I know that's a lot but the same issue remains if I arbitrarily reduce the number of covariates.  Basically the scatter plot shows an obvious positive relationship between the dependent variable and say x1.   However when I run my regression model, a negative coefficient is returned for x1, and on top of that is highly significant.  How can that be?  The correlation matrix I constructed earlier doesn't show any real multicollinearity to be concerned with.  Similarly, VIFs for all the x's are around 1 or 2.   Forgive me if this is a stupid question.   My training in statistics has been limited.   I greatly appreciate any insights and/or guidance ",How to interpret counterintuitive signs from my regression?,9au6qv,new,12,6,6,0
"Trying to figure out that if I have 7 variables, each with 49 possible states, how many possible states are there in the system of variables? <3","Easy question from one confused boi; 7 variables, 49 states each, how many states?",9au18m,new,5,1,1,0
" 

Hi there, I'm a bit confused about usage of Firth regressions, and I was hoping to get some input. (Full disclosure, I did post this on /r/AskStatistics first but didn't receive a response, so the mods said I could cross-post here).

​

I have a large dataset with many variables, and the outcomes I'm   currently interested in are binary, with simple scales as independent   variables and a couple of covariates.

​

In my first analysis pass, I analyzed (in SPSS) using binary logistic regressions for each of the scales of interest (plus 2 covariates). Everything seemed to go smoothly, except I noticed that some of my variables had massive betas and SEs. I realized this was due to complete or quasi-complete separation in some of my variables. Some research led  me to Firth logistic regressions, which can compensate for complete or quasi-complete separation.

​

I guess what I'm confused about is the following:

​

\-   Is it more correct to just run the ""standard"" logistic regressions, forget about the complete separation variables, and just report the   non-""separated"" significant results, or to use the Firth regressions?

\-   If using Firth regressions, is it more correct to run Firth  regressions  for EVERY variable of interest (for consistency), or only  those  variables with complete separation (as the non-separates don't  need  correection)

\- In SPSS, for the  logistic regression the main p value I was looking at to determine  significance  was just the p value for the independent variable amongst  the covariates  in the ""Step 1 Variables in the Equation"" box. For Firth, the SPSS  output also specifies a ""significance"" value above, for  the test as a  whole. Is it relevant whether this value is <0.05 (or  whatever alpha) if I'm only concerned with my one independent variable?

​

Thanks greatly in advance for any input, I really appreciate it!",Questions about Firth logistic regressions,9atdm1,new,0,1,1,0
"I have ranked preference data for 7 items. However, I have only asked respondents to rank their top four choices. So I have data that look like 1, 0, 0, 2, 4, 3, 0 across the 7 items.

I am wondering if I can use Friedman's test for analyzing this ranked data, or if there is a better approach. Essentially, will the introduction of zeroes into the data set mess up the test? I will be analyzing in SPSS when I return to the U.S. and have access to that software. Histograms of each of the items suggest the data are largely right-skewed (lots of zeroes, since respondents only rank their top four choices).

Thanks for your help. ",Analyzing Ranked Preference Data,9at4ar,new,0,1,1,0
"I am measuring the effect of scale on the number of correct answers to a question.

I have 2 question prompts for large scale & 2 question prompts for small scale.

To find my p value and r statistic, what test should I use?

Can I pool the 2 large scale question prompts and compare to another pool of the 2 small scale question prompts?

Thanks in advance for the help!",Wondering which test to conduct and how to conduct it for our data.,9asnh3,new,0,1,1,0
"Ignore for a moment the issues with NHST.

If a study returns a p value very close to .05 (assuming this is the cutoff one is using), what are some steps one can take to ensure the results are valid? Two that come to mind are: examining effect size and power. Are there any others?","If you are working in the paradigm of NHST, what steps should you take in response to a p value of .049?",9asepc,new,15,6,6,0
"Hi. I need to write two predictive supply and demand models, one for each, where I’m given monthly volume data from around 2008/9 to now on supply and demand volumes, which are very cyclical showing peaks around 5/6th month of each year. These volumes are by country  and therefore I could use variables like exchange rate, economic indicators etc., the choice is huge. The problem I’m having is coming up with a model that is able to forecast accurately from having only roughly 110 data points for each. I don’t think I can go toward machine learning with this kind of data, I had a stab at it and worked terribly. Does anyone have any pointers or examples for this kind of problem? Thank you ",Predictive supply and demand model,9ascgq,new,3,5,5,0
"I'm looking at some instruction/examples on A/B permutations testing for nonparametric statistics.  

In one example, they analyze the difference between the 80th percentile of the original dataset and the 80th percentile of the randomized permutations.  

Why would one want to do that?  I see the obvious rationale for evaluating a difference in mean, medians.. but under what circumstances would one want to see a difference in the nth percentile?

Thanks","[Q] non-parametric, permutations A/B testing",9as3z4,new,2,1,1,0
"  

Hello,

I’ve been doing some analysis regarding what factors might be correlated with a specific type of local government revenue. To explain, I’ll pretend I’m interested in examining what factors might be correlated with how much revenue a local government gathers in the form of parking tickets. Here is where I am at:

I have total amount of parking ticket revenue for each government included in the study.

I have the population of the government’s constituency for each government included in the study. 

With this data, I’ve calculated the parking ticket revenue for each government per capita.

For instance,

Government A : 1 dollar per person per year

Government B: 2 dollars per person per year

And so on.

What I’d to do is take the log of each of these values (parking ticket revenue per capita) to adjust for some skewness. 

However, I am running into trouble as some of the values (parking ticket revenue per capita) are less than 1 (e.g. each person pays 50 cents in parking tickets per year, on average).

Thus, when I take the log of these values which are less than 1 the result is a negative number.

In other studies I’ve seen utilizing log functions for the dependent variable (even when the value is less than 1)  they don’t report negative log values. 

Tl:dr

How do utilize log functions of a dependent variable where only *some* of the values are less than 1 (but greater than 0).

Thank you!",How to deal with the log of a variable where some values are less than 1?,9arjom,new,9,2,2,0
"For the “big” study this group says they hypothesize a drop in average number of adverse events by about 40%. I asked if they would like to detect reductions smaller than that, and they said they’d like to detect a 30% or larger decrease. The pilot data showed a 44% decrease which corresponds to an effect size of .83 when taking means and standard deviations into account of the groups.

So how do I figure out what the effect size is for the next study? Effect size is a function of mean and SD, so I’m thinking I just copy the mean and SD from the pilot study, but just make the mean for the experiment group 30% smaller than the control group instead of 44%? 

Is it that easy?","How can I use pilot data to plan sample sizes for the next study, when they want to detect a smaller effect size than the pilot data?",9aqukx,new,45,9,9,0
I just finished gelmans Bayesian data analysis and thought it was great. What would a good next textbook in the area be? Preferably something with lots of practical examples of the variety that gelman includes.,Where to go after Gelman's BDA3?,9apz17,new,15,15,15,0
"An illustration of my issue: 

For e.g. X is a hormone that affects both the growth of hair, feet and nails. 

&#x200B;

A case-control study was conducted with cases having a condition causing excessive hormone X. 

As such mean length of hair of case = 10cm, mean length of hair of control= 2cm 

Mean length of nail of case = 1cm, mean length of hair of control= 0.5cm

Mean length of feet of case = 20cm, mean length of feet of control= 15cm

&#x200B;

 t-test done for all variables each yielded p=0.01

How do I show that X affected hair more than nails and feet?

&#x200B;

Thanks!

Edit:
Dependent variable (hair nail and feet length are continuous)
Independent variable (hormone x is binary, yes and no)",Determining which variable is more affected,9aog8e,new,12,11,11,0
"Correlation And Causation By Example

http://blog.codonomics.com/2018/08/correlation-and-causation-by-example.html",Correlation And Causation By Example,9ao8rg,new,0,0,0,0
"Hi all,

&#x200B;

I'm having a bit of trouble here.  
I ran a study online with Qualtrics involving 4 groups. 3 groups answered 3 items related to 4 pieces of information (labelled A, B, C, D) - effectively 12 questions per condition.  


Now, with my data, there are a heap of blank spaces where participants in other conditions did not answer the same item as another in another condittion.  


For example, for condition 1, they answered Q1\_A (related to info A), however, this variable in SPSS has missing values, which are in another variable for condition 2 Q1\_A.  


How do I merge item answers say, for Q1\_A, into a single variable, instead of having 3 variable representing each condition (whilst having each value tied to it respective condition?)  


A bit of a hard question to ask, given the nature, but I'm at wits end.  


Thanks in advance.",Merging item responses into a single variable (SPSS),9al1gs,new,3,3,3,0
"Howdy, 

So I’m in the beginnings of a PhD in epidemiology, and although my field is already heavily stats related, I find myself more and more drawn to the type of work done by PhD level statisticians and biostatisticians. I’d love some advice on moving forward from anyone who’s in a similar situation. 

What’s the hold up of me just forging ahead and going for that career, you may ask? Well, three things primarily: 

1) My advisor has been very gracious and supportive trying to get me involved with her research before technically being enrolled, despite being incredibly busy. She is an epidemiologist and a primary care doc, so she is (understandably) more content/CVD health focused than on analyses or methodological advancement. How do I progress my desired skill set under her advising in this case? 

2) I do not have the math educational background (only masters applied stats courses and auditing of calculus 1,2). It appears that linear algebra and the full calc series are almost necessary to progress in the stats world. Honestly don’t see that going well to pitch to the department to pay for me taking those classes... 

3) Grant reviewers and employers already knock me enough for a humanities undergrad and epi masters. Would this tack not just appear as another case of waffling and uncertainty? 

**TL;DR:**PhD student in epidemiology looking to increase focus on statistics and programming for future career prospects. Advice? ",Any grad students from other fields also looking to combine/switch fields to a more statistics-based career?,9akyir,new,7,5,5,0
"To make a long story short:

I am doing a protein-ligand binding analysis (trying to make an analytical approach at least!)

We generated a huge correlation matrix of what amino acids are in contact with other amino acids at the same time throughout many simulations We then normalized how often they are so the contact ratio's are between 0 and 1.

&#x200B;

We generated the heatmap (below), and now we want to try to ""extract"" ""groupings"" of correlations. The matrix is 600x600 and the ticks are superimposed, hence why the axes look like black streaks.

&#x200B;

Is this possible? Is there a name for it? Much obliged!!!! ","How to ""make"" ""groups"" using a (huge) covariance matrix???",9aio0i,new,3,1,1,0
&#x200B;,What is a good tutorial for learning how to calculate sample size?,9aim6b,new,17,27,27,0
&#x200B;,i'm a psych phd student who wants to befriend statistics. I love the concept of statics; I just have hard time wrapping my head around the concepts themselves. I would appreciate hearing peoples' philosophies and motivations behind their love for statistics.,9aihf8,new,19,26,26,0
"Hey all. I'm interested in organizing a Reading Group on PGMs using Koller's textbook. I want to gauge if there are enough people who would want to join, so please let me know in the form below. I'd ideally like to keep it based in EST so one weekday and maybe a Sunday video call for an hour would work. I'm only asking for your City info to gauge the best time to accommodate some of the West Coasters. Thanks!

https://www.wolframcloud.com/objects/f544624d-b80c-4675-920c-6b3153aeac3a",Running a Reading Group on Probablistic Graphical Models,9ai9rz,new,2,0,0,0
https://scholar.smu.edu/datasciencereview/,"SMU's latest data science review journal is here, volume 3, from their master's program",9ahh02,new,0,2,2,0
"Currently in high school and need some help on my maths project - 

I plan on making a time series graph of 1970 and 2007 to prove that global warming is accelerating, the maths part I would like to know more is decomposing the time series with different methods (should I use the least squares method, filtering by moving average, seasonal differencing or would you suggest another?) 

If you dont mind, pls send me any powerpoints or notes that you think might be useful for me! thanks in advance",help with methods of decomposing temperature time series,9ago2a,new,3,3,3,0
"Hey,

I'm looking to do email interviews with people who are working on interesting statistics/ machine-learning projects for my site [Black Swans](https://www.blackswans.io).

If you'd like to do an interview, or know of somebody else who might be willing to, please let me know. 

Thanks,

Jack",Would Anybody Working on an Interesting Statistics Project Do an Interview for My Site?,9afkfh,new,2,2,2,0
"Can somebody help this statistics rookie?

Research:
-pre test
-intervention
-post test

 I was dumb enough to not match the data per respondent between pre and post test. So I can only do analyses based on group data.  

Question 1: I still want to measure whether gender (and later on education level, for that matter) has an effect on the effectiveness of the intervention. Do I use a Two-Way ANOVA?

Question 2: What if I matched pre and post test data for each respondent. How would I measure the effect (or lack thereof) of gender (and later on education level, for that matter) on the effectiveness of the intervention?

Thanks in advance!",[Question] Should I use a Two-way ANOVA?,9afiy3,new,5,9,9,0
"Assume I have  an experiment of three settings (xyz) and the same group of people undergoing these three settings. I get different results for each experiment, but I want to know whether it is the experiment variant influencing my result or not.   
What test should I use to find the answer?  


Thank you :) ",What test should I use for comparing the results of the same group of people in different settings?,9adcv8,new,8,4,4,0
"Hi all!

I am presently working on a write-up / vignette the delves into the [practical utility of the Kolmogorov-Smirnov Test](https://github.com/pmaji/stats-and-modeling/blob/master/hypothesis-tests/distributional-tests/ks_test.md) (KS Test for short). It is still in its very early stages (haven't even coded up the actual ks.test() call yet, but I'd appreciate any thoughts you might have on the various sections I do have completed at this time. Hope you like the custom visualizations I've build too!

In particular, I'd like to survey the community to get a better understanding of the following:

**What in your opinion are the strengths and weaknesses of the 2-sample KS test vis-a-vis other distributional / hypothesis tests?**

Thanks!

*edit: To add “2-sample” clarification. ",What are your thoughts on the strengths & weaknesses of the KS test?,9ad0dv,new,18,3,3,0
"I got an opportunity to collaborate with phd engineering student during summer to work on 3 papers that is going to be published in engineering journal or applied statistics journal, but not enitrely pure statistical journal. Is this a good achievement as an undergrad ? I am hoping to get to a good grad school for statistics phd. My contribution is on statistical portion of the paper where we had to run different parametric, semi-parametric and non-parametric tests in R. The data sets and other engineering stuffs were done by others. Does it matter what journal it is submitted? Also, if the paper is not published by the time I apply, how would grad school percieve it? Any advices are welcome. Thanks!",Journal as an Undergrad,9acra3,new,4,10,10,0
"Hi guys, it is the weekend, my statistician is on maternity leave and my abstract is due next week. Bottom line is this non-statistician needs to do her own stats. 

Can anyone kindly advise which test I should be using? 

Background: 

Case-control study (Case=A, Control=B), normal distribution. Comparing 4 variable (a,b,c,d) 

Paired t-test used to determine that a,b,c,d are different in both groups 

However, how should i analyse the data to show that the difference in a in both groups is greater than the difference in b, c and d in both groups?

&#x200B;

Thanks!",Quick question on stats,9acjyo,new,2,1,1,0
"Hi r/statistics! I am a Statistics student. Machine Learning is currently a hot field, and a large number of folks in my department are trying to get into. I myself also get to learn a bit of Machine Learning while working on some projects with my professors, and I have to agree that it is a very interesting field. However, I don't think Statistics is just about it, and would love to find out and learn more about some other cool areas in Statistics. I believe that way will help me understand the field better, and have more options in the future. Hence, I hope to hear and receive advice from you guys.  I am looking into Biostatistics, but I am open to know about others as well. Thank you so much.",What are some growing areas in Statistics that don't involve much Machine Learning?,9a9n3k,new,21,15,15,0
When is it appropriate to drop a covariate? How do you know if it's not helping reduce the noise with your main predictors?,When is it appropriate to drop a covariate?,9a8x6k,new,4,1,1,0
"question about variance 

if a and b are scalar numbers instantly picked up from Gaussian distribution of variance s, then can we derive a formula/estimate for the variance of c where c=(a+b)/(a-b)",question about variance,9a8dql,new,6,1,1,0
"Let's say I have two data sets each containing 1000 points. I want to get the respective averages of both data sets, then divide the resulting averages. What is the best way to propagate the error (e.g. SD) for this calculation?

Edit:
I was thinking of obtaining the SD for each average, i.e. Ave1 ± SD1 and Ave2 ± SD2. Then, I can propagate the error when dividing the two values, e.g. (Ave1 ± SD1)/(Ave2 ± SD2). Thus, the error, dC = (a/b)*sqrt((dA/A)² + (dB/B)²). Is this a good approach?",Error propagation in dividing the averages of two data sets,9a8b9l,new,11,1,1,0
"Here's   an example:

[https://stats.stackexchange.com/questions/241685/combining-time-series-data-from-multiple-individuals-to-get-a-estimate-for-the-p](https://stats.stackexchange.com/questions/241685/combining-time-series-data-from-multiple-individuals-to-get-a-estimate-for-the-p)

&#x200B;

Suppose I have *m* individuals from a population (not necessarily people: here ""individuals"" may also indicate machines of common design). *m*   is small, say, 10. For each individual I measure some quantity as a   function of time: the number of time observations (the sample size) is   different for each individual, i.e., *N*1,…,*Nm*

. **EDIT**: the sampling rate is the same for all individuals.

The  quantity in question doesn't (seem to) exhibit some trend: it  randomly  oscillates around the time mean, which may differ from  individual to  individual, with a standard deviation which is roughly  constant with  time (for a given individual). Just to be clear, the  oscillation  doesn't seem to be harmonic: it's random, but its amplitude  doesn't  seem to depend on time.

I have two goals:

​

* I  want to estimate the time mean and standard deviation of the   population. How do I combine the time series from the different   individuals to do that? Do I need to take into account the different   sample sizes, and if so, how?
* I  want to estimate the time mean and standard deviation for a new   individual, not observed. In other words, I would like to compute a   prediction interval for the time mean and the time standard deviation.   How do I do that with time series?

**EDIT**: here is a plot of a single time series for one individual.

​

[https://i.stack.imgur.com/yyfEm.png](https://i.stack.imgur.com/yyfEm.png)

​

Looking  at the standardized time series, it could seem that the  variation is  not small. However, consider that the oscillation is much  smaller than  the time average: I cannot show the actual values, but for  each  individual *j* the sample coefficient of variation

​

(**My question**:  he said   the oscillation is  much  smaller than the time  average....  how  he calculated  the time  average?  Is  he referring to the  oscillation of  each time  series, or of all time series)?

[https://image.ibb.co/far0A9/A\_uplo.png](https://image.ibb.co/far0A9/A_uplo.png)

&#x200B;

&#x200B;

Then he said:

I still miss two steps: how to compute in practice *neff*,*jj*  for each individual, and how to combine all these data to get an  estimate and a standard error for 1) the time mean of the population and  2) the time mean of a new, unobserved, individual.

&#x200B;

Can you explain to me please  what is neffjj?    Where  it  comes  out?

&#x200B;

What is the pratical meaning of   ''time  mean of population'' ?  (A visual example   would be  appreciated)

&#x200B;

Thanks  very mkuch

&#x200B;

&#x200B;

&#x200B;

​",Help with time series,9a7x5o,new,1,3,3,0
"I am investigating in statistics now.

Since I heard about different distributions of random variable I could not understand how was it discovered. Basically [this](https://stats.stackexchange.com/questions/361653/how-were-statistical-distributions-discovered) question on «Cross Validated» Stack Exchange completely repeats my thought pattern. It's unanswered. If you know the answer or have any ideas could you please share?",[Question] «How were statistical distributions discovered?»,9a6w7j,new,24,42,42,0
"I need to report the results of a multiple regression to people who work in my business who have little or no statistical background (including my boss too). I'm wondering how to show the results so that they can be understood easily. The report will be sent by email. I suppose that the best way to report the results of a regression depends on the particular fitted model (linear model, CART, GAM, etc..).  For the sake of simplicity let's consider multiple linear regression.

Personally, I would report the numerical output of the model (with coefficient estimates and standard errors, p-values, the R squared and F statistic, etc..) and I'd explain in layman''s terms what it means below it. Of course, I don't expect the audience to understand the numerical output on their own without reading my comments. But I think that it may be a good idea to report the output in any case along with the comments to make it clear what I exactly did.

What do you think? Would you show the results graphically instead? If so, how would you do it?

Thank you.",What is the best way to report the results of a regression to my boss?,9a5hyn,new,19,13,13,0
"In this scenario, there are 4 different events that can occur very rarely after an action has been taken.

The odds of the four events occurring (independent of one another) are as follows:

Event A: 1 in 3000

Event B: 1 in 4000

Event C: 1 in 6554

Event D: 1 in 6554

From what I understand, If I'd like to calculate the chance of event A happening at least once within 1000 actions, I would divide 1/3000 to get .0003333 repeating. Subtracting this from 1 would give .9996667 which is the chance of the event not occurring after each action. Then, raising .9996667\^1000 will give .7165, which means there is a 71.65% chance of event A **not** occurring within 1000 actions, and a 28.35% that event A will occur if 1000 actions are taken.

**What I really want to know is: What are the odds that any event (A, B, C, or D) will occur within 1000 actions, 2000 actions, 3000 actions etc? Essentially, in less mathematical terms, how unlucky would I have to be to not see any event happen at all within X number of actions. Or how lucky would I have to be to see all 4 events happen before X number of actions.**

The equation is probably much simpler than I'm thinking, but it would also help if someone could calculate it it for 1000, 2000, 3000, 10000. Any help is appreciated, thank you!!

​",Help with calculating the chance of any or none of multiple events occurring,9a4nzt,new,5,0,0,0
"I'm considering getting into graduate school, hopefully an affordable masters program. I'm looking into Data science programs and also Statistics programs. 

&#x200B;

I'd like to know what other people think of this? Pros vs Cons of each of these programs? Why are data science programs SO MUCH more expensive? Are they preferred more to a people with a traditional stats masters program?

&#x200B;

I've done a couple of online data science courses (DataCamp Career Track in R and Python), and wondering if this is close to a Data science masters program or should I just get a degree in Statistics? 

&#x200B;

I just feel like it's the next big thing and maybe I should get a job in this field to make a good living.",Difference between a masters program in data science vs statistics?,9a4fpp,new,4,3,3,0
"Hi,

I'm possibly being groomed for an economist role with the government. 

I have a Msc in business and economics and feel that I need to brush up on my stats skills, any MOOC recommendations? (we use SAS). 

&#x200B;

&#x200B;",Stats MOOC recommendations for budding government economist?,9a3qx1,new,0,2,2,0
"I've got a bunch of random questions associated with model selection and dropping/adding factors and covariates. The program that I'm using doesn't have any selection techniques so I'm doing everything manually. Quick rundown of my model, I've got: a continuous predictor (the main effect I'm interested in), a categorical blocking factor, and the interaction between the two. Additionally, I have two covariates; I didn't necessarily hypothesize that the covariates would impact the response variable (although I could see them having an effect) but decided to include them since I have the data anyways.

1. So, big picture. When I've been dropping factors my general progression has been: (1) drop covariates if they're not significant, (2) drop interaction if it's not significant, then (3) drop block if not significant. Is this the right way to go about selecting a reduced model? I'm using AICc to inform whether or not I should drop factors. 
2. Similarly, can I drop covariates that aren't significant? This is operating under the assumption that they weren't originally hypothesized about. 
3. What does it mean if a covariate is significant? It doesn't mean that the covariate has a significant relationship with the response, does it?
4. Say I have a situation where the covariate is not significant but one of my main factors is. Dropping the covariate causes that main factor to lose significance. Should I keep the covariate in then?
5. Question about AICc...say I have a full model that has a bunch of significant factors/covariates. Removing all of these factors and covariates (besides the main predictor that I'm interested in) provides the lowest AICc though. What should I do? Do I go with the full or reduced model?
6. I think that one of my covariates might be related to my blocking factor. Not sure how to phrase it, correlated wouldn't be the right would since my blocking factor is categorical. Anyways, one of the covariates that I'm looking at is individual size and the size is significantly different between my two blocks. Because of this, wouldn't block already take size into account? So is size a redundant covariate to have? How should I take this into account in my model?

Any help on any of these questions would be greatly appreciated. Thank you!",Dropping/including factors and covariates. Model selection?,9a2nr0,new,3,2,2,0
Sorry if this is a dumb question but what does it mean if factors are significant but the model as a whole isn't?,Factors are significant but model isn't?,9a2gbd,new,1,2,2,0
"I posted a similar question some time ago but I think this is a much better formulation of the problem.

A game is played at a computer, as follows:

There are two coins, one silver coin and one gold coin, in a box.
The computer “shakes” the box, and then tells the player whether the two coins landed on the same side or not. 
At this point the player must guess whether the coins are both heads or both tails (if they landed on the same side), or which of the two coins, silver or gold, is head (if they landed on different sides).

Now, if the coins were both fair, the player would just guess. The problem is that each coin can be of three types:
-	Fair coin (lands on head 50% of the times)
-	Biased coin coming up head 25% of the times
-	Biased coin coming up 75% of the times

The player does not know the exact probabilities; he (or she) only knows that the coins are not necessarily fair.

The game is repeated nine times with all the possible coin combinations.

During one game session the coins are tossed 32 times. The goal of the player is to make as many correct guesses as possible. 

Now, since this is a computer game, the sequence of results is predetermined. When I say that a coin comes up head 75% of the times, it means that the coin will be head 24 times out of 32 (again, the player does not know that, he only knows that the coins are not necessarily fair). Since the sequences are all predetermined, I can compare different players, as they will all go through the same sequences.

The problem has two parts.

**PART ONE**

**DATA**

My data is the sequence of coin tosses. In the following example:
-	the gold coin has 25% chance on landing oh head
-	the silver coin has a 50% chance of landing on head.

1: landing on head 
0: landing on tail

    Gold:  0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0
    Silver:0,1,0,1,0,1,1,1,1,0,0,0,1,1,1,0,0,0,1,0,0,1,0,0,0,1,1,0,1,1,1,0

**QUESTION**

I would like to model what an ideal observer would do. The ideal observer would start the game with a beta prior and the update it at each coin toss. I would like to see the updating at each toss, and what’s the final posterior after 32 tosses.

Is Stan a good way of doing this? I looked at the Rate1 example from the book Bayesian Cognitive Modeling: A Practical Course (2014) by Michael Lee and Eric-Jan Wagenmakers:

    // Inferring a Rate
    data { 
      int<lower=1> n; 
      int<lower=0> k;
    } 
    parameters {
     real<lower=0,upper=1> theta;
    } 
    model {
      // Prior Distribution for Rate Theta
      theta ~ beta(1, 1);
  
      // Observed Counts
      k ~ binomial(n, theta);
    }

In the book example k, the number of successes, is 5 and n is 10. The code calculates the posterior distribution centered at 0.5. In my case I would like to see how the prior is updated step by step.

I also looked at the example “Hierarchical Partial Pooling for Repeated Binary Trials” on the Stan website, but again it considers an overall rate of successes.

Another thing I would like to know is which initial prior (comparing different types of beta priors, or other kinds of priors) would yield the best score (out of a max of 32, when one is making the correct decision at each turn).

**PART TWO**

**DATA**

My data is the player’s actual choices. For example, the following is a list of a subject’s choices in the case we considered above, according to the following code:

The coins landed on the same side, the subject bets on heads: 1
The coins landed on the same side, the subject bets on tails: 0
The coins landed on the opposite side, the subject bets the gold is head: 1
The coins landed on the opposite side, the subject bets the silver is head: 0

The line after the subject's choices shows his score: 1 if he guessed right, 0 if he guessed wrong

    Gold:  0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0
    Silver:0,1,0,1,0,1,1,1,1,0,0,0,1,1,1,0,0,0,1,0,0,1,0,0,0,1,1,0,1,1,1,0
    Guess: 0,1,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,1,0,1,0,0,1,1,0,0,1,1,1,0,1,0
    Score: 1,0,1,1,1,0,0,0,0,0,0,1,1,0,0,0,1,0,1,1,0,1,1,0,1,1,0,0,0,1,0,1

**QUESTION**

I would like to reverse engineer the process: given the choice, what was the player prior? 
How did it change through the trials, and is the player biased toward the silver of gold coin?
",Guessing the probability of heads tossing two biased coins,9a1u9v,new,0,0,0,0
I would like to hear what kind of jobs people became with their Statistics degree but with Bachelors. ,What job did you get with Bachelor degree in Statistics?,9a1q7p,new,46,27,27,0
[https://academy.microsoft.com/en-us/professional-program/tracks/data-science/](https://academy.microsoft.com/en-us/professional-program/tracks/data-science/),Is Microsoft's online program for Data Science any good?,9a19eg,new,8,4,4,0
"This might be a common question or a common practice in some fields (which I am neither part nor aware of), or it might be absent in some, so I thought I might as well ask you. 

So a disclaimer - I am not a statistician, nor I have much knowledge of them, but I did to rely on them for some of my projects (under the guidance of those who know more than me). In one project I ran several canonical variate analyses (CVAs) using different combinations of principal components (PCs) to establish that there are clear 'classificatory' trends in the data. However, the classification of an observation to one group or another is based on posterior probabilities. I can image a situation where one might run 20 CVAs using different combinations of PCs and get pretty consistent results, but with posterior probabilities that are very ambiguous (e.g. with two groups you might get 0.54 and 0.46). So while the results may appear very clear cut, the assignment of an observation to either one of two group is based on only a slight preference for one. 

I am just wondering if 1) I am understanding the importance of interpreting the results of an LDA or a CVA using posterior probabilities and 2) whether if the practice of not providing posterior probabilities is not common but should be.",Should posterior probabilities always be published along with the classification results of an LDA or CVA?,99zqa8,new,3,1,1,0
"Apologise for the mistake in the title. 

Correction:
In an ARIMAX model, if you difference an explanatory variable to ensure stationarity, do you have to different all explanatory variables for consistency even if they're already stationary? ","In an ARIMAX model, if you difference one explanatory variable to ensure non-stationarity, do you have to difference all of the others for consistency, even if they're already stationary?",99yk6j,new,0,1,1,0
"Suppose I have a random variable [;S_x;], which takes values in [;\mathbb{Z_2};] and whose distribution depends on a parameter [;x\in\mathbb{Z}_2\^K;]. I'd like to estimate

[;Var_{x\sim Unif(\mathbb{Z_2\^K})}(E(S_x));]

that is, the variance in the probability that S_x=1 as we draw x uniformly from the hypercube.

What is the cleverest way to do this? Sampling from S_x is rather expensive (a single sample takes between 1 and 5 seconds already when K=10), so I really want to get the best test possible for this. I also would like some sort of confidence interval, to combine my estimate of the variance with other estimates to test my actual hypothesis.",Best way to estimate a variance of an expectation?,99yet4,new,1,1,1,0
[https://github.com/seiflotfy/pcsa](https://github.com/seiflotfy/pcsa),Open Research: Reducing the size of PCSA to that of a HyperLogLog,99wni7,new,0,1,1,0
"Hey everyone,

I want to learn how to properly use p and z-scores, t-tests and hypothesis testing in the context of Python but with a focus on the academic stats side. Are there any free sources people would recommend?",Any Recommended Online Courses for Stats in Python?,99vspq,new,10,34,34,0
"Hey,

Here's another tricky probability puzzle based on Connect 4: [https://blackswans.io/post/8/](https://blackswans.io/post/8/).

**Best solution wins Spotify Premium for a Year 🔥🔥🔥.**

Enjoy,

Jack",Can Anybody Solve this Probability Puzzle 😁 Based on Connect 4?,99vsif,new,0,3,3,0
&#x200B;,"Can Z score be used for non-normal distributions? If not, then how to approach similar probability questions?",99v17u,new,3,3,3,0
"I wasn't sure where to post because my question has to do more with algebra, but I'm on a problem in a book I'm working out of and this one little hiccup is stopping me from completing it.  

Formula for exponential smoothing forecast is: 
alpha*Yt + (1-alpha)*Ft

The time series goes to 13 and the problem wants to continue to expand the expression until it is written in terms of the past data values going back until period 8.  I'm using alpha = 0.2

Starting with F13 = 0.2*Y12 + 0.8F*12

I know F12 = 0.2*Y11 + 0.8*F11 so I can take that and plug it back into F13 to get F13 = 0.2*Y12 + 0.8(0.2Y*11 + 0.8*F11) = 0.2Y12 + 0.16Y11 + 0.64F11

The answer in the book is 0.2Y12 + 0.16Y11 + 0.64Y11 + 0.64F11.

I know my algebra is a little rusty but not sure where they pulled the 0.64Y11 from.  This is the only thing holding me back from getting the answer",Exponential Smoothing Forecast,99tzoz,new,1,6,6,0
Basically what the title says from the perspective of different disciplines: statisticians and econometricians.   It's very confusing  and should be taught to all students. Mandatory like ,"Discrepency between random effects, fixed effects, mixed models, and random coefficients terminology",99sqig,new,5,4,4,0
"Hey guys I always love reading all these posts and learning new things from you guys, and I never post anything on reddit so I thought why not here. I’m from New Zealand and I’m doing bSc with a double major in math and statistics, only in second year doing my third stats paper as there is only one first year paper and two second year papers. At the moment we’re using R doing permutation testing, randomised block design, nested designs and poisson (that’s next month ) last semester we used sas and just did the basic anova and permutation and stuff like that.
The real question with post is: any textbooks I can read over summer too strengthen my knowledge in the basic statistics along with the SS and variation calculations from RBD’s as sometimes it flies over my head? Also I’m doing a math major but my first algebra paper isn’t till next year so I’m wondering since there’s lots of summation and expectations in this class if there is a good book on that?

Thanks much in advance if anyone can help!!! Love this sub keep it up!",Second year statistics,99s01h,new,0,3,3,0
"Hi all, not sure if this is the right place to ask this question, perhaps you can point me to a better place if its not.

I have a data set that contains the ages of patients with given conditions. I am trying to create a simulation based off the data that I have. So for example for a patients who had an abscess I want to randomize the age of the patient based off the distribution of the ages in the data set, and then continue to simulate more patients with that condition.

I used SAS to fit either a log normal, Weibull, or Gamma distribution to the age of presentation for the conditions.

I am trying to pull the information from the output of SAS into Python so that I may randomly draw the age of a simulated patient.

I've tried to do some research, but alas I do not understand statistics well enough to solve my problem.

So lets take the log normal example for now. SAS spits out some parameters for the log normal distribution that I THOUGHT I could use in Python (mean 30.7628, std = 16.02414). However, when I try to plug these in to Python to have it draw a random number, it spits out a HUGE number (something\^23), which is obviously not what I want given that the my mean is 30, and I need real life ages.

Now the documentation for drawing a random number in Python ([here](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.lognormal.html)), says that the mean and std must be the log of the underlying normal distribution. I tried using log(mean) and log(std) and got 514 or something, better but still not a realistic age.

​

Is there a way that I can calculate the parameters of the underlying normal distribution?

​

Happy to provide more information if needed.

&#x200B;

EDIT: I now realize that proc univariate spits out the mean and standard deviation for the normal distribution and will simply need to figure out how to make Python draw a number within a given range. ",Using SAS distribution parameters to draw random number in Python?,99ryph,new,1,1,1,0
"I’m am looking into jobs for statistics majors, and am looking to see what I would be doing and the pay I can expect right after school. ",What jobs & pay can I expect with a Bachelor is Science in Stats?,99rkdm,new,16,7,7,0
"I work in call center operations. What we do is share certain calls to other eligible departments when it’s busy. There can be anywhere from 1-30+ changes within a half hour interval. I have access to the following data:

-Average answer time

-Type of call

-Departments involved

-Percentage of each call type answered in 20 seconds and 60 seconds

-Abandonment rate (% that hang up while waiting)

-Frequency of each call type

The limitation is that it’s only available in half hour intervals. So in many cases, the longest call(s) waiting are directed to a different department for only a minute or two. Often times, a call type will be shared more than once within the interval. Because the longest waiting call type is always changing, many temporary moves are made. I log everything I do down to the minute, if it helps.

I’m not a data analyst or scientist by any means. However, I need to show the correlation and impact of these actions with abandonment rates and average answer time. 

I hypothesize that the actions improve the performance of both of these because the longest waiting calls are targeted, but I can’t find a way to prove it. Because a control group (identical call center day without any interference) isn’t feasible, I’m having a lot of trouble. 

I have the information accessible in an excel file, which I cannot share because it’s confidential. 

Can anyone help me come up with a way to quantify this to say, “because I did these actions, average answer time went down by x amount of seconds or improved by x%. Abandonment rate went down by x amount of calls or improved by x%”?

Preferably step by step direction in excel for formulas or equations to be used. Thanks guys.",Need help quantifying my impact at my job,99r7jy,new,6,5,5,0
"Hey guys,

let's say I have a correlation between two variables, X and Y. 

Y is a mean score of some arbitrary measure: Let's say the average income in the town in which the respective subject lives. I also have a variable that represents the standard error of this mean score.

Is there a simple way to correlate X and Y, while also accounting for SE(Y) ? Would I just enter the SE as a covariate?

&#x200B;

Thanks!",Consider SE of a variable in a correlation,99r2vk,new,2,1,1,0
"My boss asked me to identify points of interest among all the company data on a weekly basis, so I've been wondering on what would be the best way to do it. Our current KPI's are new: most series only exit from January to now.

So far, there are 2 topics in which i would like this sub suggestions:

&#x200B;

1) How can I determine if a variation  from a week to the other is relevant or not? Some KPI's have some pretty big variations from a week to the next. I was thinking about calculating the mean of the last 4 weeks and test if the current week is different from the mean or not.

&#x200B;

2) How to determine if a KPI is slowly, but steady, going up/down? I though about building a regression from the start of the year until now and test if the coefficient is different from zero.

&#x200B;

To do this analyses, I intend using R, Excel and Access (which are the tools that I am used to)

&#x200B;

Thanks!",Which methods can help me identify if a variation in a series is relevant or not?,99q1i5,new,5,1,1,0
"Hi guys I'm doing the coursera time series and I have a few questions just wondering if yall can clarify before I dig deeper into reading.

1. **The difference between AR vs MA?**

  It seems like MA is a linear combination of white noises? Where as AR is the combination of random variables? Is there an example that better illustrate the difference?

2. **And the X's in MA and Z's in AR.**

  Do they represent different sampling timeline but of the same distribution and random variable? Or is it like different predictor sampling at different time? 

3. **Why do we care about invertibility of MA? I do get that some how it tell us which model to use if the are many model having the same ACF graph but why do we care about uniqueness?**

Thank you!",time series questions,99p7ic,new,4,14,14,0
"Particularly for working through traditional statistically- (or otherwise mathematically-) oriented textbooks. I have found over the years that the thing that most prevents me (i.e., causes me to give up) from successfully working through any particular resource is that I don't have a way to self-study that I find optimal, and so I eventually just give up, because the endeavor soon becomes to feel like a waste of time.

For example, one day I may be of the mindset, ""okay, I'll just write down every. single. equation (proof) in this book! Then I will learn everything in it!"" I give up, because that seems like overkill, and too choppy for actually comprehending the text as I work through it. Then I think, ""maybe I just need to focus on reading it, and maybe do some practice problems along the way."" I give up on that, because my reading comprehension isn't great, and I think I retain more if I take notes along the way. Especially with the maths.

I certainly have difficulty sticking with any method that I start out with, and I give up easily when I start thinking that my chosen method isn't ""working"". And I take issue with altering the method as I go along, which maybe something I need to deal with. I am just hopeful that maybe I could get some ideas from this community, because I do feel that in order to make it through a full text, I need some type of learning algorithm that I can lean on whenever I feel like my time would be better spent elsewhere. And I think I'll feel better working in a way inspired by suggestions from others smarter than myself, more so than whatever shit I can think up to do. 

Preface complete, now pretend you just cracked open that classic text on statistics. What does your journey look like? What is your method?

Do you outline any learning goals prior to beginning? Is your method more free-form, allowing for variability from chapter to chapter, section to section, depending on the nature of the subject matter? Or do you have a structure that you follow rigidly? Do you work through a text/resource with the goal to create something you can later revisit? Do you just read and do practice problems? Do you utilize any memorization techniques, such as flash cards? What kind of workload (how many texts/resources) do you allow yourself to take on? Do you experience feelings that you aren't studying optimally? If so, how do you cope with that? How do you commit to studying one resource, through to the end?

&#x200B;

TL;DR, how do YOU self-study statistics? ",What is (are) your method(s) for optimal self-study?,99nrdz,new,9,7,7,0
"I'm trying to come up with a template in Excel for eCommerce and Digital Marketing projections in Excel. I want to set monthly and annual gross revenue goals for their whole ecommerce business and also have projections for digital marketing spends (i.e. what will $X amount produce in terms of clicks, impressions, and transactions). 

I've seen some videos for time series forecasting for ecomm projections, but I have some questions. I want to use Google Analytics data or straight from their ecomm platform:

* Will 1 year's worth of data be enough to create a basic projection? I know that it won't be super accurate. 
* How would I set it up to make reforecasting easy?
* What's the best method? Would doing a time series forecast work or woudl it be better to find CAGR then calculate based off that? Is 1 year of data enough to calculate CAGR?

I think I can figure out the DM projections based on that, but I'm having a difficult time choosing the best path to go down. Any help would be great! Thanks!",Best way to do eCommerce sales and digital marketing forecast/projections in Excel?,99njh2,new,3,0,0,0
"Hi all,

&#x200B;

I've come across the above question and I am a little stumped. I presume this is a trial that has determined a set sample size.

&#x200B;

I had always thought it was best practice to first determine number of interventions necessary and then work out necessary sample accordingly and am a little unsure how to do this process backwards (especially as usually I have only done calculations with one intervention).

&#x200B;

If anyone could shed some light on this problem, I would be very grateful. 

  
Thanks",Power calculations to determine number of interventions possible in a trial?,99m6xm,new,1,7,7,0
"Hello everyone,

I am Student pursuing masters in statistics, 

Although Im not fully aware of the subject and Content in it.

Im good with analysing and understanding the subject but lack basic knowledge has been 

a real trouble for me. 

Reason to choose this subject is because of the keen interest that this subject creates to a individual to solve or 

analysis the facts. 

Please Help me out with the issues I come up with \[ though its  all about the subject terms and content \] 

Im graduated from decent university with statistics as core subject but I really feel I know nothing when I stepped into 

Post graduation. 

I really hope I find knowledge and Help from the community persons. 

Thanking you.",Newly Cracked Egg,99lv8b,new,2,0,0,0
"Anybody know of an easy way to create professional-looking tables from a lavaan object? I've got a bunch of multigroup CFA results. I'm imagining something like stargazer where you just pass the object to it and it returns an html/latex table.

(If I'm dumb and you can easily do this in stargazer, please educate me). ",Lavaan output -> presentable tables?,99ic7h,new,0,2,2,0
"Suppose I have a bunch of single women seeking adoption and they apply to adopt children. Suppose further I have an unanticipated random intervention which helps some of those applicants succeed in their adoption. So for each application, I can randomly encourage a successful adoption.  Now suppose I want to model the effect of the number of children on labor market outcomes for those women on the basis of the random intervention.  How do I do it?  The outcome is a group level outcome, the independent variable is an endogenous count.  ",Reverse hierarchical regression with endogenous counts,99gcti,new,2,1,1,0
"Right now i have a quick-and-dirty model that projects cash flows by running a linear regression on the last 4 quarters worth of information. I’m not new to finance but I am new to statistics, is this a valid approach to projecting cash flows? ",Financial projections,99fsl2,new,2,1,1,0
"​I work for a small regional news organization, and was given the task to create an AI news reporting system that takes hydrocarbon well drilling data and production data, takes a small amount of human input (location to analyze, primarily), and outputs a short story highlighting 2-5 top statistical trends/observations. 

&#x200B;

I was given very little guidance on what the stories can look like (i.e. any story that accurately describes any trend or observation is useful), and only have a few ideas of what I can report on.   

&#x200B;

My current ideas are to look at year-to-year growth, comparing growth between different locations, comparing oil-to-gas growth ratio, etc.  What other common statistical measures and observations can be made from a list of every oil/gas well drilled from the past decade?  Any ideas are advice are appreciated!","As part of a small news organization, I was tasked with developing an AI statistics news reporting system. Advice?",99fpom,new,3,2,2,0
"Hello, thanks in advance,

(My knowledge within statistics are quite limited, but I am willing to learn more about more analytical (statistical) methods, I am working within R Studio)

I am doing analysis on big data and so far I have done a 5-fold cross validation with a logistic regression. I have been told that I need to create a odds ratio plot, I know how to get the coefficients from the model 

    (exp(coef(modfit))

But I am unsure how to move closer to plotting a odds ratio plot. 

Thanks,
A hopeful analyst",Odds Ratio plot in R,99eqf5,new,1,1,1,0
"Obviously it's career dependant, but I want to leave it open to ideas.  
  
 For me I already have an MSc and  experience, plus knowledge of R/Sas/Stata. But soon i'll have 6 months in a very easy job in the middle of nowhere, so it's a real situation. I think it's a great chance to spend time filling gaps in my knowledge.
  

Top of my to-do list is SQL because I see it in job ads now and then for data extraction purposes.   
  
I like learning about latent class mixed modelling, because I totally failed it at university. But it's not that useful to most private jobs.   
  
What would yours be?  
Edit: wow a lot of great ideas here, thanks everyone who contributed.",Statisticans: you have 6 months for purely personal development. What do you learn?,99d6ly,new,115,84,84,0
"Hey,

I built a site where you win prizes by solving statistics and probability puzzles.

Prof. Peter Winkler recently sent us in a brilliant problem called ""Who's Doing the Dishes?"".

Check it out here: [https://blackswans.io/post/9/](https://blackswans.io/post/9/). **Best solution gets £100.**

Enjoy,

Jack",Can Anybody Solve Prof. Peter Winkler's Statistics Puzzle?,99cvha,new,18,15,15,0
"I'm self studying through Casella & Burger and I'm struggling with deciding which exercises are appropriate to tackle. I mostly just scan them and pick a few to do that seem interesting but I'm worried I might be stifling my learning by missing some. Does anyone know of some list of ""necessary"" problems to work through? Just looking for some type of template so I have some more structure.",Recommended problem sets for Casella & Berger?,99cgbj,new,17,7,7,0
,Help with a statistical test,9989q0,new,0,2,2,0
"I recently finished AP Statistics in high school. While I didn't enjoy the class at first (due to the nature of learning lots of vocab. rather than ""math"" at the beginning), as our class got into inferences and modeling scenarios using t-student distribution, z-distribution, chi-square distribution etc., I began to enjoy the subject more. One of the best moment for me was during the [AP test and doing the last ""investigative problem"" on the exam about power](https://secure-media.collegeboard.org/ap/pdf/ap18-frq-statistics.pdf). At that point, I actually got chills of how cool and different the problem was and how it allowed me to understand power in a more ""powerful"" way.

I want to learn more about models, how they managed to come up with all those models and what other models exist. But AP stats is just an introductory class, so I'm wondering whether statistics in college will be interesting in the sense that we will get to learn more ways to model and interpret the world.

What is statistics in college like? Will it be as interesting as I hope it would be?

For some context, I plan on majoring in math, but I am deciding between statistics and pure math...as I enjoy both but is slightly leaning towards the interesting ""ugly"" formulas/how they were derived as well as the interpretation/freedom that stats seems to offer even after calculating a certain P-value (.06 vs .05).

​

**TLDR: I am a higher schooler wanting to know how AP stats is similar/different to Statistics at an undergraduate/graduate level**.",Is stats as interesting as I think it is?,997yw7,new,20,20,20,0
"Hello everyone,

I am an international student in my senior year majoring in maths with a minor in computer science. I am interested in phd in statistics or biostatatistics. 

Some background: I did summer REU and have been working on publishing some papers related to survival analysis. I have given talk on math conference and am expecting good recommendation letter. I am currenly taking lots of proof based courses such as real analysis, complex, and abstract and planning to take some more on my last semester.

 However, I am afraid that I may not have enough time to prepare for gre and take it in couple of months. I am considering to take a gap year and apply and want some opinions of you guys on this matter. Thing is, I feel that I might be less competative by taking a gap year. Also, I am planning to work in my gap year on any firm thats related to statistics or programming as I have some cs background and prepare my application along the way. However, I am also afraid that I do not have any work experience. Could anyone advise me on this matter? I would greatly appreciate any advice. Thanks.",Gap year,997ynt,new,11,4,4,0
"I'm helping out a friend with a model that uses several items from a very long questionnaire. The problem is that there are still variables we'd like to adjust for, but that would make the model unidentifiable by exceeding the number of available observations.

Would doing regularization or Bayesian inference with regularizing priors be a reasonable way to proceed? Any other ideas?",What do you do when you have variables you want to adjust for but don't have the sample size to do so?,996wgz,new,3,2,2,0
" 

I have a paper to do this semester that requires me to research a topic in which there is one dependent variable that has at least three independent/explanatory variables.

Ex) wage differences b/w male and female. The dependent variable is wage, while a couple independent variables could be gender, age, race, education, etc.

It's going to need to be around 20-25 pages, so I'll need something that has a lot to be written about; but I'm stumped on possible subjects/topics.  I'm interested in fantasy foodball (or just football in general), technology, and the marijuana industry (live in CO).  Any help is appreciated!",Possible interesting subjects for empirical paper for Econometrics class?,996sl6,new,6,3,3,0
"I have been thinking about working as a statistician, however the mathematics major's curriculum seems to be more interesting to me.

Would I have any problem majoring in math and then searching for a job as a statistician? ",Can I major in mathematics and work as a statistician?,996ckz,new,8,1,1,0
"Hi - forgive my ignorance, all of this is new to me. I'm analyzing a data set for two groups with unequal variances based on Levene's Test. The group comparisons (independent samples t-test) suggest that the means are significantly different. I also need to calculate effect size - can I use Cohen's D when variance is unequal? TIA!",Cohen's D for Unequal Variances?,995kk3,new,3,1,1,0
"I am working with stata to run cox proportional hazard analysis. We're looking how a biomarker (continuous variable) affects likelihood of recurrence of cancer (binary variable). 

I am trying to wrap my head around the results. 

The HR for one of the biomarkers is 1.034. From my understanding, this means that for every one unit increase in said biomarker, the risk of achieving the outcome (recurrence) increases by a factor of 1.034. Based on this, a 20 unit increase in said biomarker would mean a 1.034^(20) =1.9x risk of achieving outcome. 

Is this an accurate interpretation or am I thinking about it wrong? ",Hazard ratio for continuous variables,995cj0,new,5,1,1,0
"Hello all,

I am trying to figure out which kind(s) of ICC I should use for a reliability+validity study I'm doing. I'm comparing some dimensional measurements from a gold standard and a digitzer. I am also going to compare my digitizer to a more outgoing digitizer.

For validity, I'm probably going to present the mean absolute differences (systematic error) and the Pearson's coefficients using both the gold standard measurements and my own. This makes sense to me. However, I'm not sure how I am going to calculate the ICC for reliability of my digitizer.

I used [this ](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4913118/#!po=21.4286)paper to read up on the basics of ICCs and [this ](https://biblio.ugent.be/publication/1319056/file/6754343)paper as a close and related example for the use of this statistic.

From what I've gathered, I should use the two-way mixed effects model for absolute agreement. But beyond this, I'm not sure how to move forward to actually perform the calculations. I am going to have 2 trials measuring the same square object using my digitizer for several different measurement scenarios (different lighting, etc.) and I will measure the length, width, and height each time using the same code. I am using the SAME object EVERY time and have already measured it well with calipers (gold standard in this case). Then, I'm going to compare the measurements of feet between my digitizer and one from industry where I only have access to one trial, and each trial uses a DIFFERENT pair of feet. Additionally, for repeatability, I can scan the same object and pair of feet many times for more data. However, I won't have access to many subjects for the latter testing.

I'm a little confused, because I believe for my purposes that the ICCs should describe measurements by the *same* method, not two different methods, but literature states that ICCs compare *paired* data. Plus, I'm unsure how I should change my calculations based on the changing scenarios and subjects.

Can anyone clear up my confusion? Please let me know if I should provide more detail.",ICCs for Reliability and Validity Study,995c9f,new,2,3,3,0
"I'm usually quite good with statistics, however I'm not entirely sure I remember how to calculate probability in this way: 

* If there is a 1 in 112 chance of an event occurring after a single action is taken, how many times would you expect that event to happen after 500 actions, 1000, actions, 10,000 actions etc.? Actions and any subsequent events are independent of any future actions/events. Is this as simple as dividing the number of actions by 112?

Then: What is the probability that (X) events occur after (Y) actions are taken?",Need help finding probability of (X) events occurring in (Y) actions,9944qa,new,4,1,1,0
,"I'm about to start an applied statistics masters program. What kinds of theory likely to be missing, and what theory should I make sure to learn (if it isn't covered)?",9940qp,new,9,20,20,0
"I'm applying for a MS in Statistics. Can someone comment on whether I should add a school to my list? I'm looking mainly at top schools with strong statistics terminal masters programs, with strong post-graduation employment opportunities. I'm also factoring in cost of program + employment/scholarship availability and location (I prefer South in order to get a job in the South i.e. Georgia/TX/FL)

- UC Berkeley MS in Statistics

- Harvard MS Data Science

- UT Austin MS Business Analytics

- UChicago Data Science

- Georgia Tech MS Statistics

- Carnegie Mellon MS Statistical Practice

- Carnegie Mellon Master of Computational Data Science

- Upenn MA Applied Math and Computational Science

- Uchicago Masters of Statistics",List of MS Statistics programs?,99026g,new,2,0,0,0
"I implemented the R stargazer regression reporting library in python because I didn't want to switch back and forth between languages for stats projects and I thought other people might be interested as well. 

It includes many of the customization features found in the original package and can be downloaded using `pip install stargazer`. Check it out if it sounds like something you could use and feel free to contribute!

[https://github.com/mwburke/stargazer](https://github.com/mwburke/stargazer)",Python implementation of R stargazer library,98zhbk,new,6,36,36,0
"So I decided to learn how to make a model of fantasy football scoring this year and simply dont know enough about statistics.

The plan is to run some multivariate regression and figure out a way to factor out colinearity.

I have two initial concerns.  
1: Should I cut out low participation games from players who either only played a few snaps, or got hurt partway through the game?   It seems like any data provides a sample to use and more samples is good, but on the same note, Im worried this will skew my Coefficients towards the bottom end because a bunch of very low usage samples are being included.

2: After finding the covariance between two individual variables with respect to the independent variable, how do I make that covariance actionable?  Went to the trouble of trying to read a few white papers on the topic, and just went cross-eyed.  Mostly the terminology gap is the problem.


If anyone has some ideas about these, Id appreciate the help.

Thanks",Need Help Determining Data Cutoff,98zcaf,new,0,1,1,0
"As the title says, that's my drama. I have a data set of 2592 records, which consists of litterfall measurements of 6 litterfall components (leaves, flowers, fuits, seeds, twigs and fine/undentifiable debris)  at 24 dates, recorded in 18 places. I also have the flood duration and vegetation type for each place.  


I want to perform an ANCOVA analysis to assess the effect of flood duration on litterfall amounts controlling for vegetation type, such as `leaves ~ vegetation * flood`. Firstly I computed the leaves average per place and fitted the model (N = 18 places), which resulted in not that impressive results (something like *R-square = 0.51, p = 0.15*). Then I had some problems in other computations and decided to join the independent variables (vegetation type and flood) to the raw data instead, which worked for that computations. But I didn't realized that was the same dataset (R dataframe) I was using to fit the model.

So when I came back to the model, all of a sudden all all my *p-values* fell to the ground. Almost all *p*'s were zero. So I figured out that I was using the raw data (N=2592). So the question is: **Is it fair to fit the model using raw data? Is it cheating?**

I mean, at the same time it looks like cheat, it sounds perfectly reasonable for me to use the raw data, because the mean values can't capture the whole picture. What do you guys think?",Linear models: Fitting using averages vs fitting raw data,98z9kk,new,1,1,1,0
"In the context of my Masters thesis in Financial Engineering, I investigate the performance of a linear model that focuses on simplicity to approximate results of more complex models. I'm however stumped when it comes to inferences with this model. It has lagged values of the dependent variable as regressors, and the error terms exhibit autocorrelation and heteroscedasticity.

As is done in the literature for this model, the coefficient standard errors are corrected with Newey-West method to calculate the robust covariance matrix. However, I cannot find any answers on how to do inference on the distribution of a particular predicted response. Assuming the Newey-West method yields a valid covariance matrix, I can easily do inference on the coefficients or the mean response, but such parameters are not observable and I cannot backtest them. What I can observe is a particular value of the dependent variable and would like to model its quantiles using the linear model (i.e. prediction bounds for a particular response). However, the equation of the prediction bounds involve the error term of the response, which is correlated with past error terms, and thus with the estimator of the coefficients.

Are any of you aware of article or other literature that deals with this issue WITHIN the framework of HAC robust linear regression models (for example by drawing on the assumptions and kernel used to derive the Newey-West estimator)? Bootstrapping is not reasonable in my context, since I'm doing over 20,000 regressions and doing a Monte Carlo simulation for each of these will defeat the purpose of the original model. Also, fitting a model on the residual (like a ARCH or something) defeats the purpose of not fitting a more complex model to begin with, and I seriously doubt that the estimated distribution of such a model would reliably approximate the true distribution of the error terms.

My out-of-sample performance measurement is for now limited to RMSE, RMSPE and other similar measures that only focuses on prediction errors and not their (asymptotic) distribution under ""H0: the model is valid"", (e.g. an interesting performance metric would be the proportion of times the actual value falls outside the prediction interval).

Thank you.

Edit: Also, the MSE does not estimate the variance of the error term of the response under conditional-heteroscedasticity and autocorrelated errors, so the usual equations for prediction bounds under OLS certainly do not apply. ",Prediction bounds & Backtesting linear models with Newey-West errors,98z2tr,new,0,1,1,0
"As the title indicates, I want to know whether it is appropriate to compare one sample to a number of other samples (in this case six others) using individual T-tests. I know that ANOVA is used to determine whether there are differences between samples in all possible pairs, but what if you are only interested in a subset of those pairs, namely sample 1 vs sample 2, sample 1 vs sample 3, etc. Can I use multiple, individual T-tests for this?",Is it inappropriate to compare one sample to a number of other samples using individual T-tests?,98x1bd,new,6,3,3,0
"I'm conducting a qualitative study with a relatively small sample (10 in one condition, 12 in the other). This is a result of it being a survey with written accounts and a specific target demographic. I was just wondering because there are some yes/no questions in the survey would I still test for significance despite the small sample making finding significance difficult? Or should I just report the descriptives?",Small sample significance testing,98ulrq,new,2,2,2,0
I have published a lot of (online and free) material I'll be using to teach my statistics classes this year. You can read about it [here](https://ntguardian.wordpress.com/2018/08/20/materials-for-teaching-applied-statistics/).,Materials for Teaching Applied Statistics,98uhdm,new,13,43,43,0
"I just created a slack group for people who would like to do a slow read of McElreath's Statistical Rethinking. I'm working through all the examples, both in R and the PyMC3 port to python, but I find the statistics confusing at times and would love to bounce ideas off fellow students.

If anyone would like to join, please email me at [pluviosilla@gmail.com](mailto:pluviosilla@gmail.com) (pluviosilla at gmail dot com), and I will send you an invitation to the slack group.

This is a truly marvelous book, best I've found so far for coming up to speed with Bayesian statistics and probabilistic programming.

John Strong",Statistical Rethinking Study Group,98u7xn,new,4,5,5,0
"I am really tired and still do not have much experience so please be gracious.

We have a simple model that estimates the rate of return 2 days out based on some averages of recent data. When I plot the estimate against the actual return I get a perfect 45 degree angle with some degree of scatter. When I feed the basic estimate into a linear regression with dummy variables for weekday and monthday, the variance around the regression line reduces however the angle shifts as shown in my image [here](https://imgur.com/a/5zKPxgk).

I know that the return on weekends is different from the other days. I assume that is the cause of this shift. But I don't understand enough to know *why* it's happening. Can anyone explain? Or do you need more data? Thanks",Why does my regression shift the prediction?,98pp3s,new,8,6,6,0
"I am interested in entering all of the place finished data for our fantasy football league that has been going on for about 15 years.  It has 10 teams.  We have where everybody placed all entered into the league's records, so all of that data is available to me.

**What I'm interested in doing:**

Seeing the average movement of teams in terms of what place the finished, and to see if finishing in last place gives you a stark advantage for the next year due to draft pick in our particular league.  My league is considering switching to a ""snake"" draft instead of a standard draft, but we have 2 ""keepers"" so I'd like to  present the data when the vote is presented.

**My Question:**

What kind of analysis should I attempt to perform to present this accurately?  I can do the data entry and give a link if anybody would be able to direct me in how to analyze it.  Thanks so much to anybody willing to give some advice!

**Concerns:**

I am thinking that because somebody finishing at the top can only finish even or go down that the data movement might be a bit skewed.  Is there a way to account for that?",Please help me prepare a statistical analysis of a fantasy football league!,98n6n5,new,4,0,0,0
What habits and resources helped you get through it? What should I avoid doing?,I start a masters program in Applied Statistics tomorrow morning. What do I need to do from day one to succeed?,98mh46,new,34,38,38,0
"Im reading the book - Computer Age Statistical Inference book.
I have a question in the 2nd chapter.
https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf
In this PDF, can someone explain from 
""Statistical inference usually begins.."" on page 12 until the start of 2.1? I am really confused by the x, X and n uses and the mapping of functions.

Thanks",Doubt in understanding mathematical/statistical terms,98m7ab,new,10,5,5,0
"In this article:

http://uk.businessinsider.com/the-british-public-now-believes-brexit-will-damage-the-nhs-2018-8

.. me and others agree the title is somewhat misleading.

This is because according to the poll, these are the numbers:

* 33% said Brexit would leave the NHS worse off;
* 30% think it will have no impact;
* 26% of people believe leaving the EU will improve the standards of NHS care;
* 8% are unsure.

Would it be correct to call the 33% a ""plurality"" in this poll? 

...Or would it be *incorrect* to do any such thing: that is, would it be *necessary* (mathematically, statistically, logically, etc.) to add up 30% and 26% because these two categories have in common that they both believe Brexit will have no *negative impact* on the NHS?

Can one even speak of a plurality here?

I'm just really interested in the answer and if combining two categories together as someone else said was necessary is an error, and if it is, if it has a name in statistical jargon.

If not, I'll have learned a new thing about how to grasp these polls and when I may call something a ""plurality"", because so far, I'm inclined to believe that would have made a more honest headline.",Question about a survey and plurality,98ghri,new,4,4,4,0
"Hello all!  I'm interested in a check of my understanding of the difference between multivariate multiple regression and univariate multiple regression.

After watching this incredible [video](https://www.youtube.com/watch?v=6941l6D0CR4), it seems that the regression coefficients are the same whether you solve for them univariately or multivariately.  This is because the regression coefficients are selected so that error is set to 0 while also controlling for other predictors.  Because error is set to 0 (whether it is univariate error or multivariate error), the partial derivatives used to create the formulas that estimate regression coefficients turn out to be the same and so yield the same values.

These regression coefficients then allow for the estimation of the SSE.  In the univariate case, SSE is computed as the squared difference between the observed and predicted criterion variables.  In the multivariate case, SSE is computed univariately for each of the criterion variables, and these SSEs are then summed (representing the trace of the error matrix).  This difference in the SSE quantity that is minimized is what allows, in the multivariate case, for the simultaneous consideration of the intercorrelations between predictor and criterion, as well as the intracorrelations among these same variables.

Have I adequately captured the differences between multivariate and univariate multiple linear regression as well as explained the reason why they are different?  

I appreciate your feedback!",Checking My Understanding of the Difference in SSE between Multivariate and Univariate Multiple Linear Regression,98dww9,new,8,10,10,0
"I'm studying for a final comprehensive exam for my master of public health program, and biostatistics is one of the core competencies.  I'm struggling with remembering which statistical test to use for which type of data / study design.  Before posting I checked the side bar for links and have been reviewing my notes, the decision tree, and youtube instructional videos.  

Are there any good tips or resources for getting this into my brain other than just reviewing what I already have ad nauseum?  ",Choosing the correct statistical test,98ddez,new,4,2,2,0
"Hi guys,

I was wondering how to go about doing this? I get the traditional categorical variable we just use dummy variable but is it the same for a categorical variable with multiple values within it selected?

Thank you",Encoding categorical variable with multiple values selected?,98cta8,new,2,1,1,0
"I have a p>>>n dataset for which I fit an elastic net logistic regression. I believed it would be easy to find a good model for the training set, but that it would perform badly on the test set due to overfitting. Out 1k variables I selected the top 10 according to variable importance (the ones after 10 all had importance <40), then I re-trained with these 10 variables to update their coefficients after removing the other variables.

To my surprise, this model performed very well on the test set. However, it performs poorly on a completely new dataset, unless I re-train the model coefficients again using a training set portion of the new dataset (then it does well again). Can I say I'm experiencing overfitting? Or is it more likely a problem of the second dataset coming from too different of a population than the first dataset?","Would an ""overfit"" model perform badly on the test set, or only badly on a fresh dataset?",98cnjl,new,15,1,1,0
,"I used ""the maths"" to figure out how to make a lot of money drop shipping",98ayq7,new,19,0,0,0
"I don't know if this falls strictly under the statistics, but I have no idea where else to ask. I'll try to explain this as simple as possible: let's say that I have the following table

|Observed variable|Placebo|Treatment|
|:-|:-|:-|
|Some name|325|108|

OK, while i have real data and naming, as well as around 14 rows in the real example, this is enough to ask the question. Now, taking the numbers into the consideration, we can see that there is a 66.77% decrease in the case of the treatment, which is good! I would like now to add that percentage into the table, but I have no idea what's the correct way to report this data along with the raw numbers. Also, I don't know how to exactly name the column (percentage change? percentage decrease?)

Thanks for any suggestions!",How to represent the percentage change in a table?,98as1d,new,10,8,8,0
Recently I've been tasked with analyzing data to quatify the effectiveness of Facebook ad campaigns. I have information on demographics and how long each person viewed the media and if the clicked on the link. What statistical concepts should I learn in order to process this information?,Need some direction,986tno,new,7,3,3,0
"I'm trying to find whether the level of ""corruption"" and ""peace"" in a person's home country, impacts upon the effectiveness of international education. My statistics knowledge is pretty basic (mainly self-taught). I generally rely on Laerd statistics to guide me through everything.

So I'm currently running an ordinal logistic regression on SPSS. My dependent variable is a Likert-Type Scale which looks at changes in student opinions. I have two independent variables which are both continuous scales that measure the level ""Peace"" and ""corruption"" in the respective 'country of origin' of those students.

When I run both independent variables together in the same PLUM/GENLIN tests the results (Wald Chi-Square, df, Sig, Exp(B), etc) come out differently to when I run separate tests for each independent variable. I'm hoping to find out whether I should run the tests separately, or run a joint test for both variables? And why?","Why are the parameter estimates on a GENLIN test different when I run the independent variables separately, versus at the same time? And what is the correct way?",983qnf,new,1,2,2,0
"I'm not sure if this is the right subreddit, but can someone let me know which topics in calculus I'll be using most in statistics, and point me in the direction of some good practice problems? Thank you!",What topics in calculus should I review before starting a Master's program in Applied Statistics?,9835dr,new,29,37,37,0
"Hey everyone,

I'm really struggling with how to best explain the differences between the two, I understand and can derive their equations but it's expressing that which I'm having trouble with. How would you guys ELI5 the concept?

Thanks for all help!",Understanding the advantage of explained variance over r2 scores,982t7g,new,12,2,2,0
How should I proceed after completing [Statistics 110](https://projects.iq.harvard.edu/stat110/home)? I'm looking for courses dealing with Stochastic Processes or Inferential Statistics. I'd prefer if the lecture videos are available even in low resolution. I'm not sure I can learn from books alone but I'd appreciate any resources. Please direct to me the right sub for this question if this isn't. Thanks! ,Are there any free online courses for Advanced Statistics?,982sdw,new,8,15,15,0
"Hello everyone,
I have been learning Bayesian inference, and often have to compute the evidence http://www.HostMath.com/Show.aspx?Code=%5Cpi(%5Ctheta%7CY)%20%3D%20%5Cfrac%7Bp(Y%7C%5Ctheta)%5Cpi(%5Ctheta)%7D%7Bp(Y)%7D
)

From what I understand...

The **posterior** is a distribution of what our parameter might be, after we take into account some data (observations) Y. 

The **likelihood** says ""ok, if we choose this parameter value, how likely is the data to be what it is?""

The **prior** is simply our belief of what the parameter should be.

But what is p(Y)? The notation suggests it is ""the probability of data"" or ""the distribution of data"". In textbooks and literature, ""its the normalizing constant, it is difficult to compute!"". So what exactly is p(Y) and why is it a number when the notation suggests it is a distribution? Or maybe it is a distribution? What's going on?

I can see it is the marginal distribution of the joint, by integrating out the theta... But still cannot seem to make sense of it!

Clearly I am misinterpreting something here, could someone please point out my mistake?

EDIT:

Thank you for all sorts of responses, from a detailed examples to concise high-level intuition type of answers. I now have a better intuition as to what the evidence is and what it is doing. If I had to point out one part that made it confusing, it would be the implicit conditional on model M. Without M, p(y) made little sense to me, but it is also redundant to explicitly write P( . | M) for every distribution.

Again, much thanks to u/StephenSRMMartin u/richard_sympson and others for the concise responses, u/shaggorama for the detailed example, and u/paanther for detailing where my confusion stemmed from. ",[Bayesian] What exactly is the evidence/marginal-likelihood?,97zh93,new,5,2,2,0
"I'm looking for a geomapping software that will allow me to plug in about 18 months worth of data to view trends from month to month and show changing trends in a geographic area. I will input a series of customer's zip codes and the months in which those customers came to visit us. I want to be able to see changing trends for the past 18 months in some sort of animation.  


I know that Batchgeo will allow me to plug in the locations, but I'm not certain I will be able to view the changing trends in an animation. Any tips?",Help finding a Best-fit Geomapping software,97x6kl,new,4,0,0,0
"Hi guys!

I am currently working with SPSS and wanted to compute a ""mean"" variable for one of my tests (27 items).

When I am trying to, it says that i am only allowed 64 characters....

So, I can't compute the mean out of those items because I am only allowed a limited number of characters????

Please help me,

thanks",SPSS computing variable error!,97x64w,new,4,0,0,0
"Hi, I’m working on a model based off a latent utility framework where Pr[d=1] = Pr[xb > -e] with e is distributed logistically, x is some independent variable, b is its coefficient, and d is an indicator for if the option is chosen. This is a simple binomial choice scenario.

Now, I understand the typical assumption is that e has a standard logistic distribution with a mean of 0, but what if it actually has a mean greater than 0? Can you still write the choice probabilities as the CDF of the logistic distribution? It seems to me that you should be able to, however when I run a simple simulation in a statistical package the predicted choice probabilities only match the value of the CDF when I designate the error to have mean 0.

Could someone explain what I am missing here? I greatly appreciate any help.",Error term in latent variable derivation of logistic regression,97x1op,new,1,5,5,0
"For instance, can I use either measurement to determine if the difference between sample means represents a confident reflection of the overall population means?",Can standard error and 95% confidence interval be used to determine if data is statistically relevant?,97seo3,new,5,15,15,0
"Hi community!

I am currently trying (:D) to evaluate my survey which also includes the NEO PI-R. Does anyone have or know how to calculate the t scores? I have raw scores from my data but I can't go on with that....

I am confused...

Thank you",NEO PI-R scoring? t-scores??,97s1ss,new,2,1,1,0
"I am entering a doctoral programme in statistics (module – economics) in autumn and have to write a »brief concept of research work«. The mayor problem is that I don't have a fixed idea of what to do yet. Firstly I thought I would do something more in the field of behavioral economics as there is really a lack of such knowledge in my country and this is something I am really passionate about. The studies will be after work as I work in an insurance company and could get probably all the needed data from my company. My mentor suggested I use the data if I already have it and maybe even more importantly know it in detail. I would really like to bridge the gap here and do something more behavioral in the insurance field using modern statistical techniques, but can’t come up with any “new” and exiting problems and approaches. Do you kind people maybe have any suggestions on where to look and find some ideas on topics like this? ",Interdisciplinary statistical research ideas,97rlig,new,2,5,5,0
"Linear regression is a basic and commonly used type of predictive analysis.  The overall idea of regression is to examine two things: (1) does a set of predictor variables do a good job in predicting an outcome (dependent) variable?  (2) Which variables in particular are significant predictors of the outcome variable, and in what way do they–indicated by the magnitude and sign of the beta estimates–impact the outcome variable?  These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables 

Linear Regression Using R ---> [CLICK HERE](https://www.youtube.com/watch?v=2Sb1Gvo5si8)

Linear Regression Using Python---> [CLICK HERE](https://www.youtube.com/watch?v=NUXdtN1W1FE)",Linear Regression Using R & Python - Explained with Example,97qwxg,new,3,0,0,0
"I'm running a logistic regression in R with regularization (caret package). I let it choose the best alpha and  lambda out of a sequence between 0 and 1 for each and end up with some value for both (i.e. an elastic net). I use 70% of the data to do a 10-fold cross validation, repeated 5 times. I have 1000 variables, but only ~300 observations. 

Many of the variables are highly correlated (1250 pairs that are > |0.7|, of course there's also 500k pairs < |0.3|). 

Depending on set.seed, the variable importance list changes a lot. The top 3 are always the same, but #7 could go down to #20 for example. Given the p>>>n problem of my dataset, what is likely going on? 

For what it's worth, grabbing just the top 10 variables and applying them to the test set produces pretty good results.","Highly different variable importance depending on set.seed() - symptom of overfitting or just correlated predictors, or both? Or nothing?",97pz6x,new,12,12,12,0
"I'm not a statistician but I know just a bit about statistics. I like to think of statistics (meant as including Bayesian inference and causal inference) as applied epistemology.

Is this sensible in your opinion?",Statistics as applied epistemology,97py3p,new,3,0,0,0
"Hi All,

I am a 25 year old with a B.S. in Accounting. I've worked 2.5 years in the Internal Audit department of a bank, hated it, and quit 2 months ago. I am about to accept a Accounting job, which I am not terribly interested in, but I need to pay the bills somehow. I've been feeling pretty down about this, since I feel like I've made a huge mistake. I've had an interest in Statistics for years, mostly stemming from my love of baseball and sabermetrics. I enjoy analyzing data and predicting the future and I feel like I really should have majored in Statistics. I am also interested in programming and have some experience with Python.

My question is, should I go back for a B.S. in Stats? It would probably take 3-4 years of part time schooling before I finish. Will I have difficulty finding a job? Is the work rewarding?

Thank you in advance.",Career change to Statistics?,97pf94,new,15,7,7,0
"Hello all, I am applying to a graduate school program (not stats, but human computer interaction). I've already talked to the head of programme and he suggested I supplement my application with more advanced mathematics - specifically statistics. I have about 6 years of work exp in basic and intermediate stats now (I do M&E for intl dev orgs).

I'd like to find something I can do online that can formally be a part of my graduate school application. I'd prefer it to be **multi-course** (it was suggested I do 10-15 credits worth) with a some sort of certificate at the end.

Looking around, there seem to be a lot of options but I wondered if there were ones that stood out. Ones I've found so far:

\-  John Hopkin's data science specialization (coursera) seems to be the most marketed but reviews were negative.

\- Methods and Statistics in Social Sciences Specialization from University of Amsterdam (coursera).

\- Statistics with R Specialization from Duke (coursera) also looks good.",Statistics Certification for Graduate School Application,97k4y9,new,4,3,3,0
"Hey people! :)  
I'm currently working on my Master's dissertation and I am going crazy with SPSS. 

  
Is there anyone out there, who can help me with scales and scoring? 

Variables are items from a personality and friendship test.

Thank you in advance! :)",Need help with SPSS...Big5 Personality,97ikxg,new,12,0,0,0
"Hi there! My group and I are in the process of making our thesis paper. One of our sub-problems aims to determine if there is a significant relationship between academic dishonesty and a student's academic performance (general average for the previous school year). What correlation method should we use (Pearson, Spearman, etc.)? One variable is numerical while the other is answerable by Yes or No (Did you commit academic dishonesty in the school year of 2017-2018?).",What correlation method should I use?,97iely,new,4,1,1,0
Any graduates of human sciences here? Do you have any advice for breaking into the data science job market?,Are there any social scientists around who got into statistical jobs / machine learning / data science?,97i37a,new,64,32,32,0
"I'm building an upgrade of a current tool in my company and I have been asked to compare the new tool vs the older one. 

How do I determine how many test cases I should let my test group run on each tool? ",Determining number of test cases.,97gvkc,new,1,2,2,0
"I'm a Year 2 uni student now, and it's almost time to have to decide if I want to specialize (since my choice of modules taken will have to reflect that). 

Currently, there are 3 routes. 1 without specialization, 1 with a specialization in data science, and the last one in business statistics. 

I am a little uncertain. On the one hand, I feel like specializing in the business statistics route as I may want to go towards that route in the future. On the other hand, I feel like if I take the route, there may be 2 problems.

A) It will greatly limit the choice of modules I can take and my timetable becomes much more rigid for the next 3 years. I'm worried that I may not be able to complete all the modules I need to complete due to future timetable clashes (especially so since I'm planning to double major in economics as well)

B) Will that close me off to a section of the job market in the future? Sure, companies who are looking for someone to analyse their business data may rank me higher than a similar job applicant without this specialization, but it could also backfire and close me off when it comes to many other industries (programming jobs, data analytics). And I would think that the number of industries that close me off would be far greater than the number of industries that are more open to me as a job applicant.

Anyone has any advice? Thanks in advance.",Should I get a general Stats degree or a degree with a specialisation?,97eobq,new,5,4,4,0
"In finance, the term “regime detection” refers to taking a time-series (typically a market indicator) and identifying different periods that are governed by different generative rules. At its most basic, regime detection is used in finance to identify bull and bear markets. The different regimes are commonly analyzed via a H(S)MM, though other approaches like regression and deep learning are also used.

I can find very little about regime detection outside of finance. Is this generally a problem that’s not well studied in other fields, or is it known by different names? I would be particularly interested in resources or pointers on it's use in political science and social network analysis.","Other terms for ""regime detection""",97cg87,new,7,2,2,0
What the title says. What are some of must read books for starting analysts ? I am finishing my graduate program and was wondering what are some things every analyst must know,Must read books for starting analysts,97b2j1,new,16,36,36,0
"I'd like to just see a big list of various data analysis scenarios/problems, where an expert statistician then goes through their thought process (and programming process if possible) of how they would do the analysis and why.

Any books or websites you've come across like this?

Here's one I've found, a bit too advanced for me though.

https://www.amazon.com/Optimal-Design-Experiments-Study-Approach/dp/0470744618",Anyone know a book/website that just hammers through a list of case studies and explains how the analyses should be done?,97aslw,new,3,3,3,0
"Can count data be used as the dependent variable for a main effects plot?

Background:
I am using minitab to create a main effects plot analyzing the effects of several factors on the reject rate of lots of product in my company.  The reject rate is calculated as a percent using the total number of parts inspected and the number of parts rejected from that lot.  There are three different factors and each factor has three ‘groups’ within each. 

Statistics are not my strong suit so I hope this makes sense.  My hypothesis is that one of these factors and one group in specific is correlated strongly to the high reject rate lots I’m seeing.  Does anyone have any idea how I can visualize and quantify this better?  Apologies if this is not the right subreddit for this question.",Main effects plot and count data,979dcl,new,1,1,1,0
I I have a bachelors in biomed sci. I am currently in school for a bachelors in stats. I am also looking into Georgia techs online masters in data analytics. I am wondering if this would be a better route than the bachelors or if it would be difficult to find a job with this degree. Basically ho0w is this degree looked at in the community?,Georgia Tech omsa,978qs1,new,18,11,11,0
"(This seemed more appropriate for /r/AskStatistics, but I didn't get any responses.)

I've recently finished working through [Introduction to Probability by Blitzstein, Hwang](https://www.goodreads.com/book/show/21558327-introduction-to-probability), the text based on Harvard's STAT 110. I really enjoyed it and I'm interested in the follow-up class: STAT 111. Unfortunately, there doesn't seem to be an equivalent textbook for 111.

What would be a good follow up? I found this [quora post](https://www.quora.com/What-are-the-top-10-big-ideas-in-Statistics-111-Introduction-to-Theoretical-Statistics-at-Harvard) that lists the topics covered in 111, but I've had trouble finding a suitable textbook that both covers that material and is still at the undergraduate level.

So far, I've considered the following texts (though I'm open to any):

- [Probability and Statistics by Morris H. DeGroot,  Mark J. Schervish](https://www.goodreads.com/book/show/979760.Probability_and_Statistics)
- [Introduction To Probability And Mathematical Statistics
by Lee J. Bain,  Max Engelhardt](https://www.goodreads.com/book/show/4022552-introduction-to-probability-and-mathematical-statistics)
- [Mathematical Statistics with Applications by Dennis D. Wackerly,  William Mendenhall, Richard L. Scheaffer](https://www.goodreads.com/book/show/115105.Mathematical_Statistics_with_Applications_Mathematical_Statistics)
- [Introduction to Mathematical Statistics
by Robert V. Hogg, Joseph W. McKean, Allen T. Craig](https://www.goodreads.com/book/show/756267.Introduction_to_Mathematical_Statistics)
- [Statistical Inference by George Casella,  Roger L. Berger](https://www.goodreads.com/book/show/383472.Statistical_Inference)

I'm including the Casella text because, while aimed at the first-year graduate student, it definitely covers all the material from the quora post and it's one I'd eventually like to work through. I'm just worried it's too much of a complexity jump from Blitzstein.

Last, I'm sure I'd be ok with any of these texts. But given that I devoted almost three months to Blitzstein, I'd prefer a recommendation here as I'll probably have to invest even more time in whichever one I pick.","[xpost /r/AskStatistics] Follow up for Introduction to Probability by Blitzstein, Hwang?",9787iu,new,4,2,2,0
Can someone send me a link for a complete z value table? The values for the z value stops at plus or minus 3.4 in google images. Thanks in advance!,Z value table,9783j6,new,12,0,0,0
"If I am using say addition in quadrature to get an estimate of the overall standard deviations of a set of measurements for something like density; where mass and volume are measured separately and then combined (divided to get the density).  When calculating the standard error or the mean, would the n value used simply be the average number of determinations used for each or the two different measured values, or something else?  Maybe similar to the way the degrees of freedom would be calculated depending on equal or un-equal variances?

Or would the better way be to calculate the standard error of the mean for the two values, then use error propagation to get the CI for the final calculated value.

OR finally, could I simply get the 95% CI for the two values, and use the margin of error for the error in the propagation equations?

Don't worry, this is not a homework problem.",Combining Error propagation and Confidence intervals,977cj3,new,2,1,1,0
"Hi! I'm trying to calculate the statistical significance of the difference between two test and control groups. I posted my question to cross validated better explaining what I am trying to do:

https://stats.stackexchange.com/questions/361856/compare-daily-control-to-test-uplifts-of-two-groups

I didn't get much response yet, and wondered if you fellas can help me either validate my assumption or have suggestions on how to conduct this test properly.

Thank you very much!",[Statistics Question] Help with selecting the correct proportions test between two groups,97789l,new,0,1,1,0
"I'm considering graduate programs in various universities, and I was wondering how (recent) master's level statistics graduates managed to pay for everything over those 2 years or so. It sounds like the odds of getting an assistantship as a master's level student is pretty hard (or outright impossible in some programs), so how else do you afford everything, especially if you're going to an out of state school or one located in an expensive city? Is subsidized/unsubsidized loans enough without having to turn to private loans, and how much debt did you have upon finishing your program?",How did you afford to go into graduate school for statistics?,975m9x,new,44,5,5,0
"Do you?

I've been reading a lot of posts on Reddit about biostatistics and honestly, I haven't heard anyone say anything bad about the profession. What makes you regret becoming a biostatistician? Or what don't you like about being a biostatistician (even if its something minor)?

Are there any salty biostatisticians lurking out there?

ETA what u/iloveciroc said: "" is getting a PhD in biostats needed or is a masters sufficient? Like do you regret the additional time taken to obtain the PhD or is it worth it when you are working? """,Do you ever regret becoming a biostatistician?,974qyl,new,55,31,31,0
Let’s say I did an annual chart comparing daily temperature averages in Palo Alto and googles day close stock price. And saw a correlation that hotter days ended with higher valued google shares. This would not be credible  insight. How do you intentionally avoid something like this ?,How do you avoid someone looking a 1 to 1 statical comparison and not saying “correlation is not causation “,971hxn,new,13,0,0,0
"Hi all!

I'm looking for something similar to a chi-squared test but that considers the extent of drift between values.  For example, using [these three distributions](https://imgur.com/a/uQaxz0w) I'm looking for one that would give a more extreme output when comparing distribution 3 vs 1 than when comparing 2 vs 1.

The context that I'm using this in is comparing two different graders' grade distributions to get some insight on whether they are likely to be grading similarly.

Any help is much appreciated!",Test of distributions for interval data,970969,new,25,10,10,0
"In simpler  terms,  could you just  explaine to me  what is  a  Lag-operator  in time series analisis  and  Autoregression?  I don't  understand, what   is it?",What is the Lag-operator?,9705tb,new,4,1,1,0
"I'm trying to figure out where the computational intensity lies for a logistic regression (I'll be doing an elastic net version later). For a linear regression model, the biggest matrix used when using OLS is X'X. So if you have 50 variables and 100 observations, the largest matrix you'll have is a 50x50 matrix, and the most cumbersome calculation is finding the inverse of that. I'm looking at MLE for logistic regression, and I can't quite tell what the largest matrix is or the most cumbersome calculation.

https://czep.net/stat/mlelr.pdf

Scrolling down to equation 11, would it be this? Or would it be equation 16? It looks like the statistical software isn't even needing to find derivatives or second derivatives, assuming it's just using these formulae here.",Where does the computational load occur when running a logistic regression?,9701zs,new,6,9,9,0
"EDIT. Can someone explain why I am being downvoted. 

My experience with stats so far is limited but I am trying to improve. I wor at an online publisher and we would like to A/B test content recommendation changes. My question is if there is anything wrong or improvable with my setup. Here is an overview:

We currently pay for a content rec widget which has 5 panels in the sidebar. It links to our own content. I don't know the specifics but it links to the current, top content across the site. The hypothesis within the company is that defining recommended content based on WHERE the user came from may yield better results. For instance, giving different results to users from FBK, AOL, Direct, etc.

The plan is to take every single existing and newly created article currently and run 50% of the traffic to the existing rec widget and 50% to a custom widget. The custom widget would recommend different content depending on where the user came from. We will have 1) one set of recommendations for paid FBK users, 2) another for AOL users, 3) another for Organic FBK users and 4) another for everyone else.

Every time a user clicks on the widget we will track: 1) user source 2) content they clicked from 3) content they clicked to 4) whether they clicked on existing widget or custom widget 5) datetime of click.

Already, this is not a traditional AB test. We would basically be using each piece of content as a host for a variety of AB tests differentiated on user source. We would only compare click through rate between existing and custom widget within source bucket, so CTR of FBK users for existing widget would NOT be intermingled with CTR of AOL users from custom widget.

One parameter we could tune is the type of content we recommend. We can recommend content based on views, comments and how recently it was ""hot"". My thought would be to adjust these parameters only after the existing test has run for a while and basically having that act as a 3rd/4th test etc.

Although it varies on source, across all traffic sources we receive millions of visitors a week. We can run this as long as it takes to get enough data.

As stated originally, what are you thoughts with the setup itself? Is there anything invalid happening here? Thanks",My first AB test... is this a valid approach?,96yntp,new,3,0,0,0
"I am doing some basic research and I have 2 different groups A and B. By means of a survey I tested how each group scored on 4 aspects which consist of a score (calculated from multiple questions). 

I want to see which group scored highest on which aspect. I also want to see if these highest scores are statistically different. Or is it better to compare all 4 scores of group A with all 4 scores of group B, if so, how?

What test do I need if it is normally distributed, what if it is not normally distributed. 

Thank you so much for helping this confused student. ","Testing how high 2 different groups score on 4 categories, which test?",96ygvl,new,0,0,0,0
"Hi,m   as  you know  one  of the assumptions  of  a linear regressione is  that  there must be no  correlation.

Sorry  for my ignorance (I am  at the beginning),  but  which  kind of correlation do they refer to? 

Let's  suppose I want to predict  weight   (Y)   based  on  height (X)...   

let's  assume  that  before  to run the model I don't  know  if they are  linear  or not.   And  I would  just  to spot  if there is  autocorrelation  because  if  any, I can't make a  linear regression model.

  
Here  the question:  


Well  what  the auto-correlation is supposed to be?  Is it  the  correlation between  the  values  in the  predictor (X)   and  the  values in the Y?

Or  is  it  the  correlation between   the values  of  the  X only?  (The correlation between the values of the heights... so 1,75 correlated to 1,80cm, and  so on...)

Basically, as an  assumption of  linear model, there must  not  be correlation,  

but  correlation  between  what?  X and Y?  This is what I don't understand",Autocorrelation,96xfzp,new,12,1,1,0
"Apologies for the basic question but I'm just not sure I'm using the right method:

Every month we check the proportion of sales returned by team as a quality check. Generally the returns are around 1%, one month a team got 1.8% returned. Should I use Poisson probabilities using lambda as the expected returns for that team based on sales (ie sales x 1%), vs. Their actual returns? Or is there a more appropriate method?",Evaluating monthly return outliers - Poisson?,96xcc3,new,3,3,3,0
"Hi! Apologies if there's some really obvious resource linked here I'm overlooking. I also wasn't sure whether to post this here or r/AskStatistics, so I understand if this has to be deleted!

My only stats background is a class I had to take as part of an undergraduate biology degree, and I'm now attempting to learn statistics for fun. I tried Khan Academy, but felt a little bored with the pace it's taught at. I'm currently taking a course on Coursera and really enjoying it. But I still feel really lost with resources as I'm not sure which are reliable and which aren't, or where to look. So I wondered if anyone had any recommendations please (textbooks, websites, etc, anything that helps gratefully taken on board!)?",Does anyone have any resource recommendations for self teaching statistics?,96x4yn,new,7,13,13,0
"I'm currently struggling with my PhD and just wanting some light ""statistical viewing"" material to get my brain ticking over again.  


Are there any good places to find lecture series or the like? I'm wanting to watch people explain statistics in an accessible way so that my brain is getting a rest while also staying within the correct head space.  


Not really wanting Khan Academy-style stuff (or MOOC videos), hopefully something more similar to podcasts where they go in depth on a specific subject area with an interviewee or the like.",Suggestions for Statistics Lecture Series or Similar,96uxi7,new,10,8,8,0
I'm going through a statistics text so that I could better apply statistics to my profession (geologist working in environmental consulting). There's a chapter on inference for categorical data which seems more relevant for the social sciences. I'm probably going to skip it since my focus is on data analysis of environmental data (e.g. Soil contamination and geostatistics. Am I wrong in my assumption? ,Is Inference for categorical data useful for statistics work focused on spatial statistics and environmental science?,96sixi,new,3,5,5,0
Variables can be (a) continuous but ordinal such as an attitude measure where the response is on a continuous scale (location marked on a line) and (b) discrete and interval if each step on the discrete variable represents an equal distance on an underlying variable. ,The concepts of ordinal and continuous are orthogonal.,96pzmn,new,13,0,0,0
"Hi everyone,


I was reading a paper on the effects of import competition on firm-level employment growth. This included a regression of firm-level employment change on import competition from various regions, some control variables and interaction effects between the technology level of a firm and import competition. I won't go into detail about the exact specifications, because I am mainly confused about the interaction effects:

The model uses 4 technology dummies: Low-tech, Medium-low-tech, Medium-high-tech, High-tech for the technology level of the firm. Low-tech is the reference category here.


The following results were found; where x stands for the interactions:



Dep. variable | Employment growth
---|---
Import competition China | -1.2538***
 |(0.2876)
x Medium-low tech | 1.0231**
 |(0.4726)
x Medium-high tech | 0.9950*
 |(0.3032)
x High-tech | 0.3843
 |(1.0380)


So I know how to interprete coefficients of interaction terms. But the following statement made by the authors left me confused:

The only group of firms that is significantly affected by Chinese import competition are the low-tech firms (the reference category). For all three other groups, the sum of the coefficient of the reference category and the interaction term is never significant.

So, my question is why exactly? How do I know if the sum of two significant coefficients is also significant?

Hopefully someone can help me out here, thanks!",When is the sum of the coefficients of the reference category and an interaction term significant?,96ob48,new,5,1,1,0
"Hi! I was wondering if there's a name to this method of feature extraction (used in machine learning):

Let's say there are *k* features,  
1. The fisher's ratio of each feature is computed then used to rank the features (from highest to lowest fisher's ratio) 

2.  The top (*k*\-1) features are used to train the machine and accuracy is recorded

3. The top (*k*\-2) features are used to train the machine and accuracy is recorded

... etc until only the best feature is used, then the combination of features with the highest accuracy is deemed as the best combination of features. 

Thank you!",Feature selection by Fisher's Ratio,96o17q,new,2,6,6,0
"Looking for some guidance on how to go about developing a model for this: 

I have a categorical outcome (O0 or O1) that I'm trying to predict based on multiple continuous (D1,D2) and categorical data (D3, D4). 

My end goal is to develop a decision tree such that entering the values for D1-4 will give me the predicted outcome with various levels of accuracy (95, 99, etc). 

Could someone please point me in the right direction for this project? ",Multivariate Prediction Model Help,96mwmt,new,6,2,2,0
"I’m doing a problem out of a book and need an explanation as to why the answer is what it is.

I’m asked to write a multiple regression equation for a two-factorial design with 2 levels for Factor A and 3 levels for Factor B.

For Factor A I have 1 dummy variable x1 where, 1 if level 2 & 0 otherwise

For Factor B I have 2 dummy variables x2, where 1 if Level 2, 0 otherwise; and x3, 1 if Level 3, 0 otherwise.

The multiple regression equation I wrote was:

E(y)=B0 + B1x1 + B2x2 + B3x3 + B4x1x2 + B5x1x3

The correct answer in the book is:

E(y)=B0 + B1x1 + B2x2 + B3x1x2+ B4x1x3

I’m not sure as to why there’s not a parameter for the second dummy variable for Factor B

Any simple explanation would be very helpful. Keeping things as simple as can be as helped me a lot in getting through statistics ",Multiple Regression with 2 Factors,96lb4q,new,3,0,0,0
"I'm trying to determine the percentage of American names with suffixes like Jr., III, IV, V, etc.  What is a good source to use for this?",How do I determine how many Juniors there are in America?,96hod8,new,8,4,4,0
"Could  you help me   with this problem?

In a tennis match player A has 40% chance of winning, while player B has 60% chance of winning.

It  has been calculated (from previous games) that if player B does not  close the set within 1h his chances of winning decrease on  average by 5% for every 5 minutes of game, while the odds of winning  for his  opponent increase by 4% for every  point in a row   he makes. Calculate   player B's chances of winning after 1h and  20 minutes,  knowing that player A  made 5 points (in a  row).

Should I use the conditional probability in this  case or  am I wrong?

How could I do?  I don't  know  how to write E1  and  E2 (events).  They  are dependent  each other",Conditional probabilities,96fpyc,new,15,0,0,0
"An electronics firm recently introduced a new amplifier and warranty cards indicate that 10k of these have been sold far. The president of the firm, very upset after reading 3 letters of complaint about the new amplifier , informed the production manager that costly control measures would be implemented immediately to ensure that the defects would not appear again.

Are these answers right? Need your comments.

QnA:

Tests for data:

1	Where did the data come from? Is the source biased?

Ans:	 Data came from letters of complaint so unless the letters have proof of purchase for new amplifiers we won't know if they are genuine and for new amplifiers.

2	Do the data support or contradict other evidence we have?

Ans:	 We don't have any past data or evidence so we can't draw any conclusion from the present data.

    If the question mentioned on an average the defect percentage is 0.5% then we can say that it is under normal conditions.

3	Is evidence missing that might cause us to come to a different conclusion?

Ans:	Yes evidence is missing about whether other amplifiers are defective or not? What if others didn't chose to write the complaint letter?

4	How many observations do we have? Do they represent all the groups we wish to study?

Ans:	We have total number of observation = 10000

    Yes, the manager is only interested in knowing the defect level of new amplifier.

5	Is the conclusion logical? Have we made conclusions that the data do not support?

Ans:	No it's not logical because the defect level is below 0.03% from the present data. Generally 0.3% defect level is acceptable according to six sigma technique.

Current defect %=	0.0300%",Tests for data.,96fjvt,new,1,0,0,0
Can anyone help me understand why the standard error equals the population standard deviation divided by the square root of the sample size?,Central limit theorem standard error question,96fiy6,new,2,21,21,0
" ""When asked what they would use if they were marooned on an island  with only one choice for a pain reliever, more doctors chose A than B, C  or D.""

Is this conclusion drawn from population or a sample?

I am confused here because how do we know if on an island there were all the doctors that they could've asked the question?

\--------------------

""25% of the cars sold in US in 1996 were manufactured in Japan.""

Even for this statement how do we know if it was drawn from sample or population?",Is this conclusion drawn from sample or population?,96f75i,new,8,0,0,0
"Basically I am trying to figure out a very simplified model of expected payout for things like hearthstone arena or Mtg drafting. In these games you play until you lose 3 games or until you win a set number, for now lets say 7. I want to know what the odds of achieving each payout are assuming 50/50 chance of winning each match (between 0 wins to 7 wins).

At first I was using Binomdist function in excel/open office but realized it was over counting, for example it was adding percentage for losing 3 games and then going on to win 4, when in reality you would have a trial end after 3 losses. If there are any other functions someone could point me to that would work for this i'd be happy to do the legwork myself.

TLDR: Odds of getting each possible number of heads (0-7) when flipping a coin assuming the trial immediately ends if you ever flip either 3 tails or 7 heads (doesn't have to be consecutively) . Thanks!  


Edit: I tried to do most of it by hand and found a pattern, after fixing it as much as I could i'm stuck. Here is a link to a google doc with my result and method: [https://docs.google.com/document/d/1Sq8DbXmH-UFS2rZz6StHx51zwpkbC2vn\_LlLl2wQl4w/edit?usp=sharing](https://docs.google.com/document/d/1Sq8DbXmH-UFS2rZz6StHx51zwpkbC2vn_LlLl2wQl4w/edit?usp=sharing)  
Any help is appreciated. This is going to drive me nuts, it's already 6AM haha.",[Statistics Question] How to figure out the odds of each possibility with multiple end states.,96exx2,new,3,1,1,0
,Why use PCA functions when you can use SVD functions?,96a933,new,1,0,0,0
how can i teach my statistics? from basic to advance topics ,self educate,969rdq,new,4,0,0,0
"I'm a rising senior in high school and I took ap stats at school my junior yr. I passed the ap exam with a 4 and the class with a 90. Both of these done with minimum work. I also like statistics, I mean from all I have learned so far (I know I have a lot more to learn). Right now I am confused on what to do my bachelor's in when I do go to college. So I was considering statistics. Is this a good idea (I do plan on doing a master's but I don't know what pairs best). Also how could I break in to the finance world through statistics. What steps would I have to take schooling and experience wise. Also what are the highest paying jobs statistics has to offer in the next 10-15 yrs.",Is bachelor's in statistics something I should look into.,969omu,new,46,12,12,0
"I am analyzing a questionnaire that has a Likert Scale from 1 to 5 as the dependent variable and one independent variable that has two categories and another independent variable that has 5 categories and was researching how to analyze this.

Because Likert Scale data is Ordinal in nature I have been seeing a lot of people say that non parametric is the way to go but there is a group of people that say you can use parametric tests and there is also an article online that tells the assumptions that need to be made in order to use parametric tests: [http://www.sim-one.ca/community/tip/analyzing-likert-scale-data-rule-n30](http://www.sim-one.ca/community/tip/analyzing-likert-scale-data-rule-n30)

I personally am leaning towards doing ordinal regression but wanted to hear other peoples thoughts and advice when it comes to analyzing Likert Scale data and whether or not the assumptions made in the article are correct",Non-parametric or parametric tests when it comes to Likert Scale data?,968n4b,new,14,13,13,0
">In searching for areas of fiscal restraint, U.S. statistical agencies, which provide much of the information that helps us understand the American economy and society, can be seen as easy prey for the budget axe. That would be pennywise and pound foolish, and a blow to the very information needed to evaluate whether government programs are working properly, how the economy is faring, and ways that our society is changing.  
>  
>Statistics produced by the federal statistical agencies are used to inform decisions made on Wall Street, Main Street, at both ends of Pennsylvania Avenue, and in local governments across the country. To make the best decisions possible, we need a core set of objectively produced facts that we agree are free from political influence. ...

[https://bipartisanpolicy.org/blog/why-the-american-public-should-trust-key-national-statistics/](https://bipartisanpolicy.org/blog/why-the-american-public-should-trust-key-national-statistics/)",Bipartisan Policy Center: Why the American Public Should Trust Key National Statistics,968job,new,1,21,21,0
"set.seed(357)

Ng <- 100 # number of cases per group

group.a.x <- rnorm(n = Ng, mean = 2, sd = 3)

group.a.y <- rnorm(n = Ng, mean = 2, sd = 3)

group.b.x <- rnorm(n = Ng, mean = 11, sd = 3)

group.b.y <- rnorm(n = Ng, mean = 11, sd = 3)

group.a <- data.frame(x = group.a.x, y = group.a.y, group = ""A"")

group.b <- data.frame(x = group.b.x, y = group.b.y, group = ""B"")

my.xy <- rbind(group.a, group.b)

\# construct the model

mdl <- lda(group \~ x + y, data = my.xy)

\# draw discrimination line

np <- 300

nd.x <- seq(from = min(my.xy$x), to = max(my.xy$x), length.out = np)

nd.y <- seq(from = min(my.xy$y), to = max(my.xy$y), length.out = np)

nd <- expand.grid(x = nd.x, y = nd.y)

prd <- as.numeric(predict(mdl, newdata = nd)$class)

plot(my.xy\[, 1:2\], col = my.xy$group)

points(mdl$means, pch = ""+"", cex = 3, col = c(""black"", ""red""))

contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np),

levels = c(2), add = TRUE, drawlabels = FALSE)

This creates a scatter plot of all the (x,y) points, different colors if group A or group B. However it also makes a singular contour line. This is the part where I am confused. The lda model predicts that either the (x,y) point is in group A or B, so there is only two possible z values. I understand that it should only be a single line, as it is the discriminant line that separates group A's from group B's. But every single point is a 1 or 2, how does R know to plot only a single line?",How does R know to only make a single contour line?,968ecb,new,1,3,3,0
"Can someone help me understand what information the Two-factor ANOVA gives? I have tested my dependent variable with 10 samples for each set. I want to know if there is a significant difference for 50% at different cure times, if there is a significant difference for 100% at different cure times, if there is a significant difference for 125% at different cure times. If there is a signficant difference for 4hr cure at different volumes, etc.... I didn't copy all the data, I actually have more samples per experiment. If I do a single factor ANOVA  6x (50%-different cure times, 100%-different cure times, 125%-different cure times, 24hr-different volumes, 72hr-different volumes, 168hr-different volumes, 300hr-different volumes), I get that there is a signficant difference between the means for 50%-different cure times, 100% -different cure times, 125%- different cure times. For the others, I get there is no significant difference. 

When I do a two way ANOVA, formatted as below in excel, I get that P> 0.05 for ""rows- volumes"", and P < 0.05 for ""columns ->cure times"", P> 0.05 for the interaction term. What exactly is the two-factor test telling me? It seems contradictory to the 1-way results. I may have understood this wrong, but since P>0.05 for the volume, it means that there is no significant difference between the volumes on my dependent variable for different cure times. Is a 2-factor anova appropriate here for the questions that I want to answer above. I have very limited statistics knowledge so if you could explain like I'm a child that would be great!

|24hr|72hr|168hr|300hr|
:--|:--|:--|:--|:--|
**50%**|10.0|15.0|15.0|12.0|
|8.0|7.0|14.0|4.3|
|5.5|7.5|3.0|2.0|
**100%**|13.3|14.3|6.0|14.0|
|8.0|12.0|3.4|18.9|
|7.0|14.0|21.0|6.0|
**125%**|13.0|3.1|6.0|14.0|
|14.0|3.0|8.9|3.3|
|5.4|9.0|5.0|5.9|


",Two Factor ANOVA Results,9673q3,new,0,2,2,0
"So i did a questionaire to my experiment in my thesis. That questionaire is based on this model:

https://imgur.com/a/kNqlsqM

So since i have now collected my data, i want to get the latent variables. But how do i calculate them? i can do a factor analysis myself but can't find proper python tools for that, anybody got suggestions? Also is building the model not some kind of overkill, since i got the relations and weights from the paper already? But how do i calculate the variables since they all are correlated?

TLDR: How do i calculate variables that correlate and does somebody know any python tools to do Structural equation modeling?",Calculating latent variables from a model that has correlations,966uc4,new,9,10,10,0
"I mean... of course there is. there's a calculation for almost everything. What is this type even called and how do I either do it or learn how to do it? 

I haven't studied statistics. ",is there any type of way of calculating how many people within a demographic will die based on how much time passes and where/what the demographics are?,964v6a,new,8,0,0,0
"Hi all,

I have a set of 20 gridded climate variables within the range of a species. I need to choose a subset of those variables based on the highest absolute loadings of PC1 from a PCA of those climate variables. There are too many observations of the variables within the range of the species to run PCA on all of them. My approach is therefore to randomly sample 1,000 observations, and run PCA on those. 

To show my results (the subset of variables with highest absolute loadings) are stable, I have repeated this process 10 times. The results are the same for all 10 iterations. I don't like the idea of simply stating that in all 10 cases the subset of variables is the same. Is there some more rigorous approach I could be taking?

Thanks!",Bootstrapping PCA results based on random sample,960r61,new,12,3,3,0
"<!-- SC_OFF --><div class=""md""><p>The reason I am looking for another option is that I do not have a good reason to choose a prior distribution for a Bayesian analysis. </p>
</div><!-- SC_ON -->

Edit: To clarify what I am after... I have a null result that, if genuine, would be quite interesting. I'm after some way to show with some confidence that there is no effect.","If I want to conclusively show that a result of mine is non-significant, is there any alternative to Bayesian statistics?",960nsb,new,36,9,9,0
"I hope this is a place for this kind of question - please forgive me if this is not the place for this (and please point me to a better suited subreddit if you know of one).

So I'm playing a mobile game that has a system setup of fusing experience containers into units to give the unit experience points. Each ""fuse"" has a 5% chance of ""great success"" which adds 1.5x the experience of the container and a 1% chance of ""amazing success"" which adds 2x the experience of the container. The catch here is that we can now fuse experience containers into other experience containers. What I'm trying to find out is how much extra experience we can get by fusing our containers first one at a time.

So let's say I have 100 separate experience containers and each one is worth 100,000 experience. With regular fusing the 5% chance of 1.5x and 1% chance of 2x should equate to a simple 3.5% extra experience. I don't know the statistics formula for this, but I calculated that out this way:

Say I fused 100 containers and I hit the expected 5% and 1% chances. That would look something like this:

* 100 \* 100,000 = 10,000,000 experience base experience from the containers.
* 5 \* 50,000 = 250,000 extra experience that we gained because 5 of that 100 gave 150,000 experience instead of 100,000.
* 1\* 100,000 = 100,000 extra experience that we gained because 1 of that 100 gave 200,000 experience instead of 100,000
* 10,000,000 + 250,000 + 100,000 = 10,350,000 which is our total experience gain.
* 10,350,000 (total) / 10,000,000 (base) = 1.035, so we gained 3.5% extra experience from our containers because of the great and amazing successes.

I think that's accurate, and wasn't too hard. Where I get stuck is where we can add extra great and amazing chances by fusing containers into each other one at a time. It would looks something like this:

* Fuse 1: Modifier 1 (94% chance of 1x, 5% chance of 1.5x, and 1% chance of 2x) \* 100,000 + 100,000 = 200,000 (for example's sake assume a 1x modifier)
* Fuse 2: Modifier 1 \* 200,000 + 100,000 = 300,000 (again assume modifier was 1)
* Fuse 3: Modifier 1.5 \* 300,000 + 100,000 = 550,000 (we got lucky and hit a 5% chance of ""great success"" for 1.5 modifier)

So as you can see, we greatly increase our experience because every single container from the previous fuse gets a chance to get 1.5x or 2x modifiers (excluding the target container which always adds a flat 100,000). What I'm trying to figure out is a mathematical formula for this because I want to know how much extra experience we are getting for this kind of fusing. Without it we get 3.5%, with it we get how much extra % do we get?

A wrench in this formula is the level cap. A container can not hold more than 4,500,000 experience. Part of the problem is knowing when to stop and start on a new container because we're at risk of a great or amazing success hitting our level cap, in the interest of maximizing experience gain from smart fusing.

While I am not a mathematics major I am a computer programmer, and I tried to get these numbers by hitting them with millions of simulated fuses and getting averages. The trouble is - so did other programmers! And our results vary by a lot. But if we had the mathematical formula behind what we're doing we wouldn't need to worry about simulation programs that probably have bugs in them and could get an exact amount.

I'm deeply appreciative of any help. Thanks!",Need help in figuring out a formula for this problem,960io9,new,5,2,2,0
"Hi all, 
I was wondering how to calculate the standardized incidence ratio for a number of variables (age, year and gender). I’m using STATA or spss. 
I want to compare the cancer incidence in my cohort with the national van cancer incidence per 100.000. It’s difficult because it differs per year and per age category. 
Thank you in advance!",standardized incidence ratio,95zx5p,new,2,1,1,0
"I am reading Introduction to Statistical Learning, and at the end of each chapter there are labs in R. For LDA in the lab we find the coefficients of linear discriminants. So say the predictor has elements x1 and x2, the book then says the observation belongs to class 1 if the sum of the products of the elements and their corresponding coefficient is largest, and the other class if it is low (only 2 classes). 

The book says these coeffecients are present in equation:

 δk(x) = xT (Σ\^−1)μk − 1/2 μT k (Σ\^−1)μk + log πk  where xT is the transpose of the predictor vector, μk is the mean vector for class K, and Σ is the covariance matrix. However, I don't see where these coefficients are present. ",What are the multipliers of predictor vector x in linear discriminant analysis?,95zsmx,new,0,1,1,0
" I am working on a project that involves case-control design. Participants with and without a particular intervention (receiving treatment X or control)) are matched on sex, diagnosis and age (+/- 7 years) and self-reported questionnaires are used to collect data related to various health outcomes.  


The outcome variables are the count data (ranging from 1 -7) with most having 0, 1 or 2 events (Poisson distribution). Because the data are matched I need to account for the correlated data and I am wondering what an appropriate analysis would be if I am interested in examining a model for both conditional and marginal distributions. I have looked at General Estimation Equations or Generalized Linear Mixed Model but need some guidance on which approach may be most appropriate.  ",Analysis of case-control,95zo1v,new,1,2,2,0
"So, my lab mate has a project she needs to run characterizing printing parameters for an experimental ink formula and printer setup. It has four dependent variables, and seven independent variables.  She would like to know what the optimal settings are for the four dependent variables.

Samples are time consuming to make.

My current plan is to use [response surface methodology](https://en.wikipedia.org/wiki/Response_surface_methodology).  In the first step, we would screen independent variables using a 1/4 fractional factorial DoE and use regression to characterize explanatory variables. We would remove variables from the second round if the p-value and effect size are both insignificant (a hybrid of the backward selection algorithm). I will also consider reducing VIF when choosing variables to remove. Second, we would use a full factorial design to characterize the surface.  Alternately, I would use a central composite design, relying on the scarcity of effects principle.

For the fractional factorial, I was considering a 2^(7-2) design (1/4 factorial) with five replicates for a total of 160 samples.  If possible, I was wanting to make all five replicates in a single batch, with a total of 32 batches.

In the follow-on full factorial, assuming only three factors survive, we would then test 3 levels, with five replicates.  This should mean that we would need to make 27 more batches, again assuming each replicate comes from the same batch.

I am sure there are things I am not considering, and I would love help knowing what they are.

Any suggestions?",Need help double checking my design of experiment,95zk3b,new,5,3,3,0
"Imagine that I have predictors A, B, C and D; with predicted value X.

I am interested in how well D predicts X, over and above predictors A, B, C. 

Is it enough to run a multiple regression model with all four predictors, and then interpret the coefficient for predictor D? Or, is it better to run a model with and without predictor D, and then compare the two (e.g., with an F-Test and R2 change)?  


What are the advantages/incremental pieces of information that either approach provides?",Examining the effect of one variable over and above others in a regression model.,95zivt,new,5,2,2,0
"I am used to the definition of IQR as Q3-Q1. However, I ran into this other definition the other day:

""In some texts, the interquartile range is defined differently. It is defined as the difference between the largest and smallest values in the middle 50% of a set of data."" \[[1](https://stattrek.com/statistics/dictionary.aspx?definition=interquartile%20range)\]

Is there any specific analysis where this other definition is used? Is  there any scenario where it is better suited? Any comments on that are welcome.",Different definition for Interquartile Rage (IQR),95yeab,new,7,3,3,0
"My tetxbook calls it a covariance matrix, but all the values are between -1 and 1. Is it a matrix of correlation terms between the ith and jth variable?",Is the covariance matrix a matrix of correlation values?,95xpr7,new,7,7,7,0
"Hi,

for my dissertation survey I used 5 scales. For one of the scales the data is not distributed normally (Kolmogorov- Smirnov test highly significant) so I tried to solve this by transforming the scale (log transformation). After that the distribution still isn't normal. Skewness is below 1 but the KS test is still highly significant. Therefore I chose to do a Mann-Whitney test instead of a t-test.

All the other scales are normally distributed so when I compare the concepts I tested with each other can I still use t-tests for them and whenever I want to compare a scale to the one mentioned above (the non- significant one) use a Mann- Whitney test?

What do I take into consideration when interpreting the results?

Thanks a lot for any advice & help!",Am I doing this right?- transformation and test choice,95xhe9,new,3,5,5,0
"What I got so far was:
It is a measure of estimate quality. 
It is somehow related to the COV matrix. 

For what do I need it?",I just don't get the Fischer Information,95x3uo,new,26,32,32,0
"Hi there,

Need to do a logit transformation of a variable, so that I may test the assumptions for a Binary Logistic Regression test. Could someone please tell me how to do that? I know when transforming I need to input 'Ln(p/1-p)' in the compute variable screen, but I have no idea what 'p' is, or how to calculate it!

Any help much appreciated!",How to do a Logit Transformation in SPSS?,95x2gd,new,5,1,1,0
"Hello,  this  nice  website  explains trought a  Markov process  the  game  score  in tennis  and  the passage from one  state  to another. It's   a very nice  to read.

[https://tobykingsman.wordpress.com/2016/02/17/markov-chains-and-tennis/comment-page-1/](https://tobykingsman.wordpress.com/2016/02/17/markov-chains-and-tennis/comment-page-1/)

But  I don't  understand  one thing and I  am getting confused...  He says (you can read it on the  website)

''So the final equation for the probability of Player A winning the game is:

[https://s0.wp.com/latex.php?latex=P+%3D+p%5E4+%2B+4p%5E4%281-p%29+%2B+10p%5E4%281-p%29%5E2+%2B+%5Ccfrac%7B20p%5E5%281-p%29%5E3%7D%7B1-2p%281-p%29%7D&bg=ffffff&fg=808080&s=0](https://s0.wp.com/latex.php?latex=P+%3D+p%5E4+%2B+4p%5E4%281-p%29+%2B+10p%5E4%281-p%29%5E2+%2B+%5Ccfrac%7B20p%5E5%281-p%29%5E3%7D%7B1-2p%281-p%29%7D&bg=ffffff&fg=808080&s=0)''

BUT

On this  website (stack  exange)

[https://math.stackexchange.com/questions/1126327/probability-of-winning-a-game-in-tennis](https://math.stackexchange.com/questions/1126327/probability-of-winning-a-game-in-tennis)

one guy asks  for the  same  question,  and  one other  guy using  Bernulli process  he  calculates  a formula  for  the probability  for  a  tennis player to  win the game and it  is:

[https://image.ibb.co/c86Wy9/Schermata\_da\_2018\_08\_09\_15\_05\_26.png](https://image.ibb.co/c86Wy9/Schermata_da_2018_08_09_15_05_26.png)

WHY  this  formula  is  different  from the  first one I have red on that  website?  (this below)

r/[https://s0.wp.com/latex.php?latex=P+%3D+p%5E4+%2B+4p%5E4%281-p%29+%2B+10p%5E4%281-p%29%5E2+%2B+%5Ccfrac%7B20p%5E5%281-p%29%5E3%7D%7B1-2p%281-p%29%7D&bg=ffffff&fg=808080&s=0](https://s0.wp.com/latex.php?latex=P+%3D+p%5E4+%2B+4p%5E4%281-p%29+%2B+10p%5E4%281-p%29%5E2+%2B+%5Ccfrac%7B20p%5E5%281-p%29%5E3%7D%7B1-2p%281-p%29%7D&bg=ffffff&fg=808080&s=0)

I can not  explain  that  ''10'' and  ''20''...  where do they came from ?",Different tennis odds formulas,95waen,new,2,0,0,0
"So I’m trying to create a code where I run a logistic regression with my dependent variable representing participation (1 if yes 0 no) I have a lot of independent variables which are basically attributes of these units like age, assets etc.

I am currently confused about what variables should I choose as part of the independent variables that go into the logistic regression. One way is that I select every attribute available and run a regression in which case there can be overfitting of the model. Since the main harm of overfitting as per my knowledge is reliability of predictive power of the model and PSM is not used for doing any out of sample prediction it should not give me any problem. Is this correct? Are there other consequences of overfitting in PSM which I am neglecting?

If there is indeed a problem of overfitting in using propensity scores to balance then what should be the approach to select the best set of variables.

Apologies if the question is not clear. I am not an expert in Statistics. Happy to clear any question that I have left unanswered.

Thanks!",Is overfitting really a problem while creating a Propensity Score Model for balancing of populations?,95vrdn,new,5,2,2,0
What  are the  ''features''  in  a  random forest model?  I  read  this  term everytime  I  read  about  machine learning  models /   algorithms.  I don't  know  what do they  mean  with  ''features''...  could  you  please make  some  simple examples?,Features random forrest,95v6td,new,8,1,1,0
"Hi there,

I've got time data (it represents time spent looking at an exhibit in the zoo). I need to log transform it in SPSS, but it changes the format. E.g. 0:49.80 to 1.70. I would presume this is no good to me for statistical tests, is that correct? How do I keep the min:sec.millsec format?

The variable of my time data is Type: Date Measure: Scale, but when I have tried Log10 it alters to Type: Numeric Measure: Scale.

Many thanks for any advice!",Can/How do you log transform time data in SPSS?,95v0ut,new,4,1,1,0
"Hey everyone, I'm an undergraduate student trying to figure out whether to take a class in *Introduction to Time Series* or *Spatial Statistics*. Based off brief glances on the Wiki pages, it looks like spatial statistics is much more ""visual-based"" compared to time series.
 
What are the primary differences between the two areas? Which of the two would be more beneficial to know if I'm interested in pursuing a career in data analysis or data science? ",Time Series or Spatial Statistics?,95ub06,new,10,9,9,0
"I am going over linear discriminant analysis and the way my textbook does it is the math isn't really explained when it can be done on software, but I want to have some intuition. I understand that if X is a single variable and the distribution is N(μ,σ\^2), then the average of X is μ and the variance is σ\^2. But when X is multivariate with a Gaussian distribution, what does it mean when the distribution is N(μ,Σ). I understand that μ is the average vector, in the LDA case it's the mean vector for a specific class K. But what exactly is the variance of each component? So say the mean vector X has components x1, x2, and x3. The diagonal of the covariance matrix tells us the variance of x1 and x2 and x3 correct? So why is it a matrix of values, and not just another vector where each term is the variance of x1, x2, etc. How do the other terms in the matrix factor in to telling us how much the values of the component will vary? Thank you!",What does the Covariance matrix exactly tell you about the distribution of the mean vector?,95qjsb,new,8,11,11,0
"reading  this  on this  website

[https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/how-to/fit-poisson-model/before-you-start/example/](https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/how-to/fit-poisson-model/before-you-start/example/)

I have  an examplr:

A quality engineer is concerned about two types of defects  in molded resin parts: discoloration and clumping. Discolored streaks in  the final product can result from contamination in hoses and from  abrasions to resin pellets. Clumping can occur when the process is run  at higher temperatures and faster rates of transfer. The engineer  identifies three possible predictor variables for the responses  (defects). The engineer records the number of each type of defect in  hour long sessions, while varying the predictor levels.

The engineer wants to study how several predictors affect  discoloration defects in resin parts. Because the response variable  describes the number of times that an event occurs in a finite  observation space, the engineer fits a Poisson model. The engineer wants to study how several predictors affect  discoloration defects in resin parts. Because the response variable  describes the number of times that an event occurs in a finite  observation space, the engineer fits a Poisson model. The plot of the standardized deviance residuals versus the fitted values  shows a distinct curve. In the plot of the residuals versus order, the  residuals in the middle tend to be higher than the residuals at the  beginning and end of the data set. For these data, both patterns are  because of a missing interaction term between the size of the screw and  the temperature. The pattern is visible on the residuals versus order  plot because the engineer did not collect the data in random order. The  engineer refits the model with the interaction between temperature and  the size of the screw to model the defects more accurately

[https://support.minitab.com/en-us/minitab/18/poisson\_regression\_resindefects\_fourpack\_plot.png](https://support.minitab.com/en-us/minitab/18/poisson_regression_resindefects_fourpack_plot.png)

For the model with the interaction, the AIC is approximately 236, which  is lower than the model without the interaction. The AIC criterion  indicates that the model with the interaction is better than the model  without the interaction. The curvature in the residuals versus fits plot  is gone. The engineer decides to interpret this model rather than the  model without the interaction.

[https://support.minitab.com/en-us/minitab/18/fit\_poisson\_correct\_model.png](https://support.minitab.com/en-us/minitab/18/fit_poisson_correct_model.png)

My question is  in this:

''For these data, both patterns are  because of a missing interaction term between the size of the screw and  the temperature. The pattern is visible on the residuals versus order  plot because the engineer did not collect the data in random order. The  engineer refits the model with the interaction between temperature and  the size of the screw to model the defects more accurately''

**1)** ''Missing iteraction term between size of screew  and temperature'' -... what  does this mean?  What  is an iteraction therm?  An example?

**2)** ''The pattern is visible on the residuals versus order  plot because the engineer did not collect the data in random order. The  engineer refits the model with the interaction between temperature and  the size of the screw to model the defects more accurately''

\+What does  it  mean did  not  collect  the data in random order?  What  does this  mean??  

''the engineer refits the model with the interaction between temperature and  the size of the screw''  HOW can  he do this?  How  does  he  takes  this  iteraction term?    

I don't  know the pratice, where   does it take  the  iteraction term, and  how  does he ''refits''  the model?  Does  he make again  temperature  vs screew poisson model inserting iterataction term?  ",What's iteraction?,95pjdj,new,1,0,0,0
"I was just wondering if anyone knew of any good outlier detection methods for strings? Preferably one that takes character positioning/typical structure into account (e.g. where ""A012"" =/= ""012A"").

I'm trying to find a way to help automate the detection of (potentially) erroneous data in our database, and a lot of our fields are strings.

Thanks in advance for any insight you can provide!",Outlier Detection Method For Strings?,95onv8,new,3,2,2,0
"I am working through the most famous texts on mediation analysis, and I am realizing the importance of a better understanding of regressions - especially logistic (seen with binary outcomes/mediators) and non-linearities (such as when there is expos-mediator interaction). Since the coeff's of the regression analysis quickly yield the NDE/NIE/CDE after almost no manipulation (only simple arithmetic), I am trying to better understand what is happening behind the scenes. My computer science background isn't rigorous and is mostly the result of wikipedia results. Is there a concise work? I'm usually in SAS or STATA. ",Regressions for Causal Inference/Mediation Analysis,95o2z8,new,3,1,1,0
"Been taking an online stats course for my major this summer. I wish I had taken this class during a regular semester because it's been very difficult to remember everything in a 7 week period. I would do very well, but I keep making little mistakes on tests and ruining my grade. 

I've been using excel to break down formulas and check my work. I thought it was helping, but now I have formulas that I can't even remember. I really need to rewrite my formulas in a coherent space and I was wondering if there was just a page with basic formulas on it so I could just format my notes without forgetting anything.

Just looking back at my last test, I scored a 72 where I could have easily had a 95+ if I didn't make stupid mistakes like using the wrong formula (when I had the correct formula right next to it in my excel notes), mixing up the definitions of skewness, rounding .0001 off because mystatlab is so anal about that...

The pressure of a timed test really gets me with maths, I have no problem with written tests in any sort of non mathematical course, but I just constantly make little mistakes that snowball.",small mistakes on tests in intro to stats is costing me big time. Is it wrong to ask a professor if I could retake a test?,95nyav,new,5,0,0,0
"## 

I am  studinf SDE

Could you  help me  outP?

From wikipedia:

[https://wikimedia.org/api/rest\_v1/media/math/render/svg/229ccc03be52a7e7c194775d970a2640176a1424](https://wikimedia.org/api/rest_v1/media/math/render/svg/229ccc03be52a7e7c194775d970a2640176a1424)

I  would like to know:  what  is that  ''d'' ?

What  is the meaning of  the ''d''  term  whcih is  next  to the t?",Stochastic differential equation,95ntkm,new,1,0,0,0
"I have conducted 6 paired samples t-tests on a data set with 6 within subjects variables. Not one of these was found to be significant (which I wanted), but when using a Cohen's dz calculation of effect size, there are some medium effect sizes. However, I am not confident in this calculation as it stands to reason that there should be some correction to be applied with multiple comparisons?",Is there a correction (similar to Bonferroni for multiple comparisons) but for effect size?,95no11,new,5,5,5,0
This person has a pdh. Does a lot of statistics. a lot more than me. However I've been thinking about this and I can't think of a way this is possible. I don't think his conclusion is valid either. So any of you guys an idea of how a study like this could be set up? I really don't think this is possible with what i've learned. I think this person has been influenced by racist propaganda tbh.,Someone is trying to convince me that it is possible to statistically confirm a causal relation beween ethnicity and IQ,95nhc0,new,230,34,34,0
So I want to learn statistics for machine learning.I have only a few books available in the library and the one I have now is Miller and Freund's Probability and Statistics for Engineers.I can't really afford any book right now.So do I start with what I have or watch videos or something?,What books do you recommend for begginers?,95n6bt,new,6,5,5,0
"I have sets of data using a fertilizer at various time points (4 different times) and various volumes (3 different volumes). I have another set of data using another fertilizer at various time points and various volumes (same time points and volumes as the other set, just different fertilizer). I have 10 data points (measuring fertility) for each experiment (24 sets of experiments). I want to compare the fertility and a single volume for various time points. I also want to compare the fertility for a fertilizer at one time point for various volumes. Is a ANOVA test appropriate for this and how could I implement this in excel?",ANOVA Test,95m3ma,new,23,2,2,0
"Hi All,

I am completing an assignment for my masters and the question is whether my data is normally distributed or not. The data is related to thigh girth measurements of males/females and there are about 500 observerations.

I've created the following charts in R and am about to conclude that they are in fact like a normal distribution. However the female QQplot chart show significant deviation from the QQline. it bearly covers from -1 to 1 quantitle, and everything outside that is quite far from the line. Is this the right conclusion to make?

    hist(x = dataset_male$thi.gi,breaks = 30, prob = TRUE, main = ""Histogram of Male Thigh Girth"", ylab =""Probability"", xlab = ""Girth in CM"", col = ""cornflowerblue"")
    curve(dnorm (x, mean = mean(dataset_male$thi.gi), sd = sd(dataset_male$thi.gi)), col=""darkblue"", lwd=2,add=TRUE,yaxt=""n"")
    hist(x = dataset_female$thi.gi,breaks = 15, prob = TRUE, main = ""Histogram of Female Thigh Girth"", ylab =""Probability"", xlab = ""Girth in CM"", col = ""lightpink"")
    curve(dnorm (x, mean = mean(dataset_female$thi.gi), sd = sd(dataset_female$thi.gi)), col=""darkblue"", lwd=2,add=TRUE,yaxt=""n"")
    qqnorm(dataset_male$thi.gi, col=""cornflowerblue""); qqline(dataset_male$thi.gi)
    qqnorm(dataset_female$thi.gi, col = ""lightpink2""); qqline(dataset_female$thi.gi)

[https://imgur.com/LNG6Js0](https://imgur.com/LNG6Js0)

[https://imgur.com/UcjYym1](https://imgur.com/UcjYym1)

[https://imgur.com/QDX3kD1](https://imgur.com/QDX3kD1)

[https://imgur.com/GMxA6dK](https://imgur.com/GMxA6dK)",Is my data normally distributed?,95i05c,new,22,3,3,0
Class starts in 2 weeks and nearly everyone I've spoken to said statistics is difficult. How can I prep myself so that I'm not completely blind going into the semester? ,"Advice, Elementary Statistics course in college",95hrcr,new,14,2,2,0
"Hi. I am in need to do cross correlation on several time series to see if there is a consistent lag time between peaks in the different series. It's my understanding the time series need to be stationary to make the analysis robust. 

However, my time series rarely contain more than 20 observations. As this may be important: The time series are radar data, so the observations are taken roughly every 5 minutes, with no series extending over 2 hours. 

The lack of length in the time series makes me think it's unlikely the series could be considered stationary. However, do I even need to worry about the series being stationary with such a small data set? I found some comments elsewhere on the internet indicating such small sets do not need to consider this, but I'm unsure if they were just conjecture or based on robust theory. 

Thanks.",Cross Correlation and Short Time Series,95fk8s,new,0,1,1,0
"The stock of a warehouse consists of boxes of high, medium and low quality light bulbs in

respective proportions: 1 : 2 : 2. The probability of bulbs of three types being unsatisfactory

is 0.0, 0.1 and 0.2 respectively. If a box is chosen at random and two bulb in it are tested and

found to be satisfactory, what is the probability that it contains bulbs (1) of high quality, (2)

of medium quality, (3) of low quality","[HELP] What is the probability that the chosen satisfactory bulb is of high, medium or low quality?",95ee7o,new,1,0,0,0
"My research project for this summer explored how children with autism improve in play skills and imitation over the course of an intervention. I have such a small sample size (8) that testing for significance is pointless. However, I want to use effect size to calculate how much they improved from week 1 to week 5 and then week 5 to week 10. How do I calculate two effect sizes for this? Do I need to? SPSS only shows one",Do I need to calculate more than one effect size?,95cbnm,new,3,9,9,0
"Hi, I  am  conducting a  chi-squared  test.

I have  the  liters  of  rain fallen, and  the  number  of  absence  of  workers  (workers who don't  go to work

This is only an excercise  to get  pratic, so  it's all immaginary.

number of liter fallen

32

44

54

number of  absents  people   at  work:

5

6

7

H.0 (null ipothesis):  the increasing of  the number  of  liters  of rain (more rain)   and  the work absences   are  not correlated and  it's  a case the  growning of  absences and  the growning of  rain

H hypotesys:  the number of  afllen liters of rain,   influences  the   absence  at work  and  it's not a  case.

I  should  have run a  F test (because  the data are  few data)

Chi squared

Independence test (Chi-square) Alpha 0.05  
df 2  
Value P 0.955324354855672  
Statistics of the test 0,09140871514642  
Critical value 5,99146454710798

I  have read that  the low the p-value the  better... I  am  asking to you, can I reject the null ipotesys  with  a 0,955  value?  The   rows (of tabelle)   are  3, the columns are 2.

I have  run the F-test ,  here the  results:

Test of F  
Alfa 0.05  
Variable 1 Variable 2  
Media 43.3333333333333 6  
121.333333333333 Variance 1  
3 Comments 3  
df 2 2  
F 121.333333333333  
P (F <= f) right tail 0.008174386920981  
Critical value of F tail right 19  
P (F <= f) left side 0.991825613079019  
Critical value of F tail on the left 0.052631578947369  
P to two tails 0,016348773841962  
Critical value of two-queue F 0.025641025641026 39",I am conducting a test (X^2 test),95c2xw,new,10,2,2,0
"I’m a seed biologist trying to get my head around a new way of scoring and analysing germination data. I have a bunch of treatments for which I have scored germination in binary but I’m having a hard time converting to binomial probability. 

Halp please! ",How to convert binary data into binomial probability,95aded,new,7,9,9,0
,What is the use/purpose of Industrial Classifications (ISIC etc)?,959pd7,new,7,0,0,0
"One of the assumption  of lm linear models  is  that the variance  must  be  Homoscedasticity

I don't  understand  to  which  variance are they  referring to.

In these  two examples:

[https://blog.minitab.com/hubfs/Imported\_Blog\_Media/fittedxobserved.gif?t=1533587587550](https://blog.minitab.com/hubfs/Imported_Blog_Media/fittedxobserved.gif?t=1533587587550)

the left one  the regression model shows  the 38%  of the variance, the right one shows  87%,  Then  I have red ''Bigger is the variance  showed  by the model, more the points get closer to  the fitted line''

I  really don't  understand  this  term:  variance  of residuals.  I don't  understand  the ''graphical out-put''.  The  variance is  ''HOW MUCH  a  variable  differ  FROM  the  MEAN'' ,

And I  have understood  this.

But  what is (graphically)  the variance of  the residuals?  Is  it  the measure of ''how much'' are  they far  from the fitted line?

I would need  a graphical example to understand",How do I verify Homoscedasticity BEFORE to make a linear regression?,959a5a,new,10,10,10,0
"I also posted this in /r/AskStatistics but maybe someone here can help me...

I'm currently working on an observational study that involves taking tissue samples (nerve and muscle) and observing the degree of nerve damage/muscle atrophy in relation to the height of the animal. I've been given an assessment as part of my study that requires me to state how my sampling is structured (e.g. any groupings in data evident and how to eliminate any variation by arranging samples in a certain way). I've identified height as the independent variable as height determines nerve length which in theory determines nerve susceptibility to damage, and nerve damage/muscle atrophy as the dependent variables, but I am having trouble determining which statistical model to use as nerve damage is also an independent variable in relation to muscle atrophy. Is there a way to structure sampling such that this isn't an issue? What statistical model should I be using to analyse the results?",What statistical model should I use/how should I structure my sampling in animal disease project?,957i02,new,6,8,8,0
"I am working on a project that wants us to compute a  random forest permutation for importance by hand.  The data is relatively small, with the predicting variable being either A or B and like 8 explanatory variables of random numbers.  I have the error rates and ever information that comes with coding a RF in R, just need to do the importance by hand.

Thanks!",How to calculate a random forest importance by hand?,9568pf,new,1,1,1,0
"I read some papers on the non-seasonal ARIMA model, and the consensus I've seen is that for ARIMA(p, d, q), p and q should not be greater than 3, maybe 5. What's the reasoning for that? Is it for efficiency? I ask because intuitively, I would think that you want p and q to be very high to get more accurate metrics, as they are reading more data to make the next prediction that way.
",Reasoning for non-seasonal ARIMA model lag-order bounds,955g6v,new,1,2,2,0
"I was trying to figure out what counts as a nested model and wanted some help. Specifically, if I have the latent variable, say, ""Intelligence,"" with indicators A,B,C and D to start...is the model with the same latent variable and indicators (average of A and B), C, D nested within the original model or no?

Thanks for the help!",CFA/SEM: Nested vs. Non-nested models,953j8z,new,2,1,1,0
"I have data collected from 8-10 year old kids. They tried to ask the kids how many servings of various foods they had yesterday, things like veggies, bread, cereal, chips, milk, juice, etc.

They can answer between 1 and ""4+"" servings. They told the kids that one serving is about the size of their fist (or the fist of the adults asking the survey, not sure).

Most kids put 0 or 1 for each food, some put ""4+"" in a few categories. 

My feeling is that hell no you can't have an 8 year old kid accurately tell you how many servings they had from the previous day from a list of 18 foods/drinks (divided into 6 breakfast, 6 snack, 6 beverage items). Is there a simple data quality test anyone could recommend for whether this data is ""bad""? Some kids will just put ""4+"" for several items which I doubt is accurate. In this case a single observation can hugely impact the data for measuring healthiness for the experiment vs control groups, for e.g. a kid could say he had 8 donuts/cookies for breakfast which will make his group as a whole a fair amount less healthy.

I'm thinking the data should be converted to binary for whether they had any of the item. This could further be collated into things like whether they had any healthy breakfast item, any unhealthy snack items, etc. 

",Test for whether data is too crappy?,952w0i,new,6,2,2,0
"My textbook is talking about prediction intervals and confidence intervals. I understand the intuition behind it, how we do not include the irreducible error in confidence intervals because we assume the error is normally distributed. However, how do you find this value? Say I did multiple linear regression on my data. To get the confidence interval I would use the Standard Error term right. So for prediction intervals what other error term is used? My textbook did a horrible job of explaining this. Thank you for any help!

EDIT: Is it the MSE our estimate for the irreducible error? (mean squared error)",What term is the reducible error and what is the irreducible error?,952eqj,new,0,1,1,0
"Binomial distribution answers questions like, ""For 60 coin tosses, what's the probability of x heads?"" 

While poisson distribution answers questions like ""If my mean measurements are lambda during a fixed time interval, then what's the probability of next measurement being x in the upcoming interval?""

What questions would be answered in a similar manner by normal distribution? Thanks!",Can someone give me an example of questions answered by normal distribution?,952cbl,new,19,1,1,0
I'm only interested in the interaction between 2 variables within a linear regression model. Is it necessary to include the non-interaction terms when building the model?,"When interested in only the interaction within a regression model, do I need to include the main effects?",951vv2,new,14,6,6,0
"Hi every one, I've applied an two samples independent t -test on my data, but I'm not sure if what I did is statiscally correct.

**Data**

The data consists out of 40,000 customer transactions over a period of 7 months. It contains the variables:

* Product the customer bought (A or B)
* Date (Day and month)
* Unique customer ID
* Money spend

Now I wanted/need to know if there is a difference if product B is bought more in the weekend (Saturday - Sunday) compared to a weekday (Monday - Friday). What I did is as follows:

* Calculated the total number of date days (213 days)
* Caculated the total number of transactions per date and what percentage of those consisted out of product B (called this the chance on product B)
* Created a dummy variable (1 for weekend day, 0 for a normal weekday)

Now I applied a Welch T-test with as indepedent variable the dummy for weekend/weekday and depedent the chance on product B being bought.

Outcome was siginificant at any level of alpha, therefore means are different for weekend. (Weekday 30% chance on average of product B being bougt, in the weekend 36%). I'm hesitating because somehow the raw data consists only out of categorical variables and I turned one into a continuous.",Can I use a two sample independent t-test here?,951pn8,new,2,2,2,0
"What is  the noise in  a predictive model, and how to calculate it?",What is the noise?,951nlp,new,0,0,0,0
I can't understand the difference,What is the difference between variance and deviance?,950z4y,new,12,6,6,0
"I have read:

''If you want to test if there is an association between two nominal variables, you do a [**Chi-square test**](https://www.theanalysisfactor.com/chi-square-test-vs-logistic-regression-is-a-fancier-test-better/).''

What  I don't  understand, is  what is an association between two nominal variables...  I don't  understand  the  word ''association''  , what does it  mean?  Is it  the correlation?  It isn't , because the correlation is measured  with the Pearson index... so  what is  ''Association''  (Could you please  make  some examples?)",Understanding Association and chi-squared test,9509ae,new,10,4,4,0
"Hello, studying the Poisson model  I was  reading about one guy who wrote: ''I  have  put  this  variable as off-set''

I was wondering what  does this mean?  Could you provide some examples? 

Thank you",what is a variable cases as off set?,94zij4,new,4,1,1,0
"I had an argument with a friend recently. There was a scandal in Germany about the ""Bundesamt für Migration und Flüchtlinge"" - BAMF - Bremen department, which is responsible for dealing with migrant and refugee affairs, integration etc.


The scandal was that there had been reports, that a lot of refugees were falsely approved instead of being denied.


Now after officials started investigating into the issue a newspaper claimed that there was no scandal. Those are the information:


1. The newspaper claimed government officials investigated ""a high 3 digit number of files"" thus it would be at least 501 cases of the 4500 reviewed.
2. The newspaper said in these ""501"" files there were only 13 cases prooven to be misjudged, a maximum of 2%.
3. The newspaper further stated that this is a ""small number"" compared to other BAMF governments nationwide.
4. The newspaper stated that according to this information it can be said with near certainty that there is no scandal.


The argument I had with my friend was now that he claimed this is true as the newspaper facts are based on statistic and this sample can give an accurate representation of the whole amount of cases.


I claimed that the officials didn't necessarily conduct a sample probe according to statistical standards and that they were merely going through files one by one. Thus it is not clear if the files were in a certain order or if a department of the BAMF which might be responsible for the scandal was not yet investigated.


Who is correct? Is it really possible with 1/9 of cases being checked in an uncertain manner to assume a reliable result?


Thank you in advance.

This is the article, it is in German though.

https://www.taz.de/Pruefung-der-Bremer-Asyl-Entscheidungen/!5523807/
",Statistics for the german BAMF scandal,94z0v5,new,2,4,4,0
"Sample mean and sample variance come out through maximum likelihood function. (by maximizing log of the likelihood function)

It is also written that :

E[sample mean] = mu (of gaussian)

E[sample variance] = (N-1)/N * (sigma)^2 (of gaussian)

How were these expected values calculated ? I am a bit new to terminology of expected value etc. ",Sample mean and Variance vs mean and variance (Gaussian distribution),94yk8a,new,2,1,1,0
Is the concept of Synchronicity related to Bayesian Statistics? ,Synchronicity and Bayesian Statistics,94wo9x,new,2,1,1,0
"Hi everyone!

I’m working on a program evaluation where I have mostly unmatched pre- and post-test data. Pre-test employees took a survey six years ago, but due to turn over, not all of them took the post-test (and some newbies who never did the pre- took the post). My sample sizes are different (n= 22, n=14), so I know an unpaired Ttest isn’t the way to go, but I want to compare the means to see if the differences are meaningful. 

Any advice would be appreciated! ",Comparing pre and post means for different sized data sets,94wemr,new,0,1,1,0
"I’m taking a position in a paid internship when fall semester rolls around. The dude I’ll be working under is an MD PHD who is focused on developing new methods which in his words are “really f***ing simple.” My experience with stats has been the equivalent of the introductory stats course at a college and some work in research which involves biostats (this was a lot more mechanical).

He recommended that I read and understand the elements of statistical learning (textbook which is available free as a pdf.) I have burned through ISLR 7th ed (introduction to statistical learning) but it was a much more focused reading on certain methods for my work. I would love to understand the math behind everything so I can hopefully keep pace at my new internship. 

Alternatively, if anyone has any resources on learning about the math behind statistical learning techniques please let me know!

Thanks for your time!

Edit: Hot dayum this post garnered a lot of attention. I am going to add a couple of things real quickly, I have two time frames to learn this, the first is around a month, the second is until around Christmas break. Because of this I will be blowing through the book for quite a while until my fall semester starts, then I will revisit each chapter at a slower pace.

I would also like to clarify that I might not do all the labs in the coming few weeks, I am targeting lower level comprehension and the ability to look at a formula and know what it does. Of course, I will be doing some side projects later on.

Finally, a few people have pmd me asking if this is on Slack or Discord. I am honestly fine with either, but I have much more experience and an affinity for Discord, especially since we can set up targeted channels for each chapter and a few more for questions, projects, etc. If y’all can let me know your preference through posting or pms I can go ahead and get started tonight itself.

Side note: Some of y’all are grad students, some are just interested in the topic. I myself am still in undergrad and I really only know just enough so that I can publish my manuscripts in medical journals. That translates to a pretty surface level understanding of stats with maybe a slightly deeper focus on some topics. I’d like this to help everyone though. 

Edit 2: This is going to be on Slack, I am gonna pm all the replies here with an invite link. If you are interested but don't want to post here for some reason, pm me and I will send you the link. If I skip you by accident, pm me and I will send you the links

Edit 3: I just got home and it’s 2 am. I’ll do my best to add everyone tomorrow! If for whatever reason I don’t and you want to be added, go ahead and send me a message again. I’m a bit leery of putting this up as a public thing because then it’s accessible by anyone, which is a scary thought. I’d prefer this admittedly rudimentary layer of protection. But who knows, after I wake up and exercise, I might end up just posting the link.

Edit 4: Back from the gym and I've added everyone who has replied or messaged me. If any of you have significant experience with this already, and would love to help out all of us, please let me know and I'd honestly be happy to have you!",Would anybody like to study Elements of Statistical Learning with me?,94w72n,new,94,87,87,0
"I'm working on large advertising dataset wherein I have missing values for three categorical variables ( 34 missing for two of the variables, and 19 for the third). I ran a correlation matrix by assigning a value of 1 for missing values and 0 for nonmissing values..to see which two variables have missing values together. I got a pairwise correlation of 1 for the two variables with 34 missing values each,  and  pairwise correlations of 0.75 each for the variable (with 19 missing values ) with the other two variables (34 missing). 

What would be the best way to deal with such missing data? 

Edit: I tried treating the missing values as a separate category, but on running a regression, I got the singularity error issue in R with one of the estimated coefficients being an NA..which is expected since the values are missing together",Dealing with correlated missing data,94vtfn,new,4,4,4,0
"I’ve been encouraged by the likes of Charles Duhigg and other productivity researchers to be a Bayesian when it comes to goals and events in life.

So, I’m curious about being to be able to create a model that can be iterative and updated given the occurrence of certain events. 

Example: I have a goal of publishing both non-fiction and fiction works by the age of 35 (am 25 currently). I assign that happening a prior of 30%—how do I add parameters (ideally yes/no indicators) that affect that probability? 

Thanks! ",I want to create a model which takes Bayesian priors/estimates of life goals?,94ug7j,new,0,0,0,0
https://bobbywlindsey.com/data-science/2018/06/27/bias-and-variance/,Understanding Bias and Variance,94ss5k,new,2,24,24,0
"Hey guys!

I'm having trouble even formulating the title of the post, but I think the idea will make sense when I explain and give an example. 

Basically, i want to model a series where the only thing you have to go on is past values of the series, but they are actually independent of each other. 

For example, say you had data of 100 trials where you rolled 2 dice and looked at their sum. 

If you regressed the ""dice sum"" on past dice sums in a VAR style or whatnot, I think you'd have autocorrelated errors across time, because after a 12 you'd systematically overestimate results because your predictors will probably be slightly too high, and after a 2 you'd systematically underestimate future rolls. 

We know the real answer for regular 6-sided dice is that there should be a constant value at 7 and a coefficient of 0 for the moving average, and that generally you should just take the mean of all the rolls you have to date, but imagine we didn't know what the dice's sides were and imagine old data becomes stale for some reason. 

Basically, I think VAR and moving average models tend to imply there is some ""inertia"" to the data or whatnot in that past values are influencing future values. What should you do when that's not the case?

Hopefully this makes some sense although I'm bracing for downvotes!","Modeling time-series data with only past values of the data (Var model etc.), BUT when the values don't depend on themselves.",94rxne,new,6,6,6,0
"Hi, I was reading on Mintab  this article  about curve fitting

[http://blog.minitab.com/blog/adventures-in-statistics-2/curve-fitting-with-linear-and-nonlinear-regression](http://blog.minitab.com/blog/adventures-in-statistics-2/curve-fitting-with-linear-and-nonlinear-regression)

I always  tought that  overfitting  should  be avoided, but as you can see  in this  article they say:

''While the [R-squared](http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit) is high, the fitted line plot shows that the regression line systematically over- *and* under-predicts the data at different points in the curve. This shows that you can’t always trust a high R-squared.''

[https://blog.minitab.com/hubfs/Imported\_Blog\_Media/flp\_quadratic-1.gif?t=1533326085524](https://blog.minitab.com/hubfs/Imported_Blog_Media/flp_quadratic-1.gif?t=1533326085524)

But why does he  want to do better than this?  This  seems  already to be a  good fit...  isn't  already overfitting the data (this line)?

Then  he  makes   a ''Fitting Curves with Reciprocal Terms in Linear Regression'' and successvley  a ''Transforming the Variables with Log Functions in Linear Regression''  and  the curve looks  like that:

[https://blog.minitab.com/hubfs/Imported\_Blog\_Media/flp\_nlin.gif?t=1533326085524](https://blog.minitab.com/hubfs/Imported_Blog_Media/flp_nlin.gif?t=1533326085524)

Isn't  this over-fitting?",Curve Fitting with Linear and Nonlinear Regression,94r9am,new,3,7,7,0
"to understand residuals?

Basically a  residuals it's  the  difference  between the fit line  and  the  observed  value?

[https://preview.ibb.co/jhWKAe/RES.png](https://preview.ibb.co/jhWKAe/RES.png)

I   have drawn  a  graph to  understand.  Is the  residual  that  green  line  (the  ''B'' is observed  valued  and  the  green line  which  connects it to  the fitted line) , which is the  distance beetween the   observed  value and  the fitted line?

Are   errors  and  residuals  the same  thing?   So i am curious  about  the  STANDARD error  measured  after a  regression line... what is  the  difference  between  residuals, errors  and  standard errors? What  does  that  standard error stands for?",Caould you please help me to understand?,94qru7,new,7,11,11,0
"Hi everyone,

I'm looking for help in creating the methodology for a study I would like to conduct about the historic difficulty of individual national-circuit high school debate tournaments.

From what I know, the easiest way to do this would be to find a way to quantitatively rank teams, and see which tourney has the hardest teams.  However, there are problems with this.

Difficulties: All tourneys have round robins and then elimination rounds.  Some tournaments break to Round of 16, others Round of 32, others Round of 64.  In addition, in round-robin rounds, teams are power-paired, meaning a ex. 2-1 team will hit another 2-1 team.  Also, how could I predict tournament difficulty in the future using historical trends?

I'd appreciate any input on general methodology.  I know really baseline stats and will maybe start to self-study actual material if I have time.  Thanks so much!",Help creating study methodology?,94pl9x,new,3,6,6,0
"I’ve been trying to wrap my head around the concept of likelihood, and this is what I’ve come up with. Could anyone let me know if this is correct?

Say you have two 6-sided die. You roll the die 3 times, getting a 7, 6, and 2. So the probability of this happening is 6/36 * 5/36 * 1/36 * 6 = ~0.39%

From what I understand, the likelihood is the probability that you rolled two 6-sided die (as opposed to k, n-sided die) given you rolled a 7, 6, and 2?

Also, could someone explain how to calculate the likelihood and why we use log likelihood to maximize?",Struggling with Likelihood intuition,94pb18,new,11,8,8,0
"Hi, I am working on a problem. I have asked for help at the statistical consulting center on campus but the consultant got stuck and said the problem is outside his expertise. I hope someone here can give me a hand or suggest what to do next! Thank you for your patience in reading this.

In my problem, a person goes through a maze made of 32 T-intersections. At each intersection they can decide to turn either left or right; each time, on one side there will be a wall, while on the other side the maze will continue to the next intersection.
Their goal is to arrive to the end of the maze as soon as possible.

At each intersection, the person is aided by two cues, a visual cue and an auditory cue. **The cues are not always correct and not always in agreement**. Specifically, the cues can have different level of reliability. For example, in one maze the auditory cue may be correct 50% of the times, while the visual cue may be correct 25% of the times.  

The cues may be reliable at three different levels: 25%, 50% and 75%.  The reliability of cue does not change within one maze, but may change from maze to maze. Each new maze is a completely different situation. 

Since we have three different reliability levels, combining all of them we have a total of nine mazes.

*Our main goal is to understand*:

- Whether the person in themaze trusts more the auditory cue or the visual cue, or whether there is any bias toward one of the two modalities.
- How long it takes for the person to learn the statistics of the environment

**I was thinking about modeling the two cues (auditory and visual) as flipping two coins independently**. 

Let's say the auditory cue is 50% reliable; I will represent it with a fair coin. Let's say the visual cue is 75% reliable; I will represent it with an unfair coin that lands on head 75% of the times.
At this point I was inspired by the paper [""Bayesian models of cognition""](https://cocosci.berkeley.edu/tom/papers/bayeschapter.pdf) by Griffiths et al., to use a Beta prior to the Binomial distribution (see formulas 13 and 14 in the article and the discussion preceding them). This way I thought I could calculate my likelihoods separately for the visual and auditory cues, and then compare them at each turn. So I was planning to use three parameters:

- 𝜃1, the probability that the first cue is correct 
- 𝜃2, the probability that the second cue is correct
- *d*,  the number of successes (the observed outcome)

And have these two likelihoods:

P(d / 𝜃1) = 𝜃1^(d) (1 - 𝜃1^(d) )^(d)

P(d / 𝜃2) = 𝜃2^(d) (1 - 𝜃2^(d) )^(d)

By then I talked to the statistical consultant and he told me that while a uniform prior is fine, the problem is very complicated and that I should use four additional parameters:

- *d*^(audio), the probability that the person would follow the auditory cue when considering it alone (this parameter is unobserved)

- *d*^(visual), the probability that the person would follow the visual cue when considering it alone (this parameter is unobserved)
 - 𝜂, combination of *d*^(audio) and *d*^(visual), leading to *d*, the decision I observe.

Then I will have:
 H*_0_* : 𝜂 = .5 (no bias)
 H*_A_* : 𝜂 ≠ .5 (bias)

Then he added I should actually consider two 𝜂s:

𝜂*_agree_* in case the cues agree
𝜂*_disagree_* in case the cues disagree

You can see a summary [here](https://drive.google.com/file/d/1PGl94cdBC191KF0s7IAiSd7mu0mT-3Dr/view?usp=sharing).
Now I am not sure about how to build the likelihood for this model! Any ideas?
",How to calculate likelihood when flipping two coins independently?,94mxa8,new,2,14,14,0
"Thanks for clicking! I recently submitted a manuscript to a journal. One of the reviewers recommended to do a Bonferroni correction. From what I understand, I just calculate each of my p-values by the number of variables which I looked at? The data is below:

    Complications    p-value    Bonferroni Correction
    Transfusions    <0.001         0.01
    Pneumonia      <0.001         0.01
    Delirium          <0.001         0.01
    UTI                 <0.001         0.01
    APA                 <0.001        0.01
    PE                   0.007           0.07
    DVT                 0.039           0.40
    MI                   0.703           7.03
    CVA                 0.020           0.20
    Ileus                0.003           0.03


So with this correction, I basically end up losing a lot of the variables which were previously statistically significant. ","Bonferroni Correction help, please?",94k0ls,new,46,9,9,0
"[This article](https://www.ncbi.nlm.nih.gov/pubmed/22569939) compares baseline characteristics with either linear or logistic regression.

[This article](https://www.ncbi.nlm.nih.gov/pubmed/?term=Risk+factors+for+atherosclerotic+and+medial+arterial+calcification+of+the+intracranial+internal+carotid+artery) compares baseline characteristics with chi-square tests, T-tests, or Mann-Whitney tests.

Why the difference? What is the decision based upon?","Comparing baseline characteristics with a logistic regression versus chi-square/T-tests, what is the difference?",94jutk,new,2,9,9,0
"I have three groups of patients, based upon the location of calcification in their arteries: (1) those with calcifications in the intimal part of their arterial walls,  (2) those with calcifications in the medial part of their arterial walls, and (3) those without calcifications in their arterial walls.

Then I have measured some (non-standard) cardiovascular risk factors for all patients (e.g. IMT, ABI, PSV).

**I want to compare the pattern of risk factors for intimal calcification (1), with the pattern of risk factors for medial calcification (2), to see what differences there are in risk factors between the groups.**

What I have done so far is...

\- Compare differences between groups 1 and 2 for each risk factor seperately (with t-tests/mann-whitney tests depending on the variable)

What I don't know...

\- How do I compare the different patterns of risk factors for both groups?

\- Can I use a logistic regression with  the risk factors as predictors and the location of calcification (intimal/medial) as outcome? So for example for the risk factor smoking (yes/no): 'the odds of you having medial calcification are 1.24 \[1.11-1.32\] if you smoke compared to not smoking'

\- If that is actually possible, Do I have to use other tests? (e.g. binomial regression?) It seems that I have not actually compared the groups.",Comparing the pattern of risk factors associated with a certain disease (ie arterial calcification) for different groups? How do I analyze this?,94jn2u,new,0,3,3,0
"Hi!

So the title pretty much explains it all. I am writing my master's thesis and want to report my findings. I already have tables. Do I have to cite them, like report the values of the parameters or can I just refer to the table and give the conclusion? I guess the latter makes more sense? I just don't want to come off as lazy, that's why I ask.",What role do tables play in a report? Do I have to quote their figures or just give the conclusion?,94j7bm,new,12,8,8,0
"Hi, I was looking  at  those plotted   residuals vs Fitted

[https://i.stack.imgur.com/cBhJN.png](https://i.stack.imgur.com/cBhJN.png)

I can't  understand how to read them.  For  example: the  first plot, residuals  vs  Fitted...  there  are  a lot  of  point  and  a  red line. What  are the fitted  and  what are  the residuals ?  What does  it tells  me  the  first  graph on the  left?   What's  that red line  and  those  points?  Residual  vs  fitted  how  can be useful for me  after a regression?

There is  also Scale  location (3 graph),   reisduals  VS  leaverage (what  is this) ?  And Normal Q-Q (what is?)

The image comes from this discussion:

[https://stats.stackexchange.com/questions/160941/accounting-for-overdispersion-in-binomial-glm-using-proportions-without-quasibi](https://stats.stackexchange.com/questions/160941/accounting-for-overdispersion-in-binomial-glm-using-proportions-without-quasibi)",Residual vs Fitted,94iwlb,new,2,0,0,0
"Hello everyone,

For an assignment I need to perform tests on .sta files. But as I work on my personal computer I use R and .sta can only be read by Statistica. Could someone with a Statistica licence convert those files (6) to a format usable in R ?

Thanks in advance

\[Edit\] Managed to convert the files with the test licence of StatTransfer, not perfect (skips one in every 16 rows) but gets me going.",[REQUEST] Need to convert .sta files to .csv or equivalent,94it05,new,4,1,1,0
"Suppose we have 4 groups of sample data with equal sizes and we want compare mean of group 1 with group 2 and group 3 with group 4. So we perform independent sample t-test on the pair of groups and their results are as follows

group 1, group 2

    t_score = -5000, p_value = 0
    mean(group 1) < mean(group 2)

group 3, group 4

    t_score = -100, p_value = 0
    mean(group 3) < mean(group 4)

What conclusion we can draw from this data. I am new to statistics, if the question seems very general then please forgive me.",Understanding students t-test,94i9ya,new,8,3,3,0
"Hi,

I've been having difficulties finding resources which explains the Ljung-Box test. Perhaps anyone in this subreddit know of any lectures/online-resources/videos which explains it?",Ljung-Box test,94i30p,new,3,2,2,0
"In my opinion no.  Because  of this:

*The* ***random walk theory*** *suggests  that stock price changes have the same distribution and are independent  of each other, so the past movement or trend of a stock price or market  cannot be used to predict its future movement. In short, this is the  idea that stocks take a* ***random*** *and unpredictable path*

And  this  is  totally not true. The movement of  stock is  NOT   an indipendent variable the same   as  the throwing of a coin/ dice,  where the probabilities  are  equally distribuited (always 1/2  for  every throwing),  because the precedent move of a stock  it influences the future  moves of  the  stock.  Let's  consider the market sentiment...  after  the stock has gone   down,  some market partecipants  may be more willing to buy the stock at the dip, causing then  the price to go  up. As  you can see  the movents  ARE NOT independent  each other the same way  as the coin / dice throwning.

I am  wondering  why in 2018 there  are still people  who ''believe''  in this  outdated method which should  be  avoided.. and  which  basically  gives  only  an ''avearge'' model  of  what the stock could do, the  same  as  the  average  mean  can do...  I can't  see no difference  between a  simple moving average and  a montecarlo simulation..  basically it  could  do  a probabilistic outcome, based on  FALSE parameters (price movement of stock = independent  variable).

What's  the point  of considerating this as  an effective method, and   how to reinforce this model  including subjective parameters, such as the fact that a stock that falls 20% could positively   push the price up in the next session?

**Let's  suppose that  I would like to put this  parameter into the montecarlo simulation (Paramter= ''after  a  dip the stock is more likley  to go up in the next session'')**  

**how  could I do?**",Is random walk at least 50% effective to predict the stock market?,94glj2,new,23,0,0,0
"Hey everyone, I’ve recently have become involved in following Sabremetrics and realized I need to up my statistics game. I was wondering if anyone could reference some helpful resources that would help give me a good primer. My degree is actually in music education, but numbers still fascinate me! ( I don’t rely entirely on my right brain hemisphere.) 

Thanks for the help!",How to Get Started?,94gk91,new,16,0,0,0
"Why  during  a  montecarlo simulation generally we take as  result of  the predction only the 25%  quantil and  the 75% quantile?

This  is  a montecarlo simulation (30 days)  based on the  price of a coin.

[https://numex-blog.com/crypto-price-simulation-using-monte-carlo-python/](https://numex-blog.com/crypto-price-simulation-using-monte-carlo-python/)

As   you can see  at the end, he only  takes  as  ''valid''  the  25%  quantile  and  the  75% one...  why ?  Why  does he  not  take the ''mean''  of the final  gaussian (the result of the Montecarlo simulation)  as  predict result of the Montecarlo?",Random walk / Montecarlo simulation,94fyp2,new,1,6,6,0
"In the game Rust a recent update added a spinning wheel minigame of sorts. Seen [here](https://files.facepunch.com/s/64622b83cdfb.jpg).

Basic mechanics of it are: you bet any amount of money on a number, if it lands on that number you get your original bet + that number times your original bet.  For example, betting 10 on 20 would yield 0 if it landed on anything other than 20, but if it did land on 20, you would get 210 back,

Based on my own napkin math, betting on 3, 5, 10, and 20 all have the same return on investment, for this I just multiplied probability of each item: 1 or 4 or 6 out of 24 times its multiplier 3, 5, 10, or 20, giving .834 return each time.  Assuming my math is correct, my question is if I'm going to bet on just one color (3x, 5x, 10x, or 20x) what should I bet after each loss to maintain profit assuming infinite currency.",Help Determining the Best Gambling Option,94fo8i,new,6,0,0,0
"Hi everyone. So my textbook states that a Prediction Interval will always be wider than a confidence interval because it includes both reducible error and irreducible error.

Alright so let's say I used multiple linear regression on my data and I have my model. So I compare my model estimates to the actual data points. I get all the residuals and square them, divide by N, sqrt that and I get standard error. The standard error is 5 so this means my model was off by 5 on average correct? So is this the reducible error or irreducible error and how do I find the other missing error?",Reducible Error vs Irreducible Error (Prediction Interval),94ewps,new,2,1,1,0
How do you find the variance of the coefficients estimated in your model? It is surprisingly difficult to find the answer to this through google. Is there an equation?,Variance of coefficients in multiple linear regression,94e8v4,new,7,4,4,0
"Hey all. I'm a recent grad trying to learn about Bayesian Statistics. (I took stats courses throughout my education, but it was pretty much exclusively Frequentist stats). I was wondering if anyone here knew where I could get my hands on some free/cheap practice problems that someone new to this could work with using R. ",Beginners Practice Problems for Bayesian Statistics,94clyo,new,17,67,67,0
"i have 12 independent events occuring, and need to figure the probabilities of X number of events. This is for a sports schedule, so basically after 5 games the probability of being 0-5, 1-4, 2-3, 3-2, etc.

What would be the quickest way to calculate this with some type of loop to figure this out without manually doing all 4095 possibilities by the time I get to week 12.",I need to find a loop to find the probability exactly X independent events occur,94c9ym,new,4,1,1,0
"Hi. Say I have my model and I plot the residuals and I think there might be a correlation in the error terms. How do I test to see how strong this correlation might be? I know there is the correlation term r which tests how strong a linear relationship is between two variables, so I could do this for the error term and my estimates, but I am not sure if this is the thing I want? Because what I want is kind of like a test of a variable with itself right? In my head what I think I want is testing if an error term will likely have some info on error terms close to it, so a correlation test isn't right for this situation to me. Any help is appreciated, thanks!

Edit: So after thinking it over some makeshift bullshit way I came up with is if my number of observations is huge and the data is time-series, I could plot my error terms and find the average. I could then take samples of error terms that are near the average, and find the distance between the point to the right of it and to the left of it. I do a ton of samples, and if the average is very small compared to the standard deviation, then I can confidently say the residuals are likely correlated. This seems way longer than it should, not to mention it would need a ton of observations. Is there some type of equation that tests this?",How do you test for correlation of error terms?,94b7jd,new,6,3,3,0
"I am doing a study for my bachelor thesis. Two groups have read two slightly different vignettes but responded to identical survey questions. Some of these questions have a binary yes/no-format, whereas others are ordinal variables. But I do not know how to analyze the yes/no-questions. If I want to know whether there is a significant difference between how the two groups answer these questions, what analysis method should I use?

","SPSS: What data analysis method should I use when I want to compare how two different groups answer to ""yes/no""-questions?",94av09,new,3,3,3,0
"I have a problem where I can easily compute the expected Fisher information matrix. An image reconstruction problem, where I estimate thousands of ""parameters"" (each one is a pixel) 

But I can't invert the FIM. So, what can I say from looking at its structure?
Suppose My FIM is circulant. Suppose that the diagonal terms do not change,but the off diagonal terms become smaller. Can I say anything regarding the expected variance? ",What can I say from looking at the structure of the Fisher information matrix?,94ake3,new,4,3,3,0
"Hi!

I'm uncertain which statistical test i should conduct to analyze my data. I have three treatment groups (40+ observations each). Within these treatment groups I have 9 categories each which count the amount of subjects having a specific utility curvature: e.g. concave for gains/concave for losses; convave for gains/convex for losses; concave for gains/linear for losses etc. I have a screenshot of a table summarizing all the categories and treatment groups:

[https://1drv.ms/u/s!AkpF\_YC4a6BrjGtnuvS44hTiwhiL](https://1drv.ms/u/s!AkpF_YC4a6BrjGtnuvS44hTiwhiL)

I really appreciate your help! Thank you very much.",HELP! Which statistical test should I use?,94aade,new,2,1,1,0
"The situation:

A hospital is looking to reduce average length of stay for patients admitted to their hospital.  Shorter length of stay -> reduced stress for patients, more efficient care, better outcomes, and ability to serve more patients.

My question:

The hospital would like to study how this change in average length of stay (say, from 5 to 4 days, assuming the variance would stay the same) would affect both their admission rate (how many unique admissions in a given period of time - less important) and their average number of patients in the hospital at any given day (more important).

If there's a way to study how the distribution of each characteristic (admission rate and length of stay) affects capacity, or just general readings on how to combine distributions in the way I'm describing, that would be helpful as well.

Thanks for any help you can provide.",How to develop and study the properties of a distribution related to hospital capacity and length of stay of a patient.,949q8c,new,2,5,5,0
"Hey there,

I calculate mobility matrices via logistic regression. I then receive probabilities of moving up or down in social classes (measured by education). From these raw probabilities I calculate a weighted probability of moving up/down:

   1      2     3

1 .5   .25 .25

2 .4  .3    .3

3 .2  .2    .6

For instance, here the upward prob for class 1 is 0.5, for 2 is .3 and for 3 is 0 (similarly for down). I aggregate these probabilities by multiplying with the population share in this class (assume 50% in 1, and 25% in others): 0.5* 0.5 + 0.3* 0.25 + 0* 0.25 = 0.325). I then get upward, horizontal and downward probabilities for every matrix. As the probabilities are estimated, I have standard errors. How can I calculated the se for the final probabilities? Doing sth similar than the risk of a portfolio, e.g. adding the cov?

Best
 
Edit: table is a mess - solved partially. Hope it is readable. Wrote on phone",Standard error of sum of estimated probabilities,949nmp,new,1,2,2,0
"Hello I am  reading  about the Poisson distribution,  but I don't  understand  some things:

*''A* Poisson distributio*n  is a tool that helps to predict the probability of certain events from  happening when you know how often the event has occurred.* *The Poisson distribution gives us the probability of a given number of events happening in a fixed interval of time''*

*''****when you know  HOW OFTEN****''*   the  event has  occured''

Is this  frequency?

***''The*** **Poisson distributio*****n gives us the probability of a given number of events happening in a fixed interval of tim*****e''**

In  a fixed  interval of time?  But  isn't  this almost  the  same  of  the density probability distribution? Or binomial distribution?

Also:  aren't there  any other  type of distribution which  cover that parameter (probability of a given number of events happening in a fixed interval of time)?  Only  Poisson  makes  this?

WHY?  Isn't  the normal distribution the same?  Example:  I analize the     height   of men  in USA in the  2015 year  and

I found  that  it  follows  a  normal  distribution.  Well isn't  this  the  same  to say: ''In a  fixed interval of time (2015 year)  the male  height  follows  this  normal  distribution (gaussian)''

Sorry  for my ignorance, I am  starting out,  and  it'  difficult.",Poisson Distribution,949mhf,new,2,2,2,0
"Reading the  R tut on linear regression I saw  this: 

(After  we have runned the regression model and  we  have found p-value, intercept, slope, F-stat, T-test and  other tings   with  the ''sumnmarize''  function)

# 

# ''Predicting Linear Models

# So far we have seen how to build a linear regression model using the  whole dataset. If we build it that way, there is no way to tell how the  model will perform with new data. So the preferred practice is to split  your dataset into a 80:20 sample (training:test), then, build the model  on the 80% sample and then use the model thus built to predict the  dependent variable on test data.

# Doing it this way, we will have the model predicted values for the  20% data (test) as well as the actuals (from the original dataset). By  calculating accuracy measures (like min_max accuracy) and error rates  (MAPE or MSE), we can find out the prediction accuracy of the model.  Now, lets see how to actually do this..''

Why  can't I simple  solve the function  *f(Y) = ax+By*  to    see  my predicted values?  and  to find the new  values  for  the  *Y*, but I have to  split  the dataset into 80-20% ?  What is  the meaning of  *training* the 80%  of the dataset?

Why   do I need  to ''train''  80%  of the dataset?  And  what's  the  meaning of ''training'' them?  From a pratical point  of wiev, what  I have to do in order  to ''train'' 80% of the dataset?",Splitting the dataset into 80:20 sample,947svv,new,5,5,5,0
"Economical models seem dominated with **Gaussian distribution, which has very thin exp(-x\^2) tails**. I have recently worked on ROI of Dow Jones daily averages sequence, and its empirical distribution fits well CDF of  [**Laplace distribution**](https://en.wikipedia.org/wiki/Laplace_distribution) **instead - with heavier exp(-|x|) tails,** what means larger risks. Plot and sources:

[https://economics.stackexchange.com/questions/23967/is-gaussian-distribution-the-proper-choice-for-step-in-financial-time-series-e](https://economics.stackexchange.com/questions/23967/is-gaussian-distribution-the-proper-choice-for-step-in-financial-time-series-e)

As our financial institutions operate on these models, for stability of economy it is crucial that they don't underestimate the risks - is this disagreement discussed in literature, handled in real models?",Is Gaussian distribution the proper assumption for step in financial time series - doesn't it underestimate the risks? For example Dow Jones sequence suggests Laplace distribution instead: with heavier tails and so larger risks,947sdw,new,23,17,17,0
"Hello I am  trying to eliminate the  outliers  from my model  cause  I want it  to be the more  accurate as possible.  I am using Blox-pot in R statistical software.  I don't understand ''how  to read''  what it's  showing  me.

I have  an  example

## Example Problem

For this analysis, we will use the *cars* dataset that comes with R by default. carsis a standard built-in dataset, that makes it convenient to demonstrate  linear regression in a simple and easy to understand fashion. You can  access this dataset simply by typing in carsin your R console. You will find that it consists of 50 observations(rows) and 2 variables (columns) – distand speed. Lets print out the first six observations here..

head(cars)  # display the first 6 observations #>

**speed** \-- **dist**

# >      4    --   2

# >      4  --  10

# >      7 --  4

# >      7 --  22

# >     8  --   16

# >     9 --    10

Before we begin building the regression model, it is a good practice to  analyze and understand the variables. The graphical analysis and  correlation study below will help with this.

## BoxPlot – Check for outliers

Generally, any datapoint that lies outside the 1.5 \* interquartile-range (1.5 \* *IQR*)  is considered an outlier, where, IQR is calculated as the distance  between the 25th percentile and 75th percentile values for that  variable.

code:

par(mfrow=c(1, 2))  # divide graph area in 2 columns boxplot(cars$speed, main=""Speed"", sub=paste(""Outlier rows: "", boxplot.stats(cars$speed)$out))  # box plot for 'speed' boxplot(cars$dist, main=""Distance"", sub=paste(""Outlier rows: "", boxplot.stats(cars$dist)$out))  # box plot for 'distance'

**Graphical blox pot:**

[http://r-statistics.co/screenshots/boxplot.png](http://r-statistics.co/screenshots/boxplot.png)

Ok, as you can see  there  are  two regtangular shaped items...   how  can I get  WHERE  AND WHAT  are the outliers, from  that pic?

Thank you",Identifying the outliers with R with Blox-pot,94748m,new,4,2,2,0
"I was  studying the R commands line, and  in the  guide they  say:  ''Before making a best fit/good linear regression  is better than your data followed a normal distribution''

I didn't know this.. so I was wondering:  what if the data  they have another kind of ditribution?  For example if the observd data  follow a  Poisson distribution then --> make a  Poisson linear regression ... is this  the way to go?  Always checking which distribution do the data  follow, and then  deciding the  appropriate regression model?",Normal distribution and linear regression,946jg8,new,4,2,2,0
"I am trying to determine the test to use to analyze this data. I am comparing success rates between two procedures to see if introduction of a certain device has improved the success rate. So this is basically our data:

*d* successes

*e* fails

*f* successes with new device

*g* fails with new device

I was thinking a chi square test but someone else thoughts a bivariate t-test would be more appropriate. I've been trying to figure out which is the more appropriate test, but I have had a lot of trouble. Thanks!",When to use bivariate t-test vs chi-square test,943q68,new,1,5,5,0
"I need help in determining what method to show correlation/causation between environmental variables and biodiversity. The null hypotheses is that facility operations that affect discharge rate, ph, do and ect does not significantly affect macro invertebrates  (measured via diversity richness, TAXA,ect) 

Here is what the data set looks like. I have 7 years of hourly sensor data including variables such as dissolved oxygen, discharge rate, pH, ect sampled right next to the facility and far downstream from the facility. Then I have 7 data points sampled yearly that measure biodiversity via 4 methods. I already checked for normality and some of the environmental variables are heavily skewed. Also I'm assuming that the environmental variables has seasonality to it.

The issue I'm having is the large difference in temporal resolution between the environmental variables and the biodiversity measurements.What I would do if I had more data points for biodiversity would be check for correlations between each environmental variable to each biodiversity measurement and then multiple correlations for the environmental variables. My first thought was to just average the hourly environmental variables to yearly. But I'm afraid that it will remove any significant event like a short but high pH spike that would drastically kill off the macro invertebrates.

I'm hoping someone here has a statical method to account for this large temporal difference. Ideally, biodiversity would be measured more frequently but alas that's not the case.",Help providing a statical method for time-series data with large temporal resolution difference.,943jen,new,4,3,3,0
"Using my grammar ""Nazi"" alt to ask this question. If I spent fewer nights partying in college,  I would have less trouble with this problem. 

Recently,  I was playing an online game in which the chance of success on one specific event was a flat 18%.

I performed this specific event 26 times and was only successful 1 time. 

Put another way,  in 26 tries at this event, the 18% chance of success only occurred once. 

Frustrated with this outcome, and suspecting some sort of temporary code glitch (from a recent update to the game) was at fault, I decided to use the in-game support option to ask if my results were in some way caused by a ""bug"".

I contacted the developers of the game and they said that my 1 in 26 success ratio was ""perfectly normal"" and that nothing could have been wrong with their programming. 

By my liberal arts education calculations,  the chances of having only 1 success out of 26 tries, when the chance of success on each try is 18%,  is something like .7%  But, to my word addled brain,  .7% seems like anything but ""normal"".

What I have no idea how to calculate is just how far from ""normal"" my results were. 

My attempts at Google-fu found plenty of probability and statistical calculators, but none that addressed my particular case. So here I am asking for help. 

What I'd really like to know is what my deviation from the norm was. If it was significant enough, I'd love to call b.s. on the game company saying my results were ""normal"".

In return for your assistance, I will be happy to provide help on the ever critical issue of the proper usage of "" fewer"" vs. ""less"".

Thank you, 

Mein_Fewer","Grammar ""Nazi"" needs help with statistics/probability question.",943fzc,new,17,0,0,0
"Suppose I have three mean centred continuous predictors (A, B and C). I run a multiple regression with all three predicting X. Am I correct that the coefficients for A reflects the effect of this variable when ""holding B and C constant""? What exactly does it mean to ""hold them constant""? How is ""holding them constant"" related to the fact that coefficients will reflect only the unique variance explained by each predictor?

Now if I run a multiple regression with all three as well as an AxB interaction predicting X, am I correct that the coefficient for A will reflect the effect of that variable when B is 0?

Edit: What triggered my feeling that I don't understand this is: I know that when there is an interaction, then component effects (e.g., A and B, when there is an AxB interaction) are only the effect of that variable at level 0 of the other variable. Is this not the same as ""holding that variable constant"" at 0? Why is this different than the interpretation when only A and B are in the model?",Question about interpreting results of multiple regression.,943csc,new,11,3,3,0
"Let's take **central tendency** (aka center) as an example. There is no rigorous definition as to how one measures it, but it is roughly defined as a ""commonly seen"" value in a distribution, or a ""expected value"" of a distribution. 

There are various *measures* of **central tendency**, eg. the sample mean, the sample median, the sample harmonic mean, etc. Each of these is rigorously defined. 

These *measures* are commonly called *statistics*. If sample mean is a *statistic*, then *central tendency* is a ___?","What's the word for a ""qualitative statistical property""? eg. central tendency, dispersion, skewness, etc.",942iw1,new,3,4,4,0
"Right now I’m at a cross roads with my education. I spent 2 years at a school studying Business Information Systems. My original “plan” was to try to find a business analyst position after I graduated. Although it felt like every other week is was changing my “plan”

Well things change and now I’m taking a year off to really nail down what it is that I want to do with my life. I’m thinking about changing to statistics as I enjoy working with data and numbers. But to be sure I want to try to read some sort of “introduction to statistics” book. I took stats in high school and it was one of my favorite courses but that was 3 years ago and it was very much just an introductory course. I’d also be interested in book that includes statistics w/ R or python. My previous major included some python courses.

TLDR: Recommendation for a Introduction to Statistics book. 

",Books to read to see if Statistics is the field that I would like to study?,941pq4,new,20,36,36,0
"Hi. Reading on multiple linear regression in Introduction to Statistical Learning, it describes interpreting the coefficient on each predictor as the average increase on the response due to a one unit increase in that predictor, holding all other predictors fixed. Thinking about this to myself, I realize this is probably fine for many situations. But in some situations wouldn't the current values of the other predictors possibly affect how ""strong"" a one unit increase in a specific predictor be on the response. Is there a name for this time of analysis and finding the relationship between how ranges of the predictors affect the weight of other predictors?",Multiple Linear Regression Question,940fob,new,10,1,1,0
"Hi everyone, I'm having some troubles with r studio right now. I divided my data set into a test and training set. Test set consists out of 1600 rows, the training consists out of 6400 rows.

Now I made a multi-linear regression model on the trainingset:

    multi.lin = lm(train$y ~ train$x1 + train$x2 + train$x3)

Now I want to test this model on the test set and test how it performs, I already tried to use the predict function but I get this message

>'newdata' had 1600 rows but variables found have 6400 rows 

How do I do this. I see quite some examples on the internet, but there all the test and training sets are equally sized.",R studio: Test regression model on test data,93zdzb,new,2,1,1,0
"So at this point in time I'm really interested in pursuing a masters or PHD.

I'd like to be in a position where I can apply my analytical skills to something meaningful, but I'm still very interested in developing new methods.

At this point I'm not really interested in deep learning because a lot of the research seems pretty hacky, I'd rather develop methods with a rigorous theoretical background. 

That is pushing me to look into statistics/applied math. My problem is I'm coming from a machine learning/ computational neuroscience background, where my background in math is lackluster.

For most stats graduate programs they expect a high GRE Math subject test, however I don't have the background in advanced topics like abstract algebra and real analysis.

Roughly my question is should I pursue stats/applied math, (if not what would you suggest instead) and if so how should I go about learning the math I need to know.",Grad school advice wanted,93z3s2,new,3,1,1,0
"You could say I'm looking for validation here as I kind of have an answer, but nonetheless I still want to hear what you guys think, especially from professionals who have much more experience than myself.

Spoiler alert: I'm leaning towards the Economics second major. But let me explain my reasons.

To set some background information, both the second major and year-long internship programmes are offered by the school. Meaning I would still pay the same school fees. The extra fees I would be paying for the internship would be the cost of living + airfare + insurance etc etc.

The reason why I'm leaning towards econs is because of my future ambitions. I'm interested in a couple areas, such as fintech, and also things like finance, studying of economies, econometrics. I envision working in a data science role, either in a tech company or finance company. Think Product Manager, Data Analyst, Data Scientist and the like.

On the other hand, the internship is more targeted towards entrepreneurship. The only thing going for it, I guess, is that it is quite popular and often oversubscribed (I heard acceptance rate is 20 - 25%). The locations students can choose from are also quite enticing, such as Silicon Valley / Beijing / New York etc. Once you are accepted, you will self-source for a internship with a tech start-up and settle everything yourself from there. You can also take classes (with a lower academic workload of course) with partner universities during the 1 year there.

My reasons for leaning towards the double major is because I'm not too interested in entrepreneurship, and I also believe that the path of this particular double major combination aligns more with my goals. I envision working in the fields I mentioned earlier, and I feel like even though there is a lot of discussion about how paper certificates are losing their appeal and students should opt for internships, this double major certification would open up more doors for me than a 1 year overseas internship.

I don't know if this matters, but I'm also considering going for a Statistics Master's after gaining some job experience.

What do you guys think? Are my opinions validated? Or am I completely missing the point here?",Year long overseas internship programme vs Economics second major,93yx34,new,0,2,2,0
"I'm testing a binary classifier which returns a softmax score. 
I produced a ROC curve and decided to use the Youden's J-statistic to choose an optimal threshold.
Now I want to calculate a measure of confidence. What is the right/best way to do this?

Thanks!",Measure of confidence for binary classifier,93y7y6,new,1,2,2,0
"Hello, could you tell me more complex  examples  of  two variables  which  could  have  a  joint pr. distribution? ",real life example of a joint probability distribution?,93xm7e,new,15,0,0,0
"If you're totally unfamiliar with baseball you might just be better looking at the tl;dr. Any help is appreciated.

I'm working on a baseball research project to predict the outcomes between individual batter and pitcher matchups. My plan is to first assign a prior for the mean outcome of each matchup, and then subsequently update this prior based upon the performance of the hitter and the pitcher throughout a few seasons of data. However, I want to be able to both weight the contributions of the pitcher and hitter differently (it's possible that either hitters or pitchers control more of the variance in general) and I also want to give more weight to the events where a hitter is hitting against a more similar pitcher (I have a continuous measure of similarity). How might I go about doing this?

tl;dr

I have a bayesian prior for the mean of a process and a set of observations. I also have reason to believe that some observations give me more information about the true value of the parameter than others. How would I go about determining the weighting the contribution of different observations to updating a bayesian prior?",Weighting Observations for Bayesian Updating,93w6ua,new,1,1,1,0
,Strategy for self-reading senior-level undergrad/1st year grad math/stat textbooks,93vy7a,new,1,1,1,0
"On my phone so the appearance of this post might suffer a bit.  Building a survival analysis with time-varying covariates (TVCs) - apparently also known as an extended Cox model.

I'm measuring survival times of recidivists of crime X. TVCs are including the cumulative number of crime A, crime B and crime C.

I was wondering if I could include more features to the TVCs like if they implied firearms (or not) or the number of people involved for each TDCs. It could also be a cumulative like the number of occasion firearms are used so far.

I read most of Terry Therneau's docs (working with the survival library in R) about it but it doesn't address, to my knowledge, this issue. 

Does it even make sense? Thanks.",Time-varying covariates question,93vqvr,new,0,2,2,0
"I'm about to finish up the beginner stuff and I want to move on towards more advanced topics now. I personally have found success with Udemy, but I'm open to any suggestions. 

As an anecdote, I might do Jose Portilla's (Udemy) various bootcamps, because I've heard good things about them.

Thank you.","What are some good online programming classes/resources on ML, Data Science, Statistics, etc... to take (Python, R, SQL)?",93vjfw,new,20,44,44,0
"A biologist wishes to know whether the number of left vs right handed people in two-child families follows the expected binomial distribution. They randomly sample 200 families composed of just 2 children each and the numbers follow:                  
both childern left handed        =  126 families

one left and one right-handed =    68 families

both children left-handed        =      6 families
",Looking for help on this question,93uqh0,new,3,0,0,0
"Hi everyone!

I'm doing a PhD in Clinical Psychology and I have some treatment data to analyse. The design is a  2 (condition: self-compassion, cognitive restructuring) x 5 (time: baseline, mid-treatment, post-treatment, 1-week follow-up, and 5-week follow-up) design. I had 119 participants randomized and engage in their respective interventions for a 2-week period, with follow-up assessments. The aim was to reduce social anxiety. 

One analysis I'm trying to do is mediation, and preferably I would use a more simple strategy such as Hayes' PROCESS macro on SPSS. However, I won't be able to use all five waves of my data if I use PROCESS. Does anyone know if that is bad practice? Should I be using all information? And if so, how?  
   
  ",Anyone have experience in longitudinal mediation?,93u96d,new,5,1,1,0
"I'm currently looking at tract-level U.S. Census data in the American Community Survey.  The Census Bureau reports counts, aggregates (things like ""the total number of cars in a tract""), and medians by tract.

I'm currently looking at distributions of medians of a few variables and comparing them to proprietary external data.  I am not looking to generalize summary statistics to individual people (avoiding the [ecological fallacy](https://en.wikipedia.org/wiki/Ecological_fallacy\)).

Are there any non-obvious pitfalls to avoid when summarizing summary statistics, or looking at distributions for them?",Any not-so-obvious pitfalls of working with distributions of aggregated numbers?,93u0om,new,1,1,1,0
"I will soon be starting a 12 months full time MS Biostatistics program. The program roughly follows 3 semesters: two which include a full time courseload and research rotation (5 hours a week) and one which includes a full time internship or research component and paper. While this format is similar to many programs in the UK, it is not the standard in the US, in which many will have two semesters of coursework, summer internship, and two additional semesters for a thesis/internship and electives. 

Does anyone have experience with one-year programs, whether in the US or the UK? How can I make the most of such a program? Would it be advantageous to intern/work during the program part time and finish the degree in 24 months instead of 12? Should the third “semester” be dedicated to an internship or to independent research? I will be coming into the program with a few intern/research experiences under my belt however they are somewhat unrelated to Biostatistics.",How to make the most out of a one-year (12 months long) MS program?,93tsdj,new,3,10,10,0
"I am looking for advice about what type of statistical analysis I need to do for my data. 

So I sent out a survey to 43 people with Yes/No/Maybe questions. The 43 survey takers were divided up by region, so 8 from the NE, 7 from the SE, 12 from the Midwest, 8 for the NW, and 8 from the SW. I'm trying to see if there is any significance between regions in the response of the survey.

There are 9 questions, but here is one of the tables I have developed for one question. I calculated the following percents.

||Yes|No|Maybe|
|:-|:-|:-|:-|
|NE|87.5|12.5|0|
|SE|85.7|14.3|0|
|MW|58.3|25|16.7|
|NW|50|25|25|
|SW|87.5|12.5|0|

So what test do I run between each of these? I was told to use XLSTAT with a Fischer exact test, but I can't understand the outcomes. The p-value is given as <0.0001 with a lot of the columns showing significance, but I'm not sure if that's correct since I feel like it might be comparing NE Yes to NE No, even though they are tied to each other.

I recognize I may not get any significant data, but I still have to run it.

Thanks!  ",What's the best test for my data?,93stwl,new,4,4,4,0
"Hello! I'm currently working on a lot of multilevel model analyses (aka hierarchical, linear mixed models, generalized linear mixed models). My dependent variables are dimensional, categorical, binary, etc. so I'm looking for a good desk resource book to buy that has examples in R.

If you have any suggestions, let me know. It's difficult to search for because of all the names for the analyses and stats programs.

Also I'd prefer the book be more advanced, something like Andy Field's R book is a little too basic as it only includes one chapter on MLM. ",Books for MLM in R,93rn86,new,13,11,11,0
"Hey everyone!

I am having some trouble grasping some aspects of the Multiple Regression model. For example, let's say I have a response variable Y and two explanatory variables X1 and X2. Both X1 and X2 are very similar, but I am trying to observe if both have individual significant effects over the response variable. In this case, when I add both explanatory variables, one of them will account for the major effects. How it is decided that it is X1 that capture the effects that both X1 and X2 have over Y?

Not sure if it is clear. Some context might help. 

I had this question when I was reading an Psychology paper that attested that Grit (X1) had no effect over Success later in life (Y) when Conscientiousness (X2) was included in the model. From that, I suppose that both Grit and Conscientiousness have high colinearity, but why is it that Conscientiousness dominates and not Grit? Is it because it explains more variability and are both correlated? ",Question about Multiple Linear Regression,93qx7j,new,4,1,1,0
"Hi, 

I have a background in Finance and Operations Management. Currently I reside in Edmonton where I work as a Data Analyst. I am looking for a good masters in statistics program that I can do online and that is also affordable (<20k).

Does anyone have any good recommendations?

Thanks",MS Statistics Online,93qg1r,new,5,1,1,0
"Hi all,

Apologies if this is the wrong place for this.

I know the odds of one person flipping one coin and getting heads twice in a row is 1/4

But is the odds of two people flipping a coin each and both people getting heads also 1/4?

Also what kind of terminology should I have been using in my google search before failing and coming here? xD

Thanks in advance :)",Probability of one person doing an action twice in a row VS probably of two people doing the action.,93q9g7,new,4,3,3,0
"My textbook states that ""The bias describes how much the average estimator fit over data-sets deviates from the value of the underlying target function.""

The underlying target function is the collection of ""true"" data correct? Does that mean bias is just how much our model deviates from the actual data, which to me just sounds like the error.",Is bias different from error?,93q0v0,new,34,20,20,0
"Full disclaimer, I'm doing my statistics final. I've finished an online portion with a 92% and now we have a hardcopy portion. It's just one question with three parts and I've already set up the hypothesis and tested it but the second portion is constructing an appropriate confidence level and I'm not comprehending it. I've gone to tutoring at school & there is no one there who does stats :/  


I've watched countless videos and I'm still not understanding. I'm using statcrunch and have gotten the first part of my problem, basically are there any other resources I can turn to, or if anyone wants to help me (and I mean really HELP, I want to learn this, not just get the answer) I'd be willing to pay.",I'm really lost on creating confidence intervals,93pfxz,new,15,5,5,0
"What is the relationship between sample size and regression/mediatoranalysis? 

More People in the sample = more likelihood of  significant results?

Because i had to split my datafile of 30 people into 2x15 people. And i can't find significant effects in the split file, however there is an almost statistically significant effect in the full data file of 30 people. 

Its a pity,....of course i would like to show the effect in my sample but it doesn't seem to hold up. ",Regression / Mediator analysis and sample size,93nj0n,new,4,7,7,0
"Hello I want  to ask you,  before  to making a regression  should  we understand  which  distribution  follow  the datas?

Let's   take an example:  if  the  data  have  a poisson distribution,  then  I  will run a  Poisson  regression?

If the  data  follow  a gaussian  then  one should  use  the linear  regression?

I don't understand  when to use linear  regression  and  when  to use  Poisson regression.   Any real use of  case  example?

I leave you an  example:  In this  post this  authot uses  the Poisson regression to build a  model based  on  average goal made  by    2  footbal teams   Home and  Away

[https://dashee87.github.io/data%20science/football/r/predicting-football-results-with-statistical-modelling/](https://dashee87.github.io/data%20science/football/r/predicting-football-results-with-statistical-modelling/)

I don't get   why  he USED POISSON regression, instead  of linear  regression.... is  it   because the data  followed  a  Poisson distribution  in  the first  place?",Before to making a regression,93mm3i,new,16,0,0,0
"Hi everyone,

I'm looking to do a simulation of Dream Team baseball teams. The SimMatchup / WhatIfSports simulators let you make dream teams, but you have to pick a single season. If I was going to select a player for my team, say Rafeal Palmeiro, how do you think I should pick the season? Best season? Closest to ""average"" season? Or maybe #3 season?

I guess I could do multiple methods and then compare them?

Just looking for fun suggestions. I don't know if this would do well in r/baseball, so I thought this might be good.",Baseball Player Simulation Choosing Season (suggestions),93jjxo,new,1,3,3,0
"Hi everyone. A textbook I'm reading poses a scenario. It states the variance of the error term,  Var(ε)=σ\^2, is extremely high. Is a flexible or inflexible statistical learning method better? I thought the answer was flexible because this means our model isn't fitting the shape of the data well. However the textbooks answer is inflexible because a flexible model would be more likely to fit the noise. I think I am misunderstanding something about the error term. Isn't it just the difference between what our model predicts and what the actual data point was? Or is the random error term independent of our model? Any help is greatly appreciated, thanks!",Understanding Variance of the Error Term,93j72l,new,4,1,1,0
,Do you prefer R or Python for Bayesian statistics? Why?,93i9bf,new,24,7,7,0
Goolge’s “Popular times” graph shows how busy a location is during different times of the day.  I was wondering what kind of graph it would be categorized as? A histogram? [Article with picture](https://www.quora.com/How-did-Google-get-the-data-for-its-Popular-Times-results-and-are-there-any-privacy-implications-with-the-acquisition-of-that-data),Goolge’s “Popular times” graph type,93hxmt,new,3,0,0,0
"I recently thought of a statistics problem that I think might be a novel problem. I submitted it to stackexchange (https://stats.stackexchange.com/questions/359854/the-fishing-problem#), but I'll repost the question here:

>Suppose you want to go fishing at the nearby lake from 8AM-8PM. Due to overfishing, a law has been instated that says you may only catch one fish per day. When you catch a fish, you can choose to either keep it (and thus go home with that fish), or throw it back into the lake and continue fishing (but risk later settling with a smaller fish, or no fish at all). You want to catch as big a fish as possible; specifically, you want to maximize the expected mass of fish you bring home.

>Formally, we might set up this problem as follows: fish are caught at a certain rate (so, the time it takes to catch your next fish follows a known exponential distribution), and the size of caught fish follows some (also known) distribution. We want some decision process which, given the current time and the size of a fish you just caught, decides whether to keep the fish or throw it back.

>So the question is: how should this decision be made? Is there some simple (or complicated) way of deciding when to stop fishing? I think the problem is equivalent to determining, for a given time t, what expected mass of fish an optimal fisher would take home if they started at time t; the optimal decision process would keep a fish if and only if the fish is heavier than that expected mass. But that seems sort of self-referential; we're defining the optimal fishing strategy in terms of an optimal fisher, and I'm not quite sure how to proceed.

This problem looks like it's related to optimal stopping (https://en.wikipedia.org/wiki/Optimal_stopping#Continuous_time_case), but it's the self-referential aspect that makes it different. The gain depends not only on the underlying distributions, but also your strategy.",The Fishing Problem,93hxb3,new,33,4,4,0
"Hello all. I am currently a Statistics major, and I planning to earn an MS in Statistics right after graduation. I am interested in becoming a statistical consultant, but I am not very clear how I can do so. I hope you guys can help me with these questions. By the way, if  you are not consulting, but you classify yourself as Statistician, I would also love to hear your story.

1. What was your level of education and the company when you first got your statistical consulting job?
2. What are some tasks you do the most frequently on a daily basis? Coding, writing, meeting, etc.
3. What are things that you like about your job?
4. What are some challenges you face in the career? And what would you recommend beginners like me to prepare for those challenges?
5. Other than yours, what are some places, like companies, that you would find interesting to work for?

Thank you so much. Any input is valued.","Statistical consultants of Reddit, how is your career like?",93hrbg,new,4,1,1,0
"Anyone here have a masters degree in statistics or biostatistics and is working in a non-academic setting (industry, government, etc.)? Do you like the work you are doing, or do you wish you went for a PhD instead?",Post masters degree work,93hm9b,new,16,30,30,0
"I typically get sent data for studies which have 10-15 variables, and the group wants to see if the intervention made a difference in those variables. Thus I basically have 15 response variables and one binary ""intervention yes/no"" predictor (sometimes a few others like age, etc). 

There's a baseline measure and then one or two follow up visits.

The variables are a mix of continuous (hours of sleep, weight), ordinal (never, sometimes, always), discrete ordinal (# of visits to emergency room). 

I know a lot of tools but really don't know the ""best"" way to analyze this data now that I'm in the real world and not in class. For e.g. with the ordinal discrete data I could use a mixed proportional-odds cumulative logit model, but that is really not interpretable to the people I'm working with, when all they want to know is whether their intervention made a difference. Most recently I completely ignored the middle time point and just looked at which variables differed between control and intervention at baseline, then did that again at the final time point. 

That was easy to interpret, but I have no idea if it's a good idea. Any advice from people who also work in the real world?

Also when there's just a baseline timepoint and one follow up time point, can I even use a mixed model for that?",Not sure what models to fit that can be understood by non-stats people,93h1l0,new,3,5,5,0
"https://i.imgur.com/vSKyQlg.png

I don't know any statistics, and I realize there's going to be an unbelievable amount of assumptions in those questions, but it's hard to know what to ask if you don't know what you're looking at. I'm trying to correctly interpret data that will help me improve my life, so I've got to at least attempt to try and work with what I can understand about the statistical process.

I'm looking specifically at the ""95% CL"" columns and the two numbers within each row of those columns. How is CL (which I assume means confidence level) different from calculating *p*, what do the two variables per row in the CL columns represent, and is either a better descriptor as to the reliability of data?",Can Anybody Tell me What Some These Terms and Their Relative Values Mean?,93gx9r,new,2,1,1,0
"Hello, has anybody here been in a PhD program and decided to do a dual major in statistics and get an MS? For example, I am a PhD student in psychology (quantitative psychology) and I have the option to get a Master's in statistics from the Stats department. To those of you who have done this, what are the pros and cons?

I have already taken Master's level probability so it's really just taking the electives at this point and finishing a comprehensive exam to get the Master's. The problem is that this would mean I would stay in grad school for another year. Anybody have some advice? Thanks.",Undertaking a MS in Statistics in a PhD program?,93gx67,new,5,1,1,0
"My idea was to built an index of customer value with its several dimensions (e.g. service quality, price transparency, etc.) and after applying a survey to more than 200 respondents I used CronBach's Alpha to test for internal reliability of items and of overall index with it's dimensions. How can I validate that I selected the right dimensions (and its items) to compute my Index of Customer Value? I believe it's important to validate it but I am not so sure how to do it, if it's through CFA (?) or a Partial disaggretation approach that I saw in another research. I want to confirm that all items of the dimensions truly represent the corresponding latent construct (Customer Value). How?

Thank you in advance, Sara",How to validate an Index with several dimensions?,93ghe5,new,0,1,1,0
"I've considered creating a bar graph with different height bars representing different amounts of time to make the impact apparent, but what I am looking for are some math formulas that will objectively tell me ""The difference in processing times from before we got the machine and after is not random and is definitely because of the machine"". How can I do this?",I have a bunch of data telling how long it took a lab to process samples. I want to show that a new machine we got significantly improved processing time. What is the best way to do this?,93gbv7,new,3,1,1,0
"I have no idea how this work and googling this have confused me even more. Does anybody have any papers or books on this subject?

I need to bin a predictor from the number of ratings to categorize it in popular and not popular.

Typing this out it seems like I can just bin it into two categories... >___> am I doing this right?

Thanks for your time.",discretizing continuous predictor help,93fcjz,new,13,7,7,0
"Well, to start, I have to say that I'm not a statistician, a mathematician or in academia in any way and this makes everything harder.

Some years ago, I thought up a voting mechanism to filter out garbage on internet forums and realized that it can be applied to way, way more than that. Since then I've been trying to learn as much as I can from many disciplines that I deemed relevant in order to see how and where it can be applied and how I should write it down to make it public. I recently started to put it in text and to show it to friends and to people that seemed interested and able to understand it but (I think) it's far from ready for publishing. So, my next thought was to contact a specialist and ask for help (to tell me how it should look in order to send it, maybe to a journal or something), buuuuut, I'm scared that because I am an outsider, he can just appropriate the idea and publish it as his own and take all the credit. I did try to learn as much as I could about voting systems from online courses but my maths are probably at an elementary school level and no matter how willing I am to learn I won't be able to compare to a specialist in voting systems. From what I've learned I think it's a type of multistage weighted voting system.  
I need advice on how to approach this conundrum. 

p.s. - I thought to go and register it as a patent but I do not have the funds to hire someone to deal with all that or even to pay all the taxes and the fees that I should pay if I'd know how to do it myself, which I don't.","Hello, I think I came up with a very powerful voting system and I need some help",93evsb,new,18,0,0,0
"I'm using R's survminer package. I can generate Kaplan-Meier curves and everything looks great. I can also generate cumulative mortality:

    ggsurvplot(fit, data = df, fun = ""event"")

[chart](https://i.imgur.com/CZFhQHr.png)

What I'm interested in is whether the mortality *rate* goes up significantly after 365 and 730 days. From this chart it doesn't seem to: the line is straight for most the time. That's great, but is there another way of displaying this data which isn't cumulative? ie, ""risk of dying at age X""

**Bonus question:** I specifically want to compare ""risk of dying"" at time intervals (0-365, 365-730, 730-1095), how should I go about that? Cox hazard model?",Simple question: how do I get *non-cumulative* mortality rate in survival analysis?,93e7np,new,14,1,1,0
"https://m.imgur.com/a/CCQJ5od

My best friend’s coworker asked this, neither of us know what it is. Any help would be appreciated :)",Who knows what this is and what it is used for?,93d9kj,new,7,1,1,0
[https://imgur.com/a/qJJ4Z6s](https://imgur.com/a/qJJ4Z6s),So I'm probably in the wrong sub but how do I know given these stats that I should fail to reject Hnull,93cmhl,new,1,0,0,0
"Hi. I have a dataset with users who have answered questions 1-10, with data that is quatifiable.
I was wondering which statistical method I should use to see  between answers. For example, persons that answered 2 on question 5 are more likely to answer 8 on question 1, and so on",Correlation between survey answers,93bwp5,new,2,1,1,0
"I’ve searched prior posts and software has been discussed, but not very recently, so hopefully it’s okay to ask.  What would you guys recommend in terms of software to learn for somewhat basic analysis on smaller datasets?  I’ve successfully avoided learning a proper stats program thus far by using things like XLSTAT and manipulating excel with VBA, but as you can imagine, this is a massive headache.  So I figure it’s time to learn.  I’ve used SPSS in the past for a class in college, but it didn’t seem particularly intuitive.  I’d like something that runs natively on a Mac and am debating between stata and R.  I must admit, R is very intimidating and I have very minimal programming experience.  I think it may take too long to learn.",Best software for non-programmer to learn quickly for basic analysis,93a6lp,new,45,24,24,0
"Hi, I'm analyzing some energy consumption data and am trying to find the effect a policy has on consumption.  The policy is a dummy variable I'm calling ES.  I want to know by what percent does the policy change energy consumption.    


I've tried two approaches, the first is to run a regression  


 kwh = a + b1\*ES + b2\*vintage + b3\*sqft + b4\*pool   


and I find ES has a coefficient b1 = -140.  This is average summer data so the average kwh = 1500 so I would have said the ES policy has a -9.3% impact.    


But not so fast, if I run a regression with a log transform on the dependent variable such that  


log(kwh) = A + B1\*ES + B2\*vintage + B3\*sqft + B4\*pool,   


I get a coefficient B1 on ES of -.035 suggesting the policy has a negative 3.5% impact.  


What's the deal? -9.3% vs -3.5% are pretty darned different!  Shouldn't both of these methods calculate similar percent changes?  I know it's just an estimate of percent change and requires the coefficient to be close to zero, but -.035 is pretty darn close to zero.   Any help is greatly appreciated!","Regression Percent Change, log vs not, which approach?",937l24,new,3,1,1,0
"I have a data set of temperature controller data that is zero for the most part but fluctuates between -20% (of the cooling capability) to +20% of the heating capability.  The actual temperature I am measuring responds well to a simple coefficient of variation because the values are all above 200 Celsius, but the same calculation on the controller behaviour gives me ridiculous values (as dividing by very nearly zero tends to) - Is there a simple transformation I can use for this?  I tried LN(X/1-X) but that was only useful for the positive values in my record set.  Any ideas?

PS:  I use LN(X+100) for every value & it gets me a repeatable outcome; but wondering if there’s a ‘proper’ method.",Coefficient of Variation (Mean gets to zero),937exp,new,4,1,1,0
"Suppose I have N numbers. These numbers are separated into P lists. If I know the standard deviation of each of the lists, is there a way to compute the standard deviation for all N numbers as a whole?",Standard deviation of N numbers based on P subsets.,9379th,new,10,1,1,0
"This is a spot in a baccarat tournament - there's 8 ppl in the round and top 4 stacks move up while the other 4 are eliminated, this is the last round to bet before elimination:

We have 100k in chips

The other stack sizes are 10k, 25k, 30k, 90k, 200k, 220k and 400k

You can bet everything from 0-100k (max limit)

The 10k, 25k, 30k stacks all went all in, the 90k bet 20k, the 200k, 220k stacks all chose to not bet this round, and the 400k bets 100k

What is our optimal bet size?

Now, what size should the 90k stack have bet if they acted second last?",Interesting spot,937109,new,1,1,1,0
"I'm trying to analyse some survival data. I have a collection of ""time-to-failure"" measurements that are interval censored: the failure time `T` is not measured directly, but is known to have taken place between `T_L` and `T_R`. I have it formatted as two columns:

    head(durations,10)
    
       TL TR
    1   6  8
    2   6  8
    3   6 12
    4   6 12
    5   6 11
    6   6 14
    7   6  8
    8   6 16
    9   6 10
    10  6 11

I've put the full data on pastebin [here](https://pastebin.com/JjL1vJGR).

According to the `survival` [package documentation](https://cran.r-project.org/web/packages/survival/survival.pdf), I have to create a `Surv` object like this:

>Interval censored data can be represented in two ways. For the first use type = ""interval"" and the codes shown above. In that usage the value of the time2 argument is ignored unless event=3. The second approach is to think of each observation as a time interval with (-infinity, t) for left censored, (t, infinity) for right censored, (t,t) for exact and (t1, t2) for an interval. This is the approach used for type = interval2. Infinite values can be represented either by actual infinity (Inf) or NA. The second form has proven to be the more useful one.

I used the second way:

    survival_object = Surv(time=durations$TL, time2=durations$TR, type=""interval2"")
    print(survival_object)
    
      [1] [6,  8] [6,  8] [6, 12] [6, 12] [6, 11] [6, 14] [6,  8] [6, 16] [6, 10] [6, 11]
     [11] [7, 11] [6,  9] [6,  7] [6,  7] [6,  9] [6, 10] [6, 10] [6, 10] [6,  8] [6, 10]
     [21] [6, 10] [6, 10] [6, 10] [6,  8] [6, 10] [6, 10] [6, 10] [6, 10] [6, 10] [6, 11]
     [31] [6,  9] [6,  9] [6, 11] [6, 11] [6, 11] [6, 11] [6,  9] [6, 11] [6, 11] [6, 11]
     [41] [6, 11] [6, 12] [6, 11] [6, 10] [6, 12] [6,  8] [6, 12] [6, 11] [6,  9] [7, 13]
     [51] [7, 13] [7, 13] [7, 13] [6, 10] [6,  8] [8, 14] [6, 10] [6, 10] [6,  9] [7, 11]
     [61] [6, 11] [6, 10] [6, 10] [6, 10] [6, 10] [6,  9] [6,  9] [6,  8] [6, 11] [6,  9]
     [71] [6,  9] [6,  9] [6,  8] [6,  9] [6, 10] [6,  9] [6,  9] [6,  9] [6,  9] [6, 10]
     [81] [6,  8] [6, 12] [6,  8] [6,  8] [6,  9] [6, 10] [6,  8] [6, 10] [6, 10] [6, 10]
     [91] [6, 11] [6,  9] [6,  9] [6, 10] [6, 10] [6, 11] [6,  8] [6,  9] [6, 10] [6, 10]
    [101] [6,  9] [6, 10] [6, 10] [6,  9] [6, 10] [6, 10] [6, 12] [6, 11] [6, 12] [6, 10]
    [111] [6,  7] [6,  7] [6, 10] [6,  8] [6,  8] [6, 12] [6, 10] [6, 11] [6, 11] [6, 11]
    [121] [6, 12] [6, 11] [6, 12] [6, 11] [6,  9] [6, 10] [6,  8] [6, 10] [6,  9] [6,  9]
    [131] [6, 11] [6, 10] [6, 11] [6,  9] [6, 11] [6, 12] [6, 13] [6,  9] [6, 10] [6, 12]
    [141] [6, 11] [6, 11] [6, 13] [6, 12] [6, 12] [6, 10] [6, 10] [6, 14] [6, 15] [6, 12]
    [151] [6, 11] [6, 10] [6, 11] [6, 13] [6, 12] [6,  9] [6, 12] [6, 12] [6, 12] [6, 12]
    [161] [6, 11] [6,  9] [6, 13] [6, 12] [6, 13] [7, 14] [6, 10] [6, 13] [6, 13] [7, 15]
    [171] [6, 12] [6, 14] [6,  8] [6,  8] [6, 10] [6, 12] [6,  9] [6,  9] [6, 11] [6, 10]
    [181] [6, 10] [6, 10] [6, 12] [6, 11]

And it seems like it has understood what I meant. However when I plot the survival graph it seems to assume that the failure occurred only at the left extreme of each interval:

    fit = survfit(survival_object~1)
    plot(fit)

[Picture of survival plot](https://i.imgur.com/kDSp1Oa.png)

Surely, the graph should extend out to 16, which is the maximum value in `durations$TR`, because we can't be sure the last failure didn't occur until that time? Have I misunderstood what the survival graph is supposed to represent?

Furthermore, when I fit a parametric model to the data it seems to be underestimating it too:

    lognorm_fit = survreg(survival_object~1, dist=""lognormal"")
    lognorm_fit$coefficients
    (Intercept)
    1.9779
    lognorm_fit$scale
    [1] 0.05768311
    lognorm_sims = rlnorm(10000, 1.9779, 0.05768311)
    hist(lognorm_sims)

[Histogram of lognorm\_sims](https://i.imgur.com/eXu8kRD.png)

As you can see, it's clustered around the left sides of the intervals, with nothing going on past 9. Shouldn't there be some realizations that get out to 10 or 12 or 14, because that's where the right sides of the intervals go to? (The same thing happens when I fit a different distribution like Weibull, so it's not just some quirk of the lognormal at play here).

So my question is: **have I misunderstood the statistical concept, or have I just coded it wrong?**","R: ""survival"" package seems to underestimate survival times; have I misunderstood how it works?",935gf9,new,17,20,20,0
"Hi y'all,

[Here's what my data looks like](https://imgur.com/a/t9MOyS9)

**Question asked by work**

 Why is it for two projects with similar technical aspects, there is different costs? (Is it because of the person who is working on it? is the real question I am being asked)

**My solution**: 

Construct a Linear Regression that takes into account a bunch of technical variables about the project, ultimately trying to see how cost is affected by the Employee working on it.

**My problem**: 

 I read somewhere I need 10-20 observations per variable I am measuring. I have a little over 30 observations, and about 30 variables measured, so I am really off. I have a second round of data I am collecting that will at most increase it another 30 observations.

**Questions**:

* Does this mean I can't do my analysis?

* Is a solution to this issue to simply eliminate variables that I think won't be as relevant? Side Question: If having less variables in a single regression allows you to see the effect better, why can't I just do 30 different regressions that looks at how each variable affects cost individually? Why is that different from doing them all in one regression?

* While not having enough observations stops me from finding the ""true effect"" of a variable I am mostly interested on only 1: The employee working on the project. While I can't find the ""true effect"" of the other variables(such as Color), would including them in the regression at least control for their effect? I am just worried if I run a regular linear regression with just the person, that is leaving the analysis open to the interpretation that one person might have had projects with more ""Widgets"" for example, which is what actually caused the cost to increase.

* What regression model is appropriate for this data? I just assumed linear regression would work? 

Sorry for the long list of questions. I would appreciate greatly help on one or any of these! 
","How to proceed when ""not enough"" observations for Regression? + Suggestions on a model",935evl,new,5,1,1,0
"Hey everyone, epidemiologist with a biostats question. Our former biostatistician was fired and my work has me filling in currently. Does anyone have any experience using a Poisson regression with a robust error variance to estimate a risk ratio for a binary outcome?

My work is having me analyze treatment completion (Yes, No) but it is a common outcome so the OR is overestimating the risk. I found this article detailing a method to provide a more accurate estimate, but have never used it so am hoping others may have and can provide feedback if they used it or went with another method.

Here is the method article I found https://academic.oup.com/aje/article/159/7/702/71883",Poisson regression with binary outcome,9347r3,new,7,0,0,0
"Hi all, I am fairly new to statistics and have a rather basic question. What exactly is meant with ""Controlling for a variable"" and how do we do such a thing?

The way I currently understand it, is that we might observe some effect (e.g: average height of people in US might be higher than average of people in Canada, just making up an example). However, we did not ""control for gender"" and if there are more men in the US than in Canada that might be skewing our result. Therefore we would have to ""control for gender"" but how would we then do such a thing?

Also, is it true that every meaningless correlation that we find (so nonsense ones that dont make sense like a correlation between temperature in Paris and amount of chocolate consumed by the average Chinese person) could simply be nullified if we were to ""control for every possible variable?"".

Thanks and sorry for the stupid question. Perhaps this is not the right place to ask.",Basic Question: meaning of controlling for variables?,933zmn,new,4,2,2,0
"Hello, new on statistics, forgive if  I say  somenthing  wrong, but   Z test  and  t- student test do they  make the same  thing?  My question is  when  to use  one, when to use the  other one?",When to run a Z- test and when to run a t-student test?,932ott,new,41,33,33,0
"I am a grad student with basic statistical knowledge working on a study a bit over my head which conducted a KAP survey on three villages directly following an intervention (1 village received a single intervention, one village received two interventions and the third is a control village where no interventions were conducted. The survey collected 23 knowledge attitude and practice variables and then a second survey was conducted two years later visiting the same individuals (matched with a caseid to the prior survey). This survey repeated 6 categorical questions on the KAP survey as well as collecting demographic data for the individuals.

There are two hypotheses. The first is that the village with two interventions would have the least fading of knowledge over the two years. The dependent measure is a count variable of the number of techniques specified (0-9) and the independent is a categorical variable of the three villages.

The second hypothesis is that the groups receiving interventions would have better knowledge attitudes and practices than the non-intervention village. The dependent variable would be a binary or a count depending on the KAP question and the independent would again be village.

I am thinking that for the second hypothesis a simple regression could be done on just the first survey without using the second.

I am not sure how to approach the first hypothesis. I have tried setting the data as survey in Stata and running analyses (xtreg), but I am having co-linearity issues and am not sure how to interpret the coefficients.

Thank you for reading, any guidance would be appreciated greatly.",Questions about analysis of post-test data,930zdy,new,0,1,1,0
"for a project I was tasked to choose a claim and preform a z test to prove or disprove it. The claim I was given was that 2% of the people in the world have green eyes. I did a poll, and 55 out of 179 said they had green eyes. How do I plug these numbers in? Is my observed proportion 55? also what would my null hypothesis be?","Trying to do a Z test, am I using the right values?",930bwl,new,32,4,4,0
"I have been working with data that involves fish lengths between nine lakes. 

I have been able to create some box-plots in R, using lengths on the y-axis and separating each lake with it's own box-plot and ordering them from closest to farthest from a city. Although some of the median lines are apart from each other, most of the quartiles overlap with each other and so I would expect a null result from any statistical test. 

Would a chi-squared test for homogeneity be most appropriate for this data? ",Statistical Tests Between Several Box-plots?,9300zp,new,4,1,1,0
"Hi,

I've got a (zero inflated) negative binomial GLM that I'm using and am curious what programs would be able to graph this analysis? I'm most familiar with JMP and SigmaPlot but unfortunately neither of them can do this. Most of my analyses include a continuous predictor, a blocking factor and the interaction or just the continuous predictor (no block or A*B). 

Any advice? To clarify, I'm not interested in graphing the distribution of my response variable. I'm interested in looking at the relationship between my response and my predictor(s). 

Thanks.",Programs that can graph negative binomial regressions?,92yge2,new,18,6,6,0
"Hi, I am a poor PhD student looking to do a merge of debt data and the IMF rules database and I only have Stata 13. Could someone help a brother out with a saveold autoold, version(13) command. I can give you the link to the data set or send an email. 

Thanks",Can anyone help me convert a Stata 14/15 to a 13?,92xt67,new,8,0,0,0
"Thanks for taking the time to read this post!

I am a marine biologist working with mortality data for bay scallops. Today, my job is to find out if there are significant differences in cumulative mortality (in %) among groups of scallops living in different densities (low, medium, and high). 

I have already looked at proportional mortality per day and have run a repeated measures ANOVA on those data, yet I'm running into a block trying to test for differences among groups with the cumulative data. 

For example the data means look like this for each time point: Low Density \[0.5 0.6 0.7 0.75 0.85\], Medium Density \[0.65 0.75 0.85 0.9 0.98\], High Density \[0.7 0.8 0.85 0.87 0.90\].

Basically, what statistical test do I run with cumulative data like these? For what it's worth, I'm working in MATLAB 2018a.

Thanks again!",Need Help Testing Significance among Groups using Cumulative Data,92wbga,new,1,9,9,0
"Hello I am  studying the polinomial regression. On this  website I just have found  an example:

[https://www.r-bloggers.com/polynomial-regression-techniques/](https://www.r-bloggers.com/polynomial-regression-techniques/)

Suppose we want to create a polynomial that can approximate better  the following dataset on the population of a certain Italian city over  10 years. The table summarizes the data:

First we import the data into R:

Year <- c(1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969)Population <- c(4835, 4970, 5085, 5160, 5310, 5260, 5235, 5255, 5235, 5210, 5175)At this point we can start with the search for a polynomial model that  adequately approximates our data. First, we specify that we want a  polynomial function of X, ie a *raw polynomial* , is different from the *orthogonal polynomial*.  This is an important addition because the controls and the results will  change in the two cases R. So we want a function of X like:

At what degree of the polynomial stop? Depends on the degree of  precision that we seek. The greater the degree of the polynomial, the  greater the accuracy of the model, but the greater the difficulty in  calculating; we must also verify the significance of coefficients that  are found. But let's get straight to the point.

In R for **fitting a polynomial regression model** (not  orthogonal), there are two methods, among them identical. Suppose we  seek the values of beta coefficients for a polynomial of degree 1, then  2nd degree, and 3rd degree:

fit1 <- lm(sample1$Population \~ sample1$Year)fit2 <- lm(sample1$Population \~ sample1$Year + I(sample1$Year\^2))fit3 <- lm(sample1$Population \~ sample1$Year + I(sample1$Year\^2) + I(sample1$Year\^3))

Or we can write more quickly, for polynomials of degree 2 and 3:

fit2b <- lm(sample1$Population \~ poly(sample1$Year, 2, raw=TRUE))fit3b <- lm(sample1$Population \~ poly(sample1$Year, 3, raw=TRUE))

The function polyis useful if you want to get a polynomial of high degree, because it avoids explicitly write the formula. If we specify raw=TRUE, the two methods provide the same output, but if we do not specify raw=TRUE(or rgb(153, 0, 0);"">raw=F), the function polygive us the values of the beta parameters of an orthogonal polynomials,  which is different from the general formula I wrote above, although the  models are both effective.

Let's look at the output.

summary(fit2)## or summary(fit2b)

Call:lm(formula = sample1$Population \~ sample1$Year + I(sample1$Year\^2))

Residuals:Min      1Q  Median      3Q     Max-46.888 -18.834  -3.159   2.040  86.748

Coefficients:Estimate Std. Error t value Pr(>|t|)(Intercept)       5263.159     17.655 298.110  < 2e-16 \*\*\*sample1$Year        29.318      3.696   7.933 4.64e-05 \*\*\*I(sample1$Year\^2)  -10.589      1.323  -8.002 4.36e-05 \*\*\*---Signif. codes:  0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(He said) 

 **Residual standard error**: 38.76------------>   WHERE IS it?  I can't  find  it out  in the output!

Then he continues:

on 8 degrees of freedomMultiple R-squared: 0.9407,     Adjusted R-squared: 0.9259F-statistic: 63.48 on 2 and 8 DF,  p-value: 1.235e-05 .

I can't  find  those  'data'  in the output.  What am I  missing?  This  is  the website: [https://www.r-bloggers.com/polynomial-regression-techniques/](https://www.r-bloggers.com/polynomial-regression-techniques/)",Residual standard error on polinomial regression,92w94z,new,1,1,1,0
"So if I wanted to estimate the true height of a person and I knew A) the width of her body and length of her feet, and B) that the variance explained (correlation) of body width and height are 50% and feet-length and body height is also 50%, can I just take the SDs of each of the two measurements and average them (say that for example she has a body width of 0.5 SD and a feet-length if 1.0 = average 0.75 SD) and compare it with a height distribution of the population and thereby estimate her height? Also, considering each measure had a 50% variance explained, could I estimate a sort of “confidence interval”? If one of the measurements had a higher correlation with the target value, would it make sense to weight it more in the estimate? Would averaging the two measurements combine a 75% confidence that the resulting composite value is not noise (50% times 50% equals 25% unexplained variance)?",Variance explained and composing different measurements to find estimated true value?,92ui75,new,5,5,5,0
"As the title says, I'm so confused by the concept. I've read so many explanations for the concept for the past few hours and I'm even confused than when I started, because a lot of the explanations seem to be contradictory.

R-bloggers states:

* It is not the probability that the true value is in the confidence interval.
* We are not 95% sure that the true value lies within the interval. (to me this means that we can't say with 95% confidence that the true value lies within the interval)

Here'sn example of several comments I've read that support these statements:

/u/TokenStraightFriend

"" Building off that because I only recently came to grips with what exactly ""95% confident"" means. It does NOT mean that there is a 95% chance that the true population average is within that range. Instead, if we were to repeat our sample taking, measuring, and averaging, we expect for 95% of the time the average height we find will be within that range we predescribed. ""

Yet other comments contradict this

""So let's say you want to be 95% confident, so mostly certain, but with just a small degree of uncertainty. Then z=1.95, so we can say that the **average population height** is somewhere between 69-3(1.95) and 69+3(1.95) inches tall""

Is that not directly contradictory to what R-blogger states?

Here's an explanation from Eberly College of Science:

""

Rather than using just a point estimate, we could find an interval (or range) of values that we can be really confident contains the actual unknown population parameter. For example, we could find lower (*L*) and upper (*U*) values between which we can be really confident the **population mean** falls:

*L* < *μ* < *U*

And, we could find lower (*L*) and upper (*U*) values between which we can be really confident the **population proportion** falls:

*L* < *p* < *U*

""

Notice they say population and not sample. The distinction is made super clear in the Eberly college example.

I keep reading this idea that if you were to construct an infinite number of confidence intervals at a single confidence level 95%, 95% of those intervals may contain the true value for the parameter. That sort of explains what a confidence level is to me, but I don't understand when someone tells me 'this specific confidence interval has a confidence level of 95%'.",I am so very confused by Confidence Intervals,92rpob,new,47,12,12,0
"Hello everyone,

I was wondering if someone would be able to help me decide on the proper set up for a multi-level model using the lmer package.

In my study, we are looking at heart rate in beats per minute before and after a medication. Some of the participants have a different number of observations, but each observations is identified with a day and month. And there is no day and month that overlaps among any of the patients.

For instance:

        id BPM  Day    Drug
        1  80   3/10   No
        1  49   3/10   Yes
        1  50   4/11   No
        1  87   4/11   Yes
        2  47   4/12   No
        2  47   4/12   Yes
        3  43   5/1    No
        3  45   5/1    Yes

In setting up the model, to control for the fact, that these are paired, could one concivably write it as either (I believe these set ups would give same results):

    lmer(BPM ~ Drug + (1 + Drug | id/Date)) 
    
    or
    
     lmer(BPM ~ Drug + (1 + Drug | id) + (1 | Date)) 

Is it okay to treat Date as a random effect? We aren't interested in time as an effect, just using it as a categorical variable to identify that these are paired? Would this be the best/appropriate approach?

Edit: And I do understand that the first model would be nesting the Date with Subject, but since these are all unique by id anyway, that would make it okay to use either of the two examples above? 

Thank you so much!",Multi-level Model Random Effects - Date,92ra2n,new,9,3,3,0
"[https://online.stat.tamu.edu/course-list/](https://online.stat.tamu.edu/course-list/)

This is the full list of courses offered at Texas A&M for their online MS in Statistics degree. Someone in another subreddit told me that it looks more like an undergraduate program, and it's missing important courses one would expect from an MS program. Apparently they only let their PhD students take the machine learning courses.

My other graduate school options right now are an MS in Data Science from University of Missouri:

[https://dsa-wh-prod.missouri.edu/curriculum/](https://dsa-wh-prod.missouri.edu/curriculum/)

Or an MS in Data Science from CUNY:

[http://catalog.sps.cuny.edu/preview\_program.php?catoid=2&poid=607](http://catalog.sps.cuny.edu/preview_program.php?catoid=2&poid=607)

I'm finding it difficult to make a decision between the three programs, but I'm leaning toward CUNY because I don't want to finish my MS without machine learning classes. My initial inclination was to go to Texas A&M, but the limited course offerings, combined with their focus on SAS over R, are leading me to shift away.

My eventual goal is to work as a data scientist. ",How do you all feel about this course listing at Texas A&M?,92pytl,new,28,8,8,0
"I've seen good recommendations for nonparametric statistics books (in general).  Any recommendations or ideas for nonparametric stats books with *a Python approach* (code examples, references, etc.)?  There exist a couple in R, but I can't find one for python.

My goals are a reference book and self-learning.  I could probably get by with statsmodels reference and any well regarded nonparametric graduate textbook, but I'm wondering if there exists a Python focused book or if anyone has recommendations that has been in this situation previously.

Thank you!",Nonparametric Statistics (applied) Book for Python?,92po1t,new,12,14,14,0
"Hi, total statistics noob (please delete if not allowed and direct me to the correct sub).

I need help understanding some data.

I have a piece of data that I need to understand.

The context is:

>Seventy patients (20%) presented BC after liver transplantation. Bile leakage (24/45%) and stenotic anastomosis (21/30%) were the most frequent complications. Presence of BC in Rh-nonidentical graft-host cases (23/76, 30%) was higher than in Rh-identical grafts (47/269, 17%) (P=0.01). BC was also more frequent in grafts with arterial thrombosis (9/25, 36% vs 60/319, 19%; P=0.03) and grafts with cold ischemia time longer than 430 min (26/174, 15% vs 44/171, 26%; P=0.01). Multivariate logistic regression confirmed that Rh graft-host nonidentical blood groups \[RR=2(1.1-3.6); P=0.02\], arterial thrombosis \[RR=2.6(1.1-6.4); P=0.02\] and cold ischemia time longer than 430 min \[RR=1.8(1-3.2); P=0.02\] were risk factors for presenting BC.

1. What does \[RR=2(1.1-3.6); P=0.02\] mean in this case?
2. Is the data provided enough to determine what kind(s) of Chi‑squared tests were used?",Understanding Results.,92p9qu,new,4,2,2,0
"Hello everyone!

I'm an economist and I want to do a MSc in Statistics at a local university. I think doing it will be very important for my career, my job, and my future plans (I want to do a Ph.D later in life... not sure if in economics or somthing like Machine Learning).

The problem is that I'm struggling with the thesis idea. I want to combine something like Neural Networks and apply them to somethig in economics. Could you help me with places to look for what is being developed, or what are current research questions in statistics-NN?

Thank you so much!!",State of the art? Thesis for MSc,92mbso,new,9,7,7,0
I get the message that it cannot find the function 'ncvTest'. I have downloaded packages car and carData. Says car has been built under R 3.5.1. How do I get this to work?,ncvTest in R 3.5.1,92lm99,new,7,4,4,0
"I have a set of people who split in to 5 groups. Each group is then monitored for sales. If they hit a certain target they are marked as successful. Each group can have a different number of people so it's not easy to eyeball the difference. 

My data contains a summary of the groups and how many were and weren't successful. 

First of all i want to see if the group is statistically significant in them being successful. I think the chi-sqaure test works for this?

Secondly how do I determine which groups performed the best? Can I take the difference between the expected results and the actual results - the bigger the difference from the norm the more successful the group?
",Is chi-sqaure the correct solution?,92kq6v,new,9,1,1,0
"hi Everyone

I'm having a bit of an issue. I'm looking to compare two dependent variables f(x) and g(y) that each are based on 3 independent variables. Basically i'm trying to compare whether the independent variables of f(x) have an impact on g(y). 

Is a Multivariate Regression good enough or is it Multivariate Multiple Regression.",Regression for 2 Dependent variables,92iqrx,new,4,0,0,0
"Hello, could you explaine to  a  newby  when / in which  cases  does it  apply  polinomial (non linear)  regressions?

I know that they generate a   curve  line which  fits  the datas.  What  I don't  understand  is when  is  used  this  kind  of regression.

In linear regression we  have IV   and  DV and  we  want to extimate  a  correlation  between, and  the  regression line  which  fits  the  data  is  the  expression  of  this  correlation and  the Least squared computation   which  produce a  linear  rect,  But  how and WHEN  do  it  originates  a curvilinear  line?

In  which  cases?  Could  you make  an example?  with  an IV   and  a  DP. V ?",Polinomial regression,92ibhj,new,9,0,0,0
"Hi! I could use some help on a experiment on plants.

The experimental design had three different types of treatments and one ""normal control"" and another control that was purely there to test if pH had an effect (the other treatments were of a nature that made some pH change inevitable). Let's call it a pH control henceforth.

My question is if the pH control must be included in the ANOVA's used to analyse the data or if it would be acceptable to first test if there's difference between the two controls and if there isn't then continue the analysis without the pH control. Is one option more correct than the other?","How to handle ""secondary control""",92hoyj,new,0,4,4,0
"Hi all,

I'm working with a data set of school characteristics ranging from student and teacher demographics to poverty rankings across years. This data set is slated to be used for matching similar schools as a control for future analysis of the impacts a treatment has on a class' various outcomes.

**My Question:**

I understand matching is often done with knowledge of which units have been treated and which are controls, but is it possible/done to do a preemptive match by characteristics without such present knowledge of treatment status?

I can understand propensity matching and the like but can't seem to find a way to implement this kind of matching via Stata.

Thank you for your time!","Matching Units by set of Characteristics, without treatment/controls",92hh13,new,2,2,2,0
"Hi. I am reading OpenIntro Statistic's chapter on multiple regression. It uses tables that contain std. errors and z values for variables, but says that knowing how to calculate these things is something that will be saved for a later class. What is the intuition behind the meaning of these  though? Does the standard error mean how far on average the coefficient of the variable would be from the ""true"" coefficient? What would the z-value mean? Is it the bigger the value the more likely it is that this variable does have a relation to what we are predicting? Thanks!",Std. Error and Z value meaning in multiple regression,92hfsg,new,10,9,9,0
"My paper ""The choice of effect measure for binary outcomes: Introducing counterfactual outcome state transition parameters"" has just been published in the journal Epidemiologic Methods. In this paper, we propose a new class of effect measures for causal effects. While this new effect measure is not generally identified from the data, we argue that it can be used to formalize biological knowledge relevant to effect modification and to the choice between standard, identifiable measures of effect such as the risk ratio and the risk difference.

The published version is available at [https://www.degruyter.com/view/j/em.ahead-of-print/em-2016-0014/em-2016-0014.xml?format=INT](https://www.degruyter.com/view/j/em.ahead-of-print/em-2016-0014/em-2016-0014.xml?format=INT) (behind paywall). A preprint is available at [https://arxiv.org/abs/1610.00069](https://arxiv.org/abs/1610.00069). 

An informal discussion of COST parameters has been posted to Less Wrong, at [https://www.lesswrong.com/posts/K3d93AfFE5owfpkx4/counterfactual-outcome-state-transition-parameters](https://www.lesswrong.com/posts/K3d93AfFE5owfpkx4/counterfactual-outcome-state-transition-parameters)

I would very much appreciate any comments, especially critical ones. As always, I invoke [Crocker's Rules](https://wiki.lesswrong.com/wiki/Crocker%27s_rules) for all discussion of this paper.",Counterfactual Outcome State Transition Parameters,92gmxd,new,1,6,6,0
"Hello everybody,

I'm currently looking to build a predicting model for a big dataset I'm working on. However I'm kinda drowning in the number of regression models out there.

Does anyone of you have any tips on how to approach this? I'm dealing with a dependent variable on a continuous scale. Three binary independent variables and two independent variables on a continuous scale as well.

Checked for multi-collinearity which is not presented.

Data consists some outliers.

Presence of heteroscedasticity

I'm seeing models like robust regression model, polynomial, ridge regression and so on. Does anyone have a tip or great articles where I can decide which type of regression will be best to use. I want to overcome the outliers, the heteroscedasticity and of course don't want to overfit or underfit the data.

Thanks for any input.",Deciding which regression model to apply,92d1uk,new,3,3,3,0
"The City of Philadelphia has planted 1,000 stormwater trees at 200 properties (e.g. park, school, street). There are a different number of trees per each location (anywhere from 1 tree to 25 trees).  We tracked when they died. We want to know if the species mattered and if the property mattered in terms of whether they died or not. For example, 26/85 Red Maples died but 0/12 Sweetgum died. But is 12 Sweetgum enough of a population to draw a conclusion? Some properties had tree deaths many times - but how do we know if there is a location bias, i.e. the property is just a bad place to plant trees vs. just the chance of randomly getting deaths at the same place twice? Thanks for any insight.... ",Population Size and Location Bias ??,92c1yx,new,5,12,12,0
"Let’s write kdn to denote the distribution generated by rolling k fair n-sided dice and adding the results.

I was talking to a friend about how if your goal is to get the highest role, sometimes rolling more smaller dice does better across the board. Let’s call A unambiguously better than B if, for all k, P(A &gt;= k) &gt;= P(B &gt;= k) and inequality occurs for at least one k. We can observe that 2d4 is unambiguously better than 1d6, and 2d6 is unambiguously better than 1d10.

**The question I have is:** what is the computational complexity of determining if xdn is unambiguously better than  ydm?

**Edit:** I can currently do it in O(α log α) time, where α = max(xn, ym). The approach is the use FFT to calculate the generator functions quickly and then compare coefficients. This is pretty fast, but is effectively “be clever about how you check all the probabilities.” I would be interested in improvements to this approach, but most especially interested in algorithms that do not need to check every probability. Given the high degree of structure to the distributions I’m considering, this doesn’t seem unreasonable to hope for.

**Edit 2:** In response to a comment, yes I am interested in generalizations such as 1d4 + 2 vs 2d5 or 1d10 + 1d6 vs 4d4. My algorithm works on the first case with no change in run-time, but for the second case the algebra gets messy and I haven’t done it yet.",An algorithm for choosing which dice are best,92a5dw,new,17,7,7,0
"Q1:I searched, and didn't find an answer. I understand that a CI with 95% confidence means that if we repeat the sampling and CI creation process then we would expect 95% of these confidence intervals to contain the true population parameter. BUT, this says nothing about the endpoints for the CI I already did calculate. Let's say I'm sampling heights in inches, setting a 95% confidence interval and my resulting confidence interval is (60,70). What is the explanation of this interval that specifically mentions ""60"" and ""70"" or ""between 60 and 70""?

If I'm using a number between 60 and 70 in whatever analysis because of the calculated CI, what reasoning can I give to back up my choice other than saying ""because this number is in the CI""?

Q2: Let's say we have a linear regression model that predicts weight in lbs (y var) from height in inches (x var). To create a PI, we need an x value (let's say we use 65 inches). The 95% PI in lbs results in (180,220) for x=65. Does this mean that if we look at many people who are 65 inches tall, 95% of the time their weight will be between 180 and 220 lbs?",Yet another CI and PI question,9299n5,new,0,2,2,0
"Hi everyone, I hope you all are doing well. I am applying to the MS Statistics at San Jose State University, which is the school I am going to as a Statistics major now (I also have a minor in Computer Science). You can read about their admission and course requirements at [http://www.sjsu.edu/statistics/degrees/statmaster/](http://www.sjsu.edu/statistics/degrees/statmaster/). The program is quite young, about 7 years old. From my observation, there is a good portion of people who weren't Math/Stats major in the program.

I am studying for GRE, but I think the only section I can do well is quantitative reasoning. As English is not my first language, the verbal reasoning is quite overwhelming to me. My writing wasn't so bad, but being a slow writer, I don't think I would perform well in the analytical writing.  With that said, I have some questions:

1. In general, is GRE an important factor in MS Statistics programs?
2. Do verbal reasoning and analytical writing section carry a lot of weight in the decision making?
3. I wonder if it is appropriate to ask the coordinator if he really puts a lot of weights on GRE, or any section of GRE?
4. How competitive is the admission to MS Statistics at a state university like mine?

I did not think of becoming a Statistics major when I started college , so I wasn't serious with all the Math classes. But by the end of my second year, I realized my passion for Statistics and thus, I tried really hard with the coursework. I got all B for Calculus and Linear Algebra, but all my upper-division Stats-related courses, namely Applied Stats I-II, Probability Theory, are all A/A+. Currently I have a GPA of 3.37. If anything that can boost my chance, I would say letters of recommendation. Some Statistics professors teaching me did recognized my effort to learn, so I think they will be willing to help me in the admission process. 

I really love Statistics, and I just want to expand my knowledge in this subject and work in the field ultimately. On top of that, I love the school, the program, and the professors. I honestly don't see myself fit in a competitive environment.

Please help me if you can. Any advice is strongly welcomed. Thank you so much.","I am applying to a terminal MS Statistics, but I am very worried about GRE. May I have some advice?",9275k8,new,10,2,2,0
"Hello,

First post here. I apologize to anyone who will be mad that I am posting something that looks like homework but I am actually trying to create random propositions of salaries for each employer in a company.

I have information that is accessible publicly:

\-Number of employees (Sample size is relatively small 20-50 )

\-Median of all salaries

\-Avarage of all salaries

\-Standard deviation of all salaries

\-Avarages of Q1 and Q3

\-My salary

\-Departments - I can weight them according to importance thus bigger salaries

\-Minimum salary

Many thanks for your help.","Knowing Medianm Q1 and Q3, standard deviation, how to propose values for individual samples",926jm8,new,5,1,1,0
"Hello /r/statistics,

In short, for a small side project I want to see if there are any correlations between U.S. school ranking and the number of students the school has (undergrad and graduate). So I am thinking that the outcome is rank(1 to 20) and covariate is total number of students (undergrad + grad). But I am not sure what is the best way to go about doing this since there are 20 categories for the outcome. 

Any input is appreciated. 

Thanks!",Best approach at analyzing ranking type of data?,926hut,new,1,1,1,0
"Hi   


I am trying to analyse the effect of the variable VC (see image, dummy) on the variable HC and I've set up the database with other variables which will serve as more of control dummies and side analyses. The database is consistent of unbalanced panel data, however I've added blanks to the data so it becomes ""balanced"" and can be imported into my stat program (Gretl). While trying the fixed effects model I've disregarded that the dummy will be omitted due to obvious exact collinearity. Since the fixed effects estimator takes out all the variance at the group level, there is nothing left for those dummies to explain. But I am interested in knowing that exact effect. Is this solvable (maybe through transformation of my dataset)? If so, how?   


A screenshot of the database composition is added below.   


Kind regards  


**Side information:**  


VC -> Venture Capital, dummy variable which indicates whether or not the cross sectional variable has been a subject to Venture Capital injection, 1 = VC investment, 0 = control group;  
HC -> HeadCount, number of employees during a given year;  
IY -> Investment Year;  
PE -> Payroll Expenses;  
Bubble -> Dummy 1 if the VC investment was done before the bubble;  
Crisis -> Dummy 1 if year = 2008;  
VA -> Value Added;   
NP -> Net Profit;  
IPO -> Dummy 1 if company has ever issued an IPO.  


Transformed database from VICO dataset ([https://www.dropbox.com/sh/9i37674y...UpLObkxa?dl=0&preview=VICO+Updated+Report.pdf](https://www.dropbox.com/sh/9i37674y115kzyi/AACf10BplF-J6VMIiUpLObkxa?dl=0&preview=VICO+Updated+Report.pdf)). As you can see, the data does not line up perfectly.   


If more information is required, please do ask.

\[Image\] [https://ibb.co/mxiQyo](https://ibb.co/mxiQyo) \[/image\]","Fixed Effects, important dummy independable variable",926f5v,new,0,1,1,0
"I have a least squares problem AX=B (where A and B are matrices, 6x4, and 6x3, respectively). so the solution X is 4x3. I want to constrain it so that the four ""corners"" of X are the same magnitude, but alternate signs as follows:     

      a * -a
      * *  *
      * *  *
     -a *  a

I'm a little embarrassed but it has been a few (cough...ten) years since I studied optimization, and I'm not sure of the best approach. Note I'm not setting the value *a* to any particular magnitude, it is free to vary: I just want these four corners to be the *same*. 

Linear least squares with equality constraints? Usually the equality constraint is something like C X = D, and frankly I'm not sure how to express the above constraint using such an equation: matrix multiplication is the dot-product of columns of A with X, so I can see how to get column elements of X equal, but not row elements. I may be doing something idiotic here.

If that's not the right approach should I be using Lagrange multipliers? And if so, how many constraints do I have officially? Three?    

     x11 + x31 = 0 (top two a's)
    -x41 -  x34 = 0 (bottom two a's)
     x11 -  x44 = 0  (top left and bottom right a's)

Ugh sorry I'm so rusty with this stuff if anyone can just point me in the right direction I'll be able to clear out my cobwebs, and will come and post any updates.

----
Edit: I just x-posted this at /r/math: https://www.reddit.com/r/math/comments/92cy02/question_about_least_squares_with_multiple/

",Question about least squares with equality constraints,9269st,new,1,1,1,0
"The course is about to start in the month of September. If we set up a channel now and work on basics, we can definitely finish up the entire micromasters like a class room learning. It'd be great if people are willing to finish the course rather than just checking it out.",Folks interested in studying MITs micromasters in Statistics and Data Science. How about a study group?,925tze,new,33,11,11,0
"Suppose I model

>p(data, theta) = p(data|theta) p(theta)

Then it typically happens (Bernstein-von Mises theorem) that my choice p(theta) doesn't *really* matter after my sample size grows. For a given data set, I could just make the prior stronger by writing something like

>p(data, theta) \~ p(data|theta) \[p(theta)\]\^r

for some fixed natural number r, and I play around with r until I get the posterior distribution I'm apparently looking for. If r is large then the prior has a larger impact on the joint probability. Is there, however, anything like

>p(data, theta) \~ p(data|theta) \[p(theta)\]\^n

where n is now the number of samples, so that the joint distribution is now somehow independent of sample size? I know that's not a desirable property in most cases, but I wonder if this kind of operation has a name.

In other words, I modify the AIC to get the BIC by letting the penalty depend on the (log) sample size. Is there a similar, named strategy when dealing with priors?",Prior on a parameter as a function of sample size?,925cg9,new,0,1,1,0
"The New York Times put out a [precinct-level map](https://www.nytimes.com/interactive/2018/upshot/election-2016-voting-precinct-maps.html) of how the entire United States voted in the 2016 presidential election.  It has a lot of cool features, like telling you where the nearest precinct that voted for the other candidate is, and typing in any address and seeing how it voted.

Choropleths are cool, but this is probably the best I've ever seen.",An Extremely Detailed Map of the 2016 Election,924ltn,new,30,86,86,0
"I am doing a quick study on availability of agricultural insurance and food security. I am using data for 3 years from 20 countries. I am using the Food Security Index (independent variable) to the availability of agricultural insurance (dependent variable).

Availability of agricutlural is a dependent variable which ahs 3 independent variables: FDI, Government spending on agriculture (%) and Insurance penetration ratio.

I'm just a bit confused as to which regression i should use to compare availability of agricultural insurance to Food Security Index

Thanks alot",Agricultural study help,923v58,new,1,1,1,0
"I asked this question [Here](https://stats.stackexchange.com/questions/357584/linear-regression-when-x-is-random-and-gaussian), but has not got any answer so far. 

I felt that this is the kind of question that should have been solved decades ago in academia and should be a part of most standard stats curriculum, but sadly i couldnt find the answer. Can anyone here provide some inputs. Will appreciate it. ",What is the covariance matrix for the regression beta estimate (beta hat) when x is random or autocorrelated?,922u72,new,3,1,1,0
"I have two sets of data, and one is more recent than the other. I’m trying to use these data to formulate a model. So how do I give greater weight age to the more recent data while being fair? I think normalizing it will give me an incorrect result, but I’m not sure if I can arbitrarily pick a 60-40 weight age or something. ",How to fairly weight variables?,921kky,new,1,1,1,0
"Most of the online resources I have consulted have elaborate methods for testing moderation in a linear regression model. My outcome variable is ordinal, has 1 item with 5 categories. The predictor is continuous and the moderator is dichotomous.

Is there a method for testing for moderation using ordinal logistic regression?

Would it be appropriate to treat the outcome variable as continuous. It isn't rare to treat 5 point likert scales as such, but there is only one item, does that hinder the assumption? The sample size is quite large.",Social science question: moderation analysis with ordinal outcome variable,921fmm,new,2,1,1,0
"Curious. As a someone who's majoring in mathematics I've heard people say linear algebra is important in statistics, but why?",Why is Linear Algebra so important in statistics?,91ywlb,new,37,52,52,0
"The question is:

A retailer sells a perishable commodity and each day he places an order for Q units. Each unit that is sold gives a profit of 60 cents and units not sold at the end of the day are discarded at a loss of 40 cents per unit. The demand, D, on any given day is uniformly distributed over [80,140]. How many units should the retailer order to maximise expected profit.

My attempt:

I defined the profit as D - 0.4Q (since the cost price seems to be 0.4 and sale price is 1). Then I tried to find the expectation for this , noting that it is a function of the random variable D - so I integrated (x - 0.4Q)(1/60) dx from 80 to 140. However, the result I get cannot be maximised. Can someone please tell me what I did wrong and how I can answer this question.

Thanks!",[RANDOM VARIABLES] Help with Uniform Distribution question,91yahm,new,10,2,2,0
"I'm a Ph. D. student studying mathematical statistics. I recently read the book [*The Lean PhD* by Julian Kirchherr](https://www.macmillanihe.com/page/detail/The-Lean-PhD/?K=9781352002829) and one of the things he talks about in his book is trying to maximize the impact of your research by, say, writing a newspaper commentary (or asking the university to publish a press release about your research).

I initially wrote this off as not applying to me since I don't think newspapers want a commentary about statistical methods. That said, would anyone like to try and change my mind? Or perhaps propose an equivalent means to maximizing your research's impact (outside academia).",Popularizing Statistics Research,91y4cb,new,4,4,4,0
"Hey all,

I have a graph with a long figure title. and whenever I try to make the graph the title extends outside the figure window. I’ve tried adjusting the text box size, which doesn’t work. Does anyone know how to wrap, or adjust my title so that it goes over two lines? 

Thanks! ",How to make my graph title on two lines in minitab 17?,91xzfq,new,1,2,2,0
"Hello, as  far I have  learnt  cumulative distribution function shows  the probability of a  variabile to  do NOT  surpass a  given threshold.

But  what  are  the  real-life use of cases of  the cumulative distribtuion?  Can you  show me  some examples?

Also  from wikipedia

This   graph shows  the Cumulative distribution function for the normal distribution (gaussian)

[https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Normal\_Distribution\_CDF.svg/720px-Normal\_Distribution\_CDF.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Normal_Distribution_CDF.svg/720px-Normal_Distribution_CDF.svg.png)

  what  do they   rappresent  those lines?

Why  there  is  different  from the yellow, the red, the blue line?",some example of uses of cases cumulative distribution function in real-life data?,91xubo,new,1,1,1,0
"I have two time series datasets and I'm trying to determine if there's a statistical difference between them. I can't post the actual data, but [here's](https://imgur.com/Qp2AqIX) a mockup of the general trends in Google Sheets. There's clearly a difference between the two groups, but because they both decrease over time they both have huge standard deviations and the result comes out non-significant. Is there a particular statistical technique for analyzing a significant difference between two lines? Thanks for any help!",What's the appropriate statistical method for determining significance with this data?,91wla1,new,6,7,7,0
"I was reading the [wikipedia article about the Bradley-Terry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model), and midway through it states that the log-likelihood for this model is (see Wikipedia for definitions of all variables in this question)

$$ L(\\mathbf{p}) = \\sum\_i\^n \\sum\_j\^n w\_{ij} \\ln p\_i - w\_{ij} \\ln(p\_i + p\_j). $$

However, I can't see how to derive this expression. I thought that the likelihood would be

$$ \\prod\_i\^n \\prod\_j\^n \\left(\\frac{p\_i}{p\_i+p\_j}\\right)\^{w\_{ij}} \\left(\\frac{p\_j}{p\_i+p\_j}\\right)\^{N\_{ij}-w\_{ij}}, $$

but taking the log here gives me a different expression to the one in Wikipedia. How can I derive the correct expression?",How can I derive the log-likelihood for the Bradley-Terry model?,91vn6t,new,1,6,6,0
"r/https://docs.google.com/forms/d/e/1FAIpQLSfSw76VdyxdAXlj9dGjMMENnodOEj93W9CWcS8qhzEH1GLHPg/viewform

Please could you fill this 10 question survey for me. I would greatly appreciate it! ",Gender-neutral clothing survey for my dissertation,91vm7v,new,7,0,0,0
,Recommend the book to learn R in 14 days for medical research to a person that already knows programming and basic statistics.,91vg2l,new,4,1,1,0
"I am trying to find out the volume of grease used in cranes at ports/harbors per year. I have the average of grease used by the cranes per year. What should I look for next? 
My thinking was the top 50 ports in the world to gauge how many cranes the largest ports have. Then take the smallest ports in the US to find how many cranes the smaller ports have an then ballpark some number. I really just need to know the market volume for grease in port cranes.

Any kind of help will be appreciated. ",Statistical significant analysis for cranes,91u79m,new,4,2,2,0
"I am  reading  about machine  learning  of R  on a website 

[https://www.dummies.com/wp-content/uploads/421586.image0.jpg](https://www.dummies.com/wp-content/uploads/421586.image0.jpg)

I don't understand  this  part: 

\> How to run the training data 

You’ll need to split the dataset into training and test sets before you  can create an instance of the logistic regression classifier. The  following code will accomplish that task:

\>from sklearn import cross\_validation 

\> X\_train, X\_test, y\_train, y\_test =   cross\_validation.train\_test\_split(iris.data,   iris.target, test\_size=0.10, random\_state=111) 

\> logClassifier.fit(X\_train, y\_train)

>Line 1 imports the library that allows you to split the dataset into two parts.  
   
>  
>Line 2 calls the function from the library that splits the dataset  into two parts and assigns the now-divided datasets to two pairs of  variables.  
   
>  
>Line 3 takes the instance of the logistic regression classifier you just created and calls the fit method to train the model with the training dataset.  
   
>  
>\>>How to visualize the classifier

Looking at the decision surface area on the plot, it looks like some  tuning has to be done. If you look near the middle of the plot, you can  see that many of the data points belonging to the middle area  (Versicolor) are lying in the area to the right side (Virginica).

[https://www.dummies.com/wp-content/uploads/421586.image0.jpg](https://www.dummies.com/wp-content/uploads/421586.image0.jpg)

[https://www.dummies.com/wp-content/uploads/421587.image1.jpg](https://www.dummies.com/wp-content/uploads/421587.image1.jpg)

My  question  is:  WHY    has  he discarded  the  plot  number  1?  Why  wasn't it  good?  He says: ''If you look near the middle of the plot, you can  see that many of the data points belonging to the middle area  (Versicolor)''   .... why is this  a ''probelm''  ?",Machine learning in the R,91tz2n,new,9,7,7,0
[https://stats.stackexchange.com/questions/358923/when-multiple-realizations-of-dependent-random-variables-are-added-they-become-i](https://stats.stackexchange.com/questions/358923/when-multiple-realizations-of-dependent-random-variables-are-added-they-become-i),When multiple realizations of dependent random variables are added they become independent,91s0lq,new,3,0,0,0
"Hi everyone. I'm in the middle of doing my college assignment and I need your help to understand something.

I have two sets of ordinal data. The first data is level of knowledge (poor, average, good) and the other is accuracy of product selection (accurate, not accurate).

I decided to analyze it using Spearman because the data aren't normal and it is a correlative research. And my lecturer asked why did I use Spearman, why didn't you use Somers D? The truth is, I don't really understand about statistic and I thought they are similar so it doesn't matter. I have searched every research journal or statistic book available in local library but I still can't find the answer. The results mostly showed the difference only between Spearman and Gamma. So can anyone help me, what is the difference between Spearman and Somers D? Thank you in advance.",SPSS: Spearman vs Somers D,91qibs,new,3,1,1,0
"Hi,

During work I've come up to a data set, where a normal distribution would be expected (it's a manifacturing process). Usually this normal distribution is well fit to these data set, but I assume that something influenced the messurements and the expected value of the data shifted.

From university I've learned about a parametric test called ' Abbe test' roughly translated from my native language. However I can't find this online anywhere. This test would require a critical value from a table, however the one I have from uni stops at 60 samples, and I have over 30.000. 

The actual value of this test is r=q^2 / s*^2 where s is the deviation &  q^2 =(1/2(n-1))*SUM(xi+1 - x)^2 if this helps.

Anyway, whats the English term for this test, can I find a table for 30.000+ N for it and if not, what other parametric of nonparametric test can I use to determine if the expected value of the data set changed midway measurements/proccess?",[Help]Deviation of Expected Value in a Data Set,91phut,new,1,1,1,0
"Generally speakin, when  to  use this  distribution?Consider the car suspension springs, whose pulsating fatigue life is described bya Weibull law with α = 300000 cycles and β = 2. We calculate:

1. The 10th, 50th and 90th percentiles
2. The percentage of pieces that breaks before 100,0003Yes 200,000 300,000 li 3.The failure rate at 200,000 and at 300,000 cycles
3. If a car mounted these same springs on both axes, at what number of cycles would be the probability of yielding10% for the overall system?

What  is the  (a)  parameter (3000 cicles)  and  B = 2 ?   What  does  this  mean?  What  does it  mean β= 2?  What is  β?

And beside  stress tests,  for what   this  distribution used for?",Weibull distribution,91o8to,new,7,2,2,0
"Howdy,

Does anyone have a suggestion on what type of test could be used to analyze data with 3 independent variables and 3 dependent variables? (I'm a total noob). 

I found this as a possibility...

""Multivariate multiple regression is used when you have two or more dependent variables that are to be predicted from two or more independent variables.  In our example, we will predict **write** and **read** from **female**, **math**, **science** and social studies (**socst**) scores.""",SPSS test for 3 IV and 3 DV?,91nqps,new,3,2,2,0
"In your opinion, 

is it better to have the data like: 

 |  Test Scores | Test Scores
---|---|----
**ID** | pre-intervention | post-intervention
1 | 70 | 86
2 | 72 | 87
3 | 74 | 88

or more like... 

ID | Test scores| Pre/Post intervention
---|---|----
1 | 70 | pre
2 | 72 | pre
3 | 74 | pre
1 | 86 | post
2 | 87 | post
3 | 88 | post

I realize that in simple cases like this one, it doesn't really matter. But in general which would you say is better? 


",A quick question regarding the format of Pre/Post type of data,91n70y,new,4,1,1,0
"Hello,

I'm looking at risk factors (about 15) that influence whether patients show up for their appointments or not. I'm trying to run a binomial logistic regression, but am currently stumped trying to decide which factors to include in my model. Do I just include every independent variable in my regression, or is there an actual method to determine which groups of variables I should analyze at any given time? Thanks in advance!

",Choosing predictors in a multivariate logistic regression,91n1t2,new,10,2,2,0
"Hello,

I am reading about regression and the tutorial mentioned t-values. Is this the same as the t-score from intro to stats?",Regression: t-value the same as t-score?,91kres,new,10,14,14,0
"Suppose a scenario where we have 5 Evolutionary Algorithms whose objective is to reach the global minimum for a function `f(x)` which is 0. Closer the algorithm to the global minimum the better.

All the algorithms are run for 25 times independently and their data is recorded  as follows

    algorithm 1: val1, val2, ..., val25  |  mean 1 = 15
    algorithm 2: val1, val2, ..., val25  |  mean 2 = 0.5
    algorithm 3: val1, val2, ..., val25  |  mean 3 = 0.6
    algorithm 4: val1, val2, ..., val25  |  mean 4 = 5
    algorithm 5: val1, val2, ..., val25  |  mean 5 = 10

We can easily rank algorithm 1, algorithm 4 and algorithm 5 as the gap in the mean is more. But as mean of algorithm 2 and algorithm 3 are very close we can not conclude that algorithm 2 is better just because it's mean is smaller than mean of algorithm 3. So we decided to perform a T-test (Independent-Samples T-Test) on algorithm 2 and 3 data to find whether there is any significant difference in their mean. Results reveal that there is a significant difference in their mean, So we concluded that algorithm 2 is better than algorithm 3.

My question is 

* have used T-test correctly
* does more statistical analysis is required to compare the algorithm or above approach is suffice

Any comments would be of great help, Thank you",How to use the T-test,91jwvj,new,5,1,1,0
"Hi,

I've been trying to learn the what and why of biostatistics but never know how to get started. Right now, I don't want to know the math behind it, I just want to know what the different statistical methods/tests/models are and why do we need them (preferably with examples). (As I like to call it, I want to know the business aspect first and then get onto the math later). Can someone please guide me on this?",Getting started with biostatistics,91jr7f,new,10,8,8,0
"We have images of a material under **three conditions** (control, process A, process B) with \~150 images in each group. For each image, we **count the number of particles** visible in a certain region.\*\* The count ranges from 0 to \*\*6 per image, i.e. the particle count per image is in \[0,1,2,3,4,5,6\]. Each image is independent.

We would like to:

(a) describe the # of particles present, for each condition, in text and graph form (visually)

(b) determine whether process A and/or B affect the # of particles found

A histogram of the control data showed that counts of 1 and 2 particles are most frequent, and the calculated mean is \~2. It is not symmetrical (obviously, since there can't be fewer than 0 particles, and despite 1 and 2 being similarly frequent, the mean is \~2).

Would appreciate any advice on how to proceed from here? **Which statistical test** can I (should I even?) use to determine whether there are **differences between the conditions**? Is there an **ideal way to plot** such data?

Extra: Box plots seem weird to me for quantized data, at least when the range is so narrow: with a median of 2, the 25% can only be 0 or 1, for example, since the 25% and 75% percentiles by definition (I guess) have to be values in the sample, and are thus also also quantized. Thus, I don't think the standard box plot has the ability to visually distinguish between distributions that may be different, but roughly similar. Is there a better approach to plotting these data?

Thank you for your help.",Compare frequency of counts (quantized data): what's an appropriate test?,91jgx4,new,3,2,2,0
"I've noticed this trend within the last few years of people recommending *Introduction to Statistical Learning* (ISL) and *Elements of Statistical Learning* (ESL) in lieu of more ""traditional"" textbooks, i.e., Wackerly et al. and Casella & Berger. I took a grad-level statistical learning course covering both ISL and ESL last semester, and I honestly wouldn't have been able to understand the details of these books without some understanding of linear models and probability with matrices.

Perhaps I'm out of the loop: are courses on ISL and ESL replacing more traditional calculus-based probability/statistics courses? Call me a traditionalist, but I personally would not be comfortable telling someone who hasn't seen a *t*-test, or even probability and matrix algebra to start on ISL and learn everything from there.","Are courses covering Introduction to Statistical Learning and Elements of Statistical Learning now replacing ""traditional"" intro stats courses?",91idzm,new,30,17,17,0
"I'm a current Stats major, and am aspiring to be a data scientist in the future. 

Looking at articles from around the web, people are always saying that comp sci is the clear complement to Stats in terms of a data science background. However, there is also another school of thought that believes that data science, or in this case I'll narrow the scope to consumers' behaviour, can be paired very nicely with social sciences such as sociology / psychology, to understand the actions of consumers or the society on a different and less scientific level. 

What do you guys think? Is there any merit in such a view?

I'm asking because I might want to declare a minor in social sciences, and I feel like this could be an added bonus in terms of helping with my data science aspirations.

Thanks, and I look forward to everyone's replies.",What other kinds of majors complement Statistics for aspiring data scientists?,91gy5r,new,36,12,12,0
"Hello /r/statistics! I was wondering if this was possible (I'm sure it is) - let's say we have two teams Team A and Team B. We've calculated the expected goals that Team A will score to be 2.8 and Team B we expect to score 4.4


So obviously we are giving Team B a higher probability of winning but is there anything we can do to put percentages on these events? A win for Team A, a win for Team B, and even a draw where both teams score the same amount of goals?


I'm not (necessarily) looking for an answer on a plate but just a little shove in the right direction would be much appreciated.


Thanks for your help!


",Converting expected goals into win probabilities (sports),91fpb7,new,1,1,1,0
"I am a rising senior who will be applying to graduate schools this upcoming fall. I am interested in machine learning, and data science. Since I majored in pure math, I have not really been able to touch on applied topics in math or stats as much as I would have liked. I have taken two semesters of basic calculus based stats that the engineers have to take but that is about it. The other most applied course I took was two semesters of linear algebra. Most of my classes have been very pure and proof heavy, courses like real analysis, modern algebra, and topology.

I noticed that stat departments continue to pop up while looking for schools that have machine learning and big data as a research interest. I am thinking of applying to some of these schools but am worried my lack of upper level stat courses will dissuade the application committees for these schools. Do stat phd programs regularly take in math majors, and how difficult will it be to remedy my weak stat background?  ",How is the transition from pure math to stats?,91f4wm,new,17,4,4,0
"Logistic regression literature ASSUMES the odds of an event has a log linear relationship to the independent variables of an observation. Then it will try to minimize the log likelihood among the training dataset and finally got the parameters which are the model result. Then it uses this model result to predict the probability of new observations. 

Question: how can we interpret the predicted value (between 0 and 1) of new observation as it's real probablity to be an event? 

What bothers me most is that the ASSUMPTION has never be validated, i.e. it's just an assumption, we couldn't absurdly use the predicted value (between 0 and 1) as real probability.

Please let me know your thoughts.",How to interpret the predicted probability of a logistic regression?,91e4tb,new,12,1,1,0
"I think this may be a rudimentary question, but would like some confirmation that my thought process is correct in this( Just learning stats and I don't have the best math background, so forgive me for what may be a simple question).

For my project, I have data where only the dependent variable has been logged transformed and used in a multi-level model in order to control for random effects. Very similar to this example: [https://stats.stackexchange.com/questions/302448/back-transform-mixed-effects-models-regression-coefficients-for-fixed-effects-f](https://stats.stackexchange.com/questions/302448/back-transform-mixed-effects-models-regression-coefficients-for-fixed-effects-f)

When looking at the data and organizing the results, I'd like to have the intercept back to it's original scale, so like the user above, would use the exponentiation function. Is the correct way of calculating the coefficients back to scale the same as the advice the user received: where if the intercept =  6.533079 and the beta = 0.152238, then

    exp(intercept + coefficient) or --> (exp(6.533079+0.152238)) - exp(6.533079) = ~113

If so, I shouldn't worry about the inclusion of random effects somehow influencing this back transformation?

Is this also the same way to go about back transforming data that had been transformed with the square root. Except, I'd obviously be squaring the intercept + coefficient and adding them together, and then subtracting from the intercept?

Thanks all!",Transformation of DV for linear regression,91d3f6,new,3,1,1,0
"Hi All,

I'm having trouble quantifying/modeling this situation and I'm hoping there's a good statistical technique or model to help understand this somewhat poorly-defined question.

I have students that play this online game to learn/practice different types of math curriculum topics (think fractions, percentages, algebra, etc.) They play this game and if they choose an incorrect answer enough, they'll fail a level and have to restart the level. When they beat that topic's handful of levels, they can progress to the next piece of math curriculum and begin the process on the next topic. Frequently, students will complete topics with more level attempts than levels successfully completed.

I want to be able to better understand what causes students to fail levels. Specifically, do students fail levels more because they haven't learned that topic well, or that they struggle on all content (i.e. they should be moved down/held back a grade).

I have a large dataset of each student's level attempt on each topic.",Quantifying/Modeling Students struggling on different content,91ciba,new,0,4,4,0
"I remember seeing this scatterplot which showed some kind of risk score, which was a continuous variable like a hypertension measurement or something, and what would happen if only people with a high risk were included in a follow-up. I can't remember more details about the picture or where I saw it. Does anyone here remember seeing something like that recently and have the source?",Picture illustrating why it's a bad idea to use cutoffs of continuous risk scores to pick subsample for follow-up?,91bp10,new,2,5,5,0
"I'd like to learn as much as I can about statistics and statistical theory. I took three stats classes in college and therefore am familiar with confidence intervals, hypothesis testing, ANOVA, etc and **my problem is I can't find good resources online for what I'd consider the next level of statistics**. Everything starts with the beginner stuff and doesn't go deeper, which is what I'd like to learn more about. 

I know R, but would prefer to learn from books, courses, lectures, etc and then play around in R instead of learning by just coding in R. 

Thank you! ",Where can I learn non-beginner statistics?,91akl5,new,22,30,30,0
My school only offers BBA but I’ve been told a BBA doesn’t prepare for grad school and is useless if not from a famous school ,"What is the difference between a BA, BS, and BBA in Statistics?",91908b,new,2,2,2,0
"Player A plays a single game against Player B.

The probability that player A will win a single point is *x*

, and thus 1−*x*

is the probability that Player B will win a point.

The scoring system in tennis goes 15, 30, 40, then game.  However a  score of 40-40 is known as deuce and the winner of the next point gains  an ""advantage"".  If this player wins again, he then wins the game, but  if he loses the score returns to 40-40, or deuce.

So given the probability player A will win a single point is *x*

, what is the probabilty that player A will win the entire game?

Here is my attempt, I would appreciate feedback:

Let *p*

be the probability that player A wins a single point.

(1) Player A can win after 40-0, with probability *p\^*4

(2)  Player A can win after 40-15 with probability (4 /1) \* *p\^*4 \* (1*−*p)

(3)  Player A can win after 40-30 with probability (5 /2) \* *p\^*4 \* (1*−*p)\^2

(4) Player A can reach deuce, with probability (6 /3) \* *p\^*3 \* (1*−*p)\^3

, and once deuce is reached, player A will win with a probability of *p\^*2  /  1−2*p \** (1−*p )*

Therefore the probability player A will win the game is given by,

*P*(*Win*)=*p\^*4+(4 / 1) \* *p\^*4 \* (1*−*p)+(52) *\* *p\^4\*(*1*−p)\^2+(6 /3*)* \* *p*5(1−p)\^3* / *1−2*p *\* (1−p)

My question is: this  is the  formula of a dicotomy variable (result of winning  the game can be yes  or  no).

The probabilties  for each player to win  the  game are not  the same (there  will  always  be a player  who has major chances  against  onether,  because  it's more talent)

If I would  calculate set  my personal  probability   for  the  player A  to win the game, and then substitute it in the ''p'' of  that  equation ( *P*(*Win*)=*p\^*4+(4 / 1) \* *p\^*4 \* (1*−*p)+(52) *\* *p\^4\*(*1*−p)\^2+(6 /3*)* \* *p*5(1−p)\^3* / *1−2*p \* (1−p))   and  I will get the  AVERAGE  probability  that  A  has to  win a game ag*ainst B

But how  can I  get this  (subjective) probability?  

I  was thinking to  make a   regression model on that A tennis  player, based on few  paramters such as  % Aces, % double faults and as independent variable    and  as  dependent   V.  the games  won,  but  I don't  know  how  to trasform a game won  into ''chance''  /  %   of  winning the game and put into  X axsiss of the linear regression model. 

 Game  won =   1

Game lost  =      0

?",Question about statistic and tennis,917v1c,new,3,0,0,0
"Player A has a 95% winrate edit: Not vs B, overall

Player B has a 50% winrate

There can be no draws

What is the chance of Player A winning when facing B?

I think the part thats confusing me is that these are concurrent yet dependent events?

edit: the winrates are lets say career winrates established vs the same pool of opponents, and these players have not faced each other. My question is also is it possible to get any meaningful probability of this event from the data we have.",Simple question my brain refuses to understand,917qyq,new,34,9,9,0
"Machine learning is one of the most searched keyword on any search engine at this point of time. The reason is quite clear; the benefits of utilising it in any industry is beyond imagination. Machine learning is making computers learn from data to find patterns & generate business insights. In e-commerce, machine learning is even far more relevant because of digitally generated user-specific data points. Daily, we read so much about big companies using machine learning in their business decisions. With the technological advancement, machine learning is very much accessible for any small to medium enterprise. However, still thousands of companies are not capitalising the value generated from machine learning. We will briefly discuss most useful cases of machine learning in e-commerce.

>*With the technological advancement, machine learning is very much accessible for any small to medium enterprise*

* User churn prediction: By using customer transactional historic data and other behavioural traits, user churn probability can be predicted. Engaging a customer at right time can help reduce the churn if we know specific customers are about to churn, machine learning plays a pivotal role.
* [Recommendation engine](https://www.datatobiz.com/2017/09/15/product-recommenders/): Up-selling & cross-selling based on machine learning basket analytics can boost revenue. Everyone know about amazon product recommendations. It has been surfaced in one of the report that 27% of Amazon revenue comes from recommendations only. The power of recommender engine can be estimated from these numbers itself.
* [Customer Life Time value v/s Customer acquisition cost](https://www.datatobiz.com/2017/10/29/machine-learning-trasactional-analytics/) : Understanding customer LTV can be very crucial for any business. Using RFM (recency, frequency & monetary), machine learning can figure out the customer LTV to make strategic decisions on acquisition channels & cost of acquisitions.
* Customer segmentation: With statistical segmentations, users can be defined in the specific type of users to better understand of your customer base. Which type of users are more profitable, who buys more stuff. These types of answers will create a solid foundation for strategic business decisions.
* Marketing Campaign optimisation: Every marketing campaign has its cost. To better manage marketing budget, one need to analyse which campaign doing well and why. Machine learning can work quite well in figuring this out.
* Spatial analytics: Matching demand supply spatially & timely can be very productive in any business. Using machine learning, demand & supply can be predicted to take business actions to reduce this gap.
* Product inventory optimisation: Another use case of machine learning is inventory management, with the demand prediction, a business can be lean enough to reduce storage & waiting for costs for various products.

The above mentioned key areas where any e-commerce firm can make better business decisions using machine learning. In addition, fraud detection, customer service, voice analytics, web page & content selection analytics, image recognition and lot more can make managers better at business decisions.

This blog has been originally published at [DataToBiz](https://datatobiz.com/) official blog.",7 Ways to use machine learning in E-commerce​,9167t9,new,3,1,1,0
"I'm working on a question, and I'm pulling out my hair because it doesn't seem to matter what I do, I cannot seem to input the values into my calculator correctly. This is the question data:

n1: 51    n2: 46

X-bar1: 3.6    x-bar2: 2.8

S1: 0.75    S2: 1.2

Formula 1; tdf: (xbar1-xbar2)/

SQRT of (S1\^2/n1 + S2\^2/n2)

= 3.6-2.8/SQRT of (0.75\^2/51 + 1.2\^2/46) = 3.8892

This is where I'm having problems:

Formula 2; df: (S1\^2/n1 + S2\^2/n2)\^2/

(S1\^2/n1)\^2/(n1-1)+(S2\^2/n2)\^2/(n2-1)

(0.75\^2/51 + 1.2\^2/46)\^2 = 0.0018

(0.75\^2/51)\^2/(51-1) + (1.2\^2/46)\^2/\*46-1) = ... 0

It doesn't matter how I punch these numbers into my calculator (BAII, which I am required to use in my course), it always returns a result of 0. Punching the formula into my TI-84, exactly the same way returns a result of 4.6105. 

I don't know what is happening. Please help :(",Hypothesis Testing Involving 2 Means - Help,912qy1,new,2,0,0,0
"I don't understand this point: 

In [probability theory](https://en.wikipedia.org/wiki/Probability_theory) and [statistics](https://en.wikipedia.org/wiki/Statistics), given a [stochastic process](https://en.wikipedia.org/wiki/Stochastic_process)X = (  X  t   ) r/https://wikimedia.org/api/rest_v1/media/math/render/svg/5647138a0187cb5f5116023e365f6c6638ea4775 , the **autocovariance** is a function that gives the [covariance](https://en.wikipedia.org/wiki/Covariance) of the process with itself at pairs of time points. With the usual notation *E*  for the [expectation](https://en.wikipedia.org/wiki/Expected_value) operator, if the process has the [mean](https://en.wikipedia.org/wiki/Mean) function      μ  t   = E \[  X  t   \] [https://wikimedia.org/api/rest\_v1/media/math/render/svg/9b9a2b5c7b15a367b656f56d2f32d6ab9bfc1dda](https://wikimedia.org/api/rest_v1/media/math/render/svg/9b9a2b5c7b15a367b656f56d2f32d6ab9bfc1dda)  📷, then the autocovariance is given by 

[https://wikimedia.org/api/rest\_v1/media/math/render/svg/8b3f23aea5d9cad0778c0592fad4baaa23ad6cbe](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b3f23aea5d9cad0778c0592fad4baaa23ad6cbe)

where *t* and *s* are two time periods or moments in time. 

Is  it  the  autocovariance  the measurament  of  HOW MUCH  does  an independent variable  change itself from its  mean  in a given time period?  So  why  wikipedia  took two time measuremants?  (t and  s)? 

Wikipedia  said  ''where *t* and *s* are two time  periods or moments in time. ''  What  does this  mean?  Any  real life example  of  a stochastic  process, and  how can I build an oscillator  in R or  Excell, to determine  how much is  the  variable from its  avearage?  Thank you",Autocovariance Stochastic process,912j8s,new,5,2,2,0
"I can't put my head around the following concepts:
1. Bayesian inference,
2. Likelihood 
3. Maximum likelihood ",How are Bayesian inference and likelihood related?,911xzq,new,14,1,1,0
"**General question**: in a population of *n*, there is a *p* chance of a certain condition being true. I sample *s* at random. What are the chances that the majority of my sample represents the majority of the population as a whole?

**Specific example**: a country has 1,000,000 people. 600,000 of them prefer Whipplescrumptious Fudgemallow Delight, and 400,000 prefer Nutty Crunch Surprise. I call 5 random people from the phone book and ask them their favourite Wonka bar. Most of the time, the majority of my five people (three or four or five of them), will prefer Whipplescrumptious Fudgemallow Delight. In this example, *n* = 1,000,000, *p* = 0.6, *s* = 5.

I have made some progress on this, but am looking for a general formula for any values of *n*, *p*, and *s*.

In the example, I was able to calculate there is a 68.26% chance my sample will reflect the majority. We can get this by adding:

    (s^0) * (p^(s-0) )  * (  (1-p)^0) )    #this is the chance 5 people in my sample will prefer Whipplescrumptious Fudgemallow Delight. This reduces to just p^s

    +  (s^1)  * ( p^(*s-1)) * (  (1-p)^1 ) #this is the chance 4 will

    + (s^2) * ( p^(s-2) ) * (  (1-p)^2 ) #this is the chance 3 will

This version stops after 3 lines because 3 constitutes a majority of 5. I just need to generalise this to any *s*. ",Probabilities in random sampling,91184c,new,6,11,11,0
"Hi,  after a  linear / non linear regression is it  necessary  to make  an analisy of residuals to know  if the model were  accurate? ù

Noob here,  could you explaine  me  please,  why  you could  do an analisy of residuals  and  for  which purpose  does it serve?",analisy of residual,90z08o,new,3,0,0,0
"Main effect of subgroup 1: There was a reduction in rates of post-cesarean endometritis (=outcome) for women undergoing a cesarean (=participants) after being in labor (=subgroup 1) who received vaginal preparation (=intervention), from 11.1% in the control group to 4.7% in the vaginal preparation group (RR 0.41, 95% CI 0.19 to 0.89). No main effect in subgroup 2: There was not a clear difference in  post-cesarean endometritis for women who were not in labor (subgroup 2) (RR 1.00, 95% CI 0.35 to 2.84).   
However, there were no clear differences between these two subgroups as indicated by the subgroup interaction test (Test for subgroup differences: Chi² = 1.80, df = 1 (P = 0.18), I² = 44.3%).  
How do I interpret these results?","Main effects in subgroup analyses: one is significant, other is not. BUT interaction effect is NOT significant. How do I interpret this?",90y2ot,new,3,7,7,0
"I'm trying to figure out what to do if I have access to a variable on which I can condition for one case but not the other. Here is the problem setup:

A test comes back telling me my cat has either an anxiety disorder, or an allergy. (Let's say the test has no false positive) Now, I'd like to estimate the odds of the two disorders. I know P(allergy) for cats in general. I know P(anxiety) for cats in general. So at first glance, I could just say P(anxiety|positive) = P(anxiety)/(P(allergy)+P(anxiety)).

But now, let's say I also know P(anxiety|female) and I know it's much lower than P(anxiety). It seems to me I can't just say:

P(anxiety|positive and female) = P(anxiety|female) / (P(anxiety|female) + P(allergy)).

Or can I do that?",How to combine rate for a subgroup with rate for a larger group?,90x3g2,new,2,1,1,0
,"Working professional here, should I pursue certification: Statistics with R specialization ?",90vmmi,new,4,4,4,0
"## 

## Reading  in Mintab website, I found somenthing I don't understand:

## 

## Example of the distribution of weights

The continuous normal distribution can describe the  distribution of weight of adult males. For example, you can calculate  the probability that a man weighs between 160 and 170 pounds.

[https://support.minitab.com/en-us/minitab-express/1/distribution\_plot\_normal\_weight\_shade\_middle.xml\_Graph\_cmd1o1.png](https://support.minitab.com/en-us/minitab-express/1/distribution_plot_normal_weight_shade_middle.xml_Graph_cmd1o1.png)

## Distribution plot of the weight of adult males

The  shaded region under the curve in this example represents the range from  160 and 170 pounds. The area of this range is 0.136; therefore, the  probability that a randomly selected man weighs between 160 and 170  pounds is 13.6%. The entire area under the curve equals 1.0.

QUESTION:  How has he calculated that  the area  of  the   (red) range is 1?  How  has  he  calculated the probability of  0.136/%???",Distribution plot of the weight of adult males,90v076,new,1,3,3,0
"Not sure if this is the right subreddit sorry if it is not.

So i am on a site that has a clan ranking system. We unfortunately lost the formula when a member quit and all we have now is old rankings(a few years worth updated about twice a month) and the data that was put into them.  Is it possible to use this to figure out the formula? If so how?

Any help is greatly appreciated.

Again sorry if this was posted in the wrong subreddit. ",HELP: Complex ranking system.,90uqxg,new,3,4,4,0
,What jobs can you get as a statistics major?,90un1l,new,5,7,7,0
"I'm trying to model the outbreak of a disease. I have data that tells me who got infected and when, and a bunch of interesting covariates (sex, household, etc).

The kind of model I'm fitting is a [Reed-Frost epidemic model](https://en.wikipedia.org/wiki/Reed%E2%80%93Frost_model). The infection can be modeled at different levels -- the simplest is where you have a single parameter q describing the probability of transmission per day, for everyone in the entire community. Or you can have multiple parameters q_1, q_2, etc, for probability of transmission within different (possibly overlapping) subgroups (e.g. households, workplaces, etc).

One of the questions I'm trying to answer is how granular do we need to be to adequately describe the propagation of the disease? Do we need to account for every single subgroup, or will just a few suffice?

I read this article about [Hierarchical Bayesian Modelling](http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/) and it seemed to be relevant to what I'm trying to do. According to this, it seems as though I can postulate all my q's as coming from a common group distribution with some hyperparameters, which would let me evaluate the tradeoff between coarse and fine-grained modeling.

Is this correct? This is the first time I've done anything serious with Bayesian modeling and it's quite overwhelming.",Is a Bayesian Hierarchical Model appropriate for this kind of analysis?,90tm0z,new,7,10,10,0
"This is a study on the prevalence of various types of anxiety disorders in the population. In the article, Figure 2 has ""Cumulative age of onset distribution"" on the Y axis. I am unable to comprehend what the Y axis means,  and was wondering if someone could explain it to me. 

Article: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3018839/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3018839/)

If this is the wrong place to post this, can someone guide me to the correct thread. 

Thanks!",Cumulative age of onset distribution - Need explanation.,90sph4,new,2,4,4,0
" [https://imgur.com/a/TKr7DyA](https://imgur.com/a/TKr7DyA)

Question (part C) and solutions are in above imgur link:

More specifically, I cannot understand why the integral was crafted as such, with limits for x from 1 to x and for y from 0 to 1. Can someone please explain how these limits (and hence the integral) were chosen and perhaps give me some insight as to how to craft the integrals for different probabilities.",[RANDOM VARIABLES & PROBABILITY] Need help with understanding Joint PDF solution,90sf03,new,1,2,2,0
"Well I've been job-hunting for almost a year, and still unable to gain a foothold in anything resembling a statistics career - data analyst, business analyst, statistician, data scientist, etc. I am over halfway finished towards my MS in statistics and fearful that I'm wasting my time on this degree. If I have no experience, who will hire me for what I'm really worth? I already went through that with my bachelors degree, and have been trying not to repeat it, but nothing I do is working. I'm not able to take an internship, as I would lose my health insurance (hooray USA). I live near NYC which is a major job hub, but have been rejected twice from positions in NYC (I'm not a Brooklyn-based hipster and don't have that aesthetic that they want).",Who hires MS grads that don't have experience?,90s6yi,new,33,27,27,0
"I am currently working on a paper that maeasures the impact of a scholarship program from a private school vs students who do not get the scholarship and have to graduate elsewhere. 

My Hypothesis is that there is a rather strong relation between getting the scholarship and graduating. 

What kind of association tests can I perform if my data is qualitative? Variables are 

STATUS: 1-got scholarship; 2-no scholarship
GRADUATED: 1-yes; 2-no
ENROLLED IN COLLEGE: 1-yes; 2-no

PS. Edit: I am not trying, by any means, to get homework help. I just need clarification on which test is more appropriate so I can run things myself.
",HELP: Association test for qualitative data,90rz79,new,1,1,1,0
"I work in biotech, and thus frequently confront high-dimensional (little n large p) problems in my work.

My goto tool is generally elastic net logistic regression.  I've continued to interpret the output of these models as probabilities in my analyses, however I'm starting to wonder if that's really appropriate.

In high-dimensional binary classification problems you always have perfect linear separation, which means that without regularization the coefficients of your linear model shoot off to ±∞.  This means that the coefficients I obtain when I regularize are shrunk/offset by an infinite amount.

This, in practice (not sure about theoretically), completely invalidates what I think is the core component of logistic regression which allows you to interpret its output as probabilities (besides them being contained in the interval [0,1]), which is that the expected probability of a sample belonging to the non-dummy class with output 0 ≤ p ≤ 1 is p.",Interpreting the probabilities in regularized logistic regression,90mhza,new,15,14,14,0
"Is it true more often than not that:  

the ith highest number in a distribution divided by the mean of the ith highest numbers < the jth highest number in the same distribution divided by the mean of the jth highest numbers, where i > j.  

Is it always true for normal distributions?  ","What can be said about the statement: the ith highest number in a distribution divided by the mean of the ith highest numbers < the jth highest number in the same distribution divided by the mean of the jth highest numbers, where i > j?",90lfj5,new,14,0,0,0
"I'm presenting a report at work, I need a place to input all my findings and create bell curves/line  graphs etc. Not too fancy,  just simple and effective, preferable free or extremely cheap  ",Any good/free programs/online resources to create a presentation?,90l4as,new,3,1,1,0
,What are some of the most unique areas or ideas in statistics?,90kkw3,new,17,1,1,0
"What do you do when you have hit the jackpot and all 20 measured statistics are significant with a p<0.05?

Do you still divide by 20 and ignore all 20 because they're not significant at the corrected significance level?",Bonferroni correction always justified?,90khpt,new,32,11,11,0
"Common types of sampling explained in the article :
https://dataschool.com/intro-to-sampling/

Probability sampling -> i. Simple Random ii. Stratified iii. Cluster iv. Systematic v. Mixed

Non-probability sampling -> i. Convenience ii. Quota iii. Snowball",Commonly used sampling techniques,90joo4,new,0,0,0,0
"I'm trying to run multiple imputations to fill in some missing values in a dataset. If I ask SPSS to analyze patterns, it shows that there are missing values and gives me the correct information. When I ask it to run imputations, the command 'scan data' in the 'constraints' menu also shows the same proportions of missing data. However, when I actually tell SPSS to run the imputations, I get this error message in my output:

 

'There are no missing values to impute the requested variables'.

 

I've played around with giving it min and max value constraints and this make no difference. Ive also tried setting it to ‘impute and predict’ and ‘impute only’ with the same outcomes. I've got the imputation method set to 'automatic'.

 

Does anyone know how to resolve this?",SPSS Multiple Imputation error message,90j54n,new,0,1,1,0
"I'm documenting my code and came across this line of code that I don't really understand and can't replicate in any fashion (found this off Stackoverflow probably lol):


i <- sapply(df, is.factor)

df[i] <- lapply(df[i], as.character)


Can someone please decipher these two lines of code and breakdown what exactly i, sapply, and lapply are doing?",R code help,90ih18,new,11,1,1,0
"I've had trouble with over fitting while doing MLR so I coded my own function. I thought I'd share it: https://textsave.de/text/sIiWwYl2C1E8emBs

**Input**: ""x"" is a matrix containing the predictors, ""y"" is a vector containing the observed values, and ""scales"" is a Boolean variable. If ""scales"" is set as ""TRUE"", 4 different models will be evaluated: the original one, a logged model where values are between 0 and ln(2), an exponential model where values are between 1 and exp{10}, a normal model where the variables have been normalised via a Yeo-Johnson transformation, and a combination of all 4 previous models. 

**Output**: 

vector: the vector ""v"" such that x %*% v ~ y

matrix.short: the reduced matrix ""x"" such that predictors that weren't considered are cleaned off

vector.short: analogous the matrix.short, but with ""v""

The rest is self-explanatory, i.e.: ssr, x.fitted, ect.

**Methodology**:

Recursively chooses the best vector so that adding a predictor minimizes the SSR. The Pseudo-Inverse is calculated via block matrix inversion / Sherman-Morrison formula in order to speed up the process. The process stops when the Adjusted R^2 no longer increases.

",Multiple Linear Regression R code with optimal Adjusted R^2,90ig3z,new,2,2,2,0
"Edited to include scatterplot


Up front: I can give more details if needed, but I think my question is pretty basic: did I accidentally skew my independent variables so bad that I can't use them?

I did a study where I dove along a river collecting mussels. Before we even look at the mussels in the study, I want to make sure I'm using the independent factors correctly. I recorded the depth I dove, and also what type of bottom there was (rock, sand...all standardized into a continuous scale). All of this was done to see if depth, bottom type, and river mile (distance along the river) had any impact on mussels.

What I found is that I unintentionally dove deeper at downstream sites than upstream. It looks like [this](https://mokacytle.tumblr.com/image/176095063762)

This is strange as I did not move in one direction (I dove upstream some weeks, bounced downstream, back to the middle...it was based on logistics). The regression shows an R2 of 0.106, and the analysis of variance shows a significant p value. 

So my question is: am I unable to analyze the dependent mussel data (size, weight, %adults, etc) with depth and river mile as separate independent variables?

P.S.  an ANOVA examining the effects of depth and river mile on bottom type resulted in a significant interaction.",How bad did I screw up while collecting data and can I fix it? (x-post from r/askstatistics),90ha0m,new,23,5,5,0
"At my office we’re having a lottery each week where you buy as many tickets as you want, then they get shuffled and one is pulled randomly from a hat. That person then gets to pull one card, if it’s the queen of hearts you win the pot. So far we’re down to 8 cards left with no winner. It got me wondering at what point on average statistically would you pull that one card from the deck? What would be the formula to determine that?",Average number of cards needed to pull one specific card from a deck,90h2x7,new,2,0,0,0
I have a sample set of 10 values. All the data values are roughly similar (~1.4) except 1 value which is 0.33. I want to statistically determine if it is an outlier and a data value that I can remove. I know the IQ range fence method but I'm wondering if there is anything else more comprehensive to determine a single outlier. ,How to test for an outlier?,90fzdo,new,8,4,4,0
"Very Helpful [R codes](http://studywalk.com/index.php?route=product/category&path=109_149), all at one place by [studywalk](http://studywalk.com)",R codes for Statistics and Data Analytics,90eedj,new,6,48,48,0
"Hello, I hope this isnt an inappropriate question for the sub. And apologies that im probably not using the correct terminology.

Im tracking my mood, sleep, exercise, and eating with simple yes/no answers: Was today a good day? Did I sleep well? Did I work out? Did I eat well?Im phrasing them as binary yes/no rather than a scale to simplify and reduce cognitive load in answering.

Im wondering if theres a good way to graph the history. What I've been doing is taking yes as +1 and no as  -1 to a running total, so over a month i can see the rise and fall, and somewhat the relationships (sleep does indeed seem to affect my mood, but exercise doesnt). This seems intuitive to me, but it has its issues, and I image there is a better way.

Can anyone help me with a method that may give a little more useful results?",What is the best way to visualize and compare yes/no answers over time? (mood tracking),90d19x,new,10,11,11,0
I have a sample size of 120 from a normally distributed population (average weight of college students).  How do I show that the normality assumption is met for my sample?,Help determining normality assumption for a sample,90bom7,new,4,1,1,0
"All Applied Statistics programs say they emphasize application, but it's hard to tell what that means exactly from the description of any given program. Some are known to teach you how to plug and chug in R, while others are known to overlap significantly with traditional programs in statistics. Most of them have the same courses listed on the curriculum, regardless of if the degree is an MS, MA, MPS, PSM, or plain old M. And then there are other degree offerings like Statistical Science and Statistical Practice that make things even more confusing.

So here's the question: how can you tell if a statistics related masters program gives a solid treatment of statistics? How do you spot the diploma mills? My first thought is to look at the prereqs - some require calc I-III and linear algebra, while others only require a semester of calculus. What else?","How does one determine in a masters in stats/applied stats/biostats/etc is ""rigorous""?",90bkte,new,7,3,3,0
"I'm getting close to complete my MS in Stats, I was wondering, with the current job market in the private sector, if it would be more valuable get a second master in computer science or go all the way and complete a PhD in stats.
For the PhD, I really want to do it, more for myself to prove I can do it, and the satisfaction of exploring a field of interest to the nitty gritty details.
For the dual MS, I feel that CS is very sought after in the private sector, but my passion in it is more about being able to quickly develop tools that are not available through the usual repositories in r and python than truly optimize and develop full fledged software.

Opinions?","Career question,Stats Phd or dual Master Stats / CS",90b6iy,new,4,3,3,0
"So, I've been working with a lot of count data and while I originally had planned on using a Poisson distribution, overdispersion led to me a negative binomial model. I was just reading that negative binomial models shouldn't be used with a small sample size though. My dataset only has 18-20 replicates. How do I know if my sample size is too small? If it is, what type of model should I use for overdispersed count data?","How do you know if your sample size is too small for a negative binomial regression? If it is, what should you use instead?",90apz3,new,2,3,3,0
"Hello I'm currently on student placement but I'm having some trouble figuring out what statistical model I should be using? I am measuring a quality of life scale by comparing age groups, to determine which age group has the better of quality of life.

The dependent variable is the quality of life scale (a number will be developed from a Likert scale. For example, 10 would indicate low quality of life whereas 60 would indicate high).

Independent variable is age ( 5 or 6 different age groups)

What test should i conduct to determine which age group has the better quality of life, T tests or ANOVA?

I would appreciate any help.",What statistical model should i be using?,90a96c,new,3,1,1,0
"Hello everyone,

After months of data collection and background writing, I am finally tackling my statistical analysis and... p is greater than 0.05. Everything in my lit says these measurements should be correlated, and I know I took them right, but they just don't. This is measurement data and I have checked the variables with a Shapiro-Wilks for normality, which they passed. They are all within the acceptable range for skewness. Is there anything else I could be missing, any other tests I should run, or am I truly unable to reject the Null? Help!",Thesis help!,90a8a7,new,11,9,9,0
"Edit: Let me rephrase my question. Why does increasing the significance level make it more likely to reject the null hypothesis? Doesn't the confidence interval increase thus making the range of plausible values that support the null hypothesis greater?

EDIT: I'm dumb, increasing SL reduces the range. damnit. sorry guys

Edit: If you think about it the name is kinda counter-intuitive. 

The OpenIntro Statistics textbook has an example trying to explain when to change significance levels. The situation is there is a car manufacturer and they are thinking about switching suppliers because they might be safer. So the book says we should change the confidence interval because even if the increased safety is only moderately strong the company should still want to do the switch, makes sense. I must be confused on something basic because I thought this meant we would then lower the significance level, in other words making the confidence interval smaller. However the book says we should change the confidence interval from .05 to .10. Doesn't this make it harder to make the switch? I.e the parts need to be safer/p value needs to be greater? Null hypothesis is suppliers parts are equally reliable.",Significance Level Question,909sbr,new,3,3,3,0
Does it mean there's a direct relationship between both of them? That is likely? or it doesn't have any meaning and you can believe what you want based on what you want to believe?,"what are we laymen supposed to think when someone says ""X is significantly associated with Y""?",906k2f,new,10,3,3,0
"A game of Russian Roulette with a 6 round chamber (1 of which is loaded), and 6 players.  The wheel is spun so it is random at the beginning of the game, but prior each player is given the option to pick which order they want to pull the trigger.  The wheel is not spun again between trigger pulls.  You're given the option of which order you want to go in (First, second, ext).  Which gives you the best odds for survival?","Russian Roulette with 6 players, but you keep pulling the trigger without spinning the wheel between trigger pulls. Which order do you want to go in order to maximize odds of survival?",906ikl,new,21,3,3,0
"Hi all,

I work for a professional services company where fee earners post their time in a system and at the end of every month an invoice is issued to the client to pay.

I would like to calculate the probability of a time entry is written off (yes or no - the discrete/binary events) based on how many days late the time entry is entered into the system after the work has been carried out (the continuous variable).

Is this something logical to calculate/analyze?

If so, what technique / method should I use?

  
I am a n00b in stats but would like to start learning so please guide me as required.

Thanks",Probability of discrete/binary events based on continuous variable?,90645l,new,11,2,2,0
"Okay I feel this may be really simple and right in-front of my nose but I have a problem I'm trying to work out and any help would be appreciated (not a homework problem!), I work for a mid-sized food company and this needs a bit of background - basically I'm trying to work out optimal fill weights for a product and all fill weights naturally vary by a certain amount. Legislation states that for a declared 900g product each individual item is allowed to be as low as 885g as long as the average for each batch is over 900g. Which for our population (batch) sized 10,000 products each weighed individually it is, its around 915g average actually, but a certain amount are filled below 900g. If independent auditors were to come assess our products they don't actually look at the overall population they do their own independent sampling. For example for a 10,000 product batch they might take 100 items at random and average those. What I'd like to know is every time a sample size n=100 is taken what is the chance the average will be below 900g.

I have an idea but just wanted to check with you guys to see if I'm on the right path. Hypothetically, assuming the population is Normal, mean 915g, 1Sd = +/- 15g, and thus 68% of the data falls will be between 900g and 930g, thus 16% of the time an individual sample will be below 900g. Can I then say when sampled, 16% of the time the average will will be below 900g? This seems right but at the same time I feel it greatly depends on sample size which isn't taken into account, or is it? Thanks for any replies!

Edit: Thanks for the help! managed to figure it out with your help, very confidence we're in the clear.",How would you analyses chance of sample average with SD=n exceeding a population average?,905ip1,new,4,0,0,0
"Hi I was studying probability density and on a site I read this example

Mr.  Rossi waits for a phone call from Mr. Bianchi who has announced that he  will call, in an unspecified instant, between 4:00 pm and 6:00 pm. Mr. Rossi must however be away from 4.45 pm to 5.00 pm. What is the probability that the call arrives while Mr. Rossi is absent?The  instant of the call is a random variable X. As far as Mr. Rossi knows,  every moment from 16:00 to 18:00 is possible, while outside this  interval the probability is zero. So  it is intuitive to consider X to be a random variable, whose density is  a constant value: c on the interval \[16,18\] and has the value zero  outside this range. How much is the constant c? It must be such as to satisfy the relationship:

\[Img\] [http://progettomatematica.dm.unibo.it/Prob2/funzionedidens9.png](http://progettomatematica.dm.unibo.it/Prob2/funzionedidens9.png) \[/ img\]

that is, the area of ​​the rectangle with base \[16,18\] and height c is 1. So 2c = 1 and therefore we have c = 1/2. The density function of the variable X will be:

\[Img\] [http://progettomatematica.dm.unibo.it/Prob2/funzionedidens5.png](http://progettomatematica.dm.unibo.it/Prob2/funzionedidens5.png) \[/ img\]

\[Img\] [http://progettomatematica.dm.unibo.it/Prob2/funzionedidens12tr.png](http://progettomatematica.dm.unibo.it/Prob2/funzionedidens12tr.png) \[/ img\]

The  area is equal to the probability P (16:45 ≤ X ≤ 17:00) and we note that  the value of this probability is 1/8 (the area of ​​the rectangle is 1)

I can ask you same questions:

My  questions:

How have he calculated  the  probability of the rectangle (1/8)? The  area of the rectangle is '' 1 '' but how did he calculate the  probability of 1/8?

question numer 2 :  Suppose that in  the interval (a, b)  (time in which  you expect  the call) there  are  some subintervals that they  have not   the  same probability of the others subintervals for the call to be  received, for example immagine that at 17:58 there  was a greater  probability of receiving a phone call compared to  other subintervals.  In that case, how do you set the density function?

question n 3

1. Are there software in R to calculate probability densitity?",density probability,905hdf,new,3,7,7,0
"Hi there! I recently found out that some of my clients don't know how to read tables. That's why I thought I write a blog post about it. Maybe it could also be helpful for you:

[https://berndschmidl.com/?p=342](https://berndschmidl.com/?p=342) 

Greetings

Bernd",Can your clients read tables?,904uwe,new,15,3,3,0
"One sample, same test four times with lots of different variables.

This is exercise based tests so temps, weight, blood levels etc.

Wondering what type of test I should run on this? I would be using SPSS

Many thanks!",Which statistical test would be suitable for this,904qco,new,6,0,0,0
"The values of x1, x2, x3, x4 and x5 are chosen randomly and independently over the range [0, 1/5]. x is the sum of these 5 values.

The values of y1, y2 and y3 are chosen randomly and independently over the range [0, 1/3]. y is the sum of these 3 values.

Which one, x or y, has a greater variance? Besides, how do you compute the variances?",Which one has a greater variance?,904q5r,new,12,0,0,0
"So my professor just claimed that in a fixed effects model, the individual effects µᵢ cannot be estimated consistently (even when using LSDV). I was unable to find something on the internet, so maybe you can help me.

My intuition would be that consistency requires that as N·T → ∞, µᵢ will be estimated at its true value; but that ordinarily, when adding observations in a panel, only N grows (adding no information to already existing fixed effects µᵢ) and T is much much smaller than N in most panels (and grows much more slowly), we never approach this state for the individual effects.

Thanks, folks.",Why can't the µᵢ in a fixed effects model be estimated consistently?,904bd6,new,23,5,5,0
"Suppose I have a dataset where each row has population counts of people that fall into various demographic measures, and then counts of an outcome (e.g.heart-attack deaths in a given year in that city or county). Each row is a given geographic location (like a city or county).


So lets say we have 5 rows corresponding to 5 different cities. For each row, we have a city-specific count of heart attack deaths in 2017. Then suppose for each row we have three variables that describe counts of people in three different income categories (i.e. one column says X number of people in City Y make $0-20k, a second column says X number of people make $20k-40k, and the third column says X number of people make $40k+).

Finally, suppose you wanted to model income as a predictor of deaths from heart-attack (this is just a hypothetical). What approach would you use to model that data?

","How would you model city or county-specific counts data for a particular outcome, looking at counts of people in various categories in said city/county?",900qas,new,3,11,11,0
"The title is a quote from the OpenIntro Statistics textbook. The population is college students with the values being how many hours of sleep they get. I do not understand why the size of the sample, in this case less than 10%, will determine if the sample is independent or not?","""Because this is a simple random sample from less than 10% of the student body, the observations are independent.""",900gxz,new,11,8,8,0
"I have a distribution of predictions I can sample very efficiently from and get the estimated density, but symbolic integration is not possible. I am interested in the confidence Interval with an upper and lower bound. I am not sure how I can tackle this, probably some kind of Variational inference would could be applied?

Any literature-tips?",Approximate Confidence Interval,900d2d,new,9,2,2,0
"Trying to figure out the house favor for a game of blackjack with these rule variations:

A tie causes the player to lose half of his bet.
A 21 is 1:1 payout and can be tied by the house.
Ace is only worth 1 no matter what.
Everything else remains the same. ",Calculate the odds. Blackjack with different rules,8zzlh5,new,0,0,0,0
"Hello! 

Haven't done stats in years but I'm doing some analysis on a given distribution and I've discovered that if I run my test 10 million times and calculate the _average_ chi squared value across all of the tests, it converges to `2`.

What does this mean? More importantly, did I write my tests wrong?

Thanks :)",What does it mean when a chi-squared value converges to a specific value?,8zxnx0,new,4,1,1,0
"The textbook I am reading (OpenIntro Statistics) said you can use the normal approximation for binomial distributions if np and n(1-p) are at least 10. But then it goes on to say it does not work for small interval range counts even if this is met, like for example the probability of finding 69,70, or 71 smokers in a group of 400. And then it states that to improve the accuracy we want to increase the cutoff values by 0.5 if using the normal approximation.

I really want to understand all of this intuitively, like when is it best to use normal and binomial approximations, or what to increase the cutoff value by and not just follow this 0.5 rule the book gives me. Do you guys have any good resources for learning this? Thank you!",Why does the normal and binomial distrbution approximation differ for small range counts?,8zwwok,new,1,0,0,0
Is the reason that E(X) is the same as E(E(X)) that when you take E(X) you take all possible values of X and multiply it by their weights to get the expected mean. And when you take E(E(X)) it's like taking a value of X that is the mean and multiplying it by the probability of the mean which is 1 ending up with the same E(X)? Thanks!,I want to make sure my logic is correct for E(X) = E(E(X)),8zwpjx,new,3,1,1,0
"Hey guys, I am working on a project for a summer stats class I’m taking and I’m a little stumped on how to solve this part of my project. 

I am analyzing a random sample 200 of my saved songs on Spotify. In that 200, two songs that were the same length played back-to-back and I want to find out the probability of that happening. 

Do I use conditional probability equations? Does the average length of the songs in my sample factor in at all? Really stumped here and could use some help. ",What is the probability that a song will be a certain length?,8zwb0j,new,25,4,4,0
"I'm deliberately changing the way my data works to simplify it. So, let's say we have data formed like this:

		Beers

Jim		5

John	7

Jane	1

we could do the frequency analysis according to this tutorial: [http://www.statisticshowto.com/how-to-make-an-spss-frequency-table/](http://www.statisticshowto.com/how-to-make-an-spss-frequency-table/)

However, my data is something like this:

		Beers	Wine	Water

Jim		5		1		0

John	7		3		1

Jane	1		0		5

So what are my options? Must I do frequency analysis for each of the drinks? I guess I could sum up the drinks and say for example how many glasses has each one had, but I'd lose the information about what was someone drinking.",Frequencies when there are several options,8zuet0,new,2,0,0,0
"Hi guys I work for a credit company and we work with large volumes of debt, essentially money that has gone delinquent, passed recoveries and been defaulted on. We contract these portfolios out to collection agencies and collect on them for our clients.

I've been tasked with building my first baseline forecast, essentially we build a 20 month curve for each segment (as percentage of liquidation - that is percentage of the accounts total initial balance we collect each month).

What I've done is build 20+ variables which may or may not have an influence on dollars collected, what I've found is that the initial account balance as well as 4-5 of the date differentials are the most significant predictors. Since I have to build these out into segments, I can only do a reasonable amount (10-15 max). What I've done is create 3 top segments, never paid before default, paid last 6 months before default and paid 6+ before default, and split these 3 top segments into balance bands (>500,500-1000-,1000-1500 etc.). I then averaged out their performance month by month to create curves.

I've built these curves on the historical 2017 data and forecast them out into the first 6 months of 2018. What I've found is that with this segmentation my training data over approximates total $ collected so far in 2018 to be 4% higher than it really is. This is OK but since 4% actually amounts to a good deal of profit for us it ideally needs to be +/- 2% to get anyone to sign off on it.

What I'm wondering is if anyone has any suggestions on how to possibly solve this. The only things I've thought of so far are:

1. Get more predictive variables, we have requested more data from the client
2. Create more segments with other predictive variables, we already have 12 segments though so another classification layer would potentially bump this up to 24-36 segments no bueno
3. Scrap the segmentation plan and build a model that approximates individual account liquidation month by month, I' built a random forest and it was 6% out on $ collected in month 6 - however, building 20 random forests that predict month 1 to 20 may yield better accuracy?
4. My random forest wasnt actually too bad, but I visualised it the predicted vs. actuals and the 0,1 abline through it was skew, low values were over predicted by 5-10% and high values were underpredicted by 5-10% on average

This is my first job and I want to impress my boss/co-workers, one other thought we had was to build a neural network using an R package and see how that predicts. Anyone who has more insight, is further along their studies or career than I I would truly appreciate insight from. I have a B.S. in stats and I do a lot of reading into data science but I'm still learning a whole lot when it come to finance.",Forecasting a Baseline,8zttam,new,4,0,0,0
"I’m planning to go into a basic data analysis job and then transition into something with more computer programming like data scientist.

What is the best route to do this? Should I get a major in statistics, minor in CS, then work to gain experience then go back for statistics masters/PHD?

Should I go for a major in Computer science, minor in statistics, then go for a masters/PHD in Computer Science?

I’m not exactly sure how statisticians are able to gain mastery of both statistics and computer science. I definitely want to do both, but what is the best way to gain knowledge and mastery of computer science in college. ",What college routes/disciplines will best prepare me for a data science job.,8ztix6,new,21,17,17,0
 Like let's say I suddenly decide I want to go into engineering or psychology because I had a change of passion or because I have too many. Could a degree statistics help me with that?,How flexible is a Bachelor's in Statistics?,8zsvtk,new,4,0,0,0
"Hello. The textbook I am reading has only talked about variance in the form  Var(X) = (x - µ)2 P\*(X = x). Could someone explain how Var(X) = E(X2) – \[E(\*X)\]2 is an equivalent equation?

The first equation to me is intuitive. If the 2nd equation makes sense to you could you explain how you think about it? Thanks!",Understanding equation semantics,8zoxkk,new,6,1,1,0
"Also, what is the likelihood observational cohort studies of aspirin use and outcomes like cancer incidence suffer from this bias?",ELI5: Immortal Time Bias,8zocxn,new,3,0,0,0
"I'm still not quite sure what the point of transforming data to the normal distribution is useful for, even though it seems to be recommended operation anytime you look up ""non-normal data"".  


For instance, say I have taken a sample and it seems fairly obvious that the data is log-normal. I could transform the data to a normal distribution, and perform some normal hypothesis tests such t-tests or ANOVA. The purpose of hypothesis tests is to set an acceptable significant threshold for alpha error (typically .05), and test with the assumption that, if the null hypothesis is true, that is your chance for a false positive. Isn't that lost once you transform your data?  For example a data point may fall beyond the significant threshold if it was transformed, but it wouldn't fall beyond that threshold in the original distribution. In this case my ability to predict the false positive rate, even in the case where i'm sure the null hypothesis is true is destroyed.  


It would seem to me that the correct procedure would to always try to estimate what the true population distribution is, and base thresholds based on percentile ranges from it's true distribution rather than transform the data. However the fact that this is a recommend procedure on many websites has me confused. ",Is transforming data to normal ever useful?,8zo3c3,new,50,28,28,0
"Hello. I'm currently working on a model that, unfortunately, I can't reproduce. I have a count type dependent variable (varying from tens to thousands), several count type independent variables (varying from zeros to tens), and thousands of observations taken from three subjects.
The model I'm trying to fit is structured in the following way: model.1<-glmmTMB(dep.var. ~ ind.var1 + ind.var2 + ind.var3 + ind.var4 + (1|Subjects/Observations), family = ""nbinom2"").
The reason I'm opting for random effects despite the restricted number of subjects is that these are sampled from a larger population, and I want the model to consider them but I'm not directly interested in their effect on the DV.
Anyway, when I try to observe the residual plot with Dharma, this is the result: https://postimg.cc/image/cbhjdyl31/
As you can see, there is a rather perfectly diagonal line in the graph on the right that I can't explain. When I fit the same formula with glmer.nb, I receive a totally different output (not diagonal, but not acceptable either). I have also thought that overdispersion may be the cause, but it should be accounted by the negative binomial family. I was wondering if any of you could help me in finding a fitting model to my data. Thank you anyway.

PS: I already posted a question about my research a few weeks ago here: https://www.reddit.com/r/statistics/comments/8x0msc/have_i_fitted_this_mixed_effect_model_correctly/",Diagonal residual line when using glmmTMB,8zmocz,new,0,1,1,0
"Hello all and thanks for your attention. I'm very new to stats and am largely self-teaching, so please feel free to correct any mistakes in my terminology or assumptions, etc.

I am attempting to determine whether there a categorical variable (season) can predict a proportion variable (prevalence of infection with a particular parasite).

For example, we might have something like this (please note this is just example data and the numbers are arbitrary):

* Spring: 20% hosts sampled are infected
* Summer: 25%
* Autumn: 60%
* Winter: 55%

N (number of hosts examined for parasites) for each season ranges from 53 to 118.

How would I go about testing whether there is a relationship between season and parasite prevalence? I have already tested to see whether there is a statistically significant difference in prevalence between each pair of seasons (I have compared Spring with Summer, Spring with Autumn, etc.), but intuitively I feel that there must be a neater and more informative way of doing this.

Any help including any links to existing resources would be much appreciated.

Thank you!","Which test? Categorical predictor variable, proportion response variable",8zmati,new,4,1,1,0
I'm finishing an intern project and writing a paper about it. Is there a service or a person who I can send a paper to for it to be reviewed and to  get a feedback on what to change and stuff ? Is there some online service that does stuff like that ?,Paper review,8zm9pp,new,3,1,1,0
"Hi Reddit,

I am using G*Power for doing Anonva: Repeated Measures within factor a priori power analysis. I would like to understand the formulas behind it, but somehow i am not able to actually find how the input parameters relate to the sample size and the actual power.

Can anyone help me out there?

Thanks!",Power Analysis - how to compute the sample size?,8zm0n2,new,4,2,2,0
"I did a 4-wave study measuring x and y (both continuous variables). I now want to know if there are causal effects of x on y. So far I've done 3 separate regressions: 1) the effect of x1 on y2, controlling for y1, 2) the effect of x2 on y3, controlling for y2, and 3) the effect of x3 on y4, controlling for y3. 

I feel like I'm losing information cutting my data up like this though. Does anyone know of an example analysing this type of data 'as a whole' through multilevel analysis? (Preferably done in R). I've googled a lot but I just can't seem to find an example. 

Let me know if you need more info! Thanks! ",Cross lagged panel analysis in R,8zkpr2,new,11,6,6,0
"I am  reading  the  Bernoulli  distribution.  In the  example   a  dado / dice  with 6  faces. 

By rolling 5 times a dice I have the chance to get 3 twice first, then two different numbers and finally the number ''3''?

 He uses a Bernoulli distribution to calculate the composite probability. let's suppose  for a moment that the random variable is not a dice (containing a finite  number of faces and numbers) but is an indeterminable event on which we  can only make hypotheses. The Bernoulli distribution will not provide the right results? Is not that ok?",Bernoulli distribution,8zjex8,new,0,1,1,0
"Hey guys
I work in the automotive industry
I have a population of 72 parts that i scanned for geometry measurements which generated the values for 55 measurement points for each part.
My problem is, I wanna calculate the capability of the process which requires that the measurement for a given point must follow a normality law for all the 72 parts.
i did the normality test and i have 40% of the points do not follow a normality law.
What are the reprecutions for that and is there a way to calculate the capability (Cp and Cpk) in that case.
thank you in advance",Normality problem,8ziy3k,new,13,3,3,0
"I have a survey and I asked the same question two different ways, I want to know if the answers to the new phrasing are more or less corresponding to the old phrasing. Both are on the same 1-10 scale, and the only difference is in the wording used. So I have three columns: an ID number, question #1, and question #2 (both questions are integers from 1-10). Visually they sometimes line up, but not always. How can I determine the relationship between the two variables? ",Determine relationship between two variables (probably super easy),8zixtl,new,12,6,6,0
"My problem is: Given a deck of cards, randomly take k cards (k > 1). Then write the results and shuffle them back. Repeat this experiment n times (consider n to be a big number). How can I know this is a fair game? That is to say the cards follow a uniform distribution.

I was trying to use the standard chi square test (the sum of (oi - ei)\^2/ei formula) to prove that. Except that I remember something about independence being a requirement. Clearly the tests are independent, but inside each test, it is not (if I take a card, it cannot be taken again till the next experiment). Does this mean the chi square test does not apply in this case? If so, what options do I have? Or is it valid but just not the best option? I need to know if what I was doing is bullshit. Please SEND HELP. Thanks!",How can I test this experiment for uniformity?,8zhvy2,new,4,7,7,0
"Just like the title says, can you use AICc to compare models if the response variable is the same but the factors/covariates are different? Or can you only use AICc when comparing different fits of the same response and independent variables?",Can you use AICc to compare models if the response variable is the same but the factors are different?,8zhfzc,new,16,11,11,0
"What's everybody's thoughts on dropping a blocking factor from models? I have a professor that's a big fan of blocking in ecological field experiments. He's always encouraged us to include a block factor (if applicable) during the experimental design. If it's significant...great. If it's not, drop it. Is this sound sound statistical practice? ",Dropping the blocking factor from models if it's not significant?,8zhcat,new,1,2,2,0
"Hey everyone!

I'm really interested in doing a little bit of studying about shuffling methods, and the first thing I'm wondering is the calculation of how random a deck of cards is. I suppose that question is closely-knit to the factors of the cards within the deck (for example, having 2 categories of cards vs. having 13 will change what the result of analyzing its randomness would be), but before I try to even work out anything like this, I wanted to know how much I can build on in terms of discoveries and existing information, rather than try to tackle it all from scratch, potentially reinventing the wheel.

Thanks! :)","Is there such thing as a ""coefficient of randomness"" of a deck of cards?",8zgkya,new,6,0,0,0
"I'm collecting some data as part of a personal web scraping project I'm programming in Python. I have a pretty large sample size, around 20 million. I'm trying to find the average value of a certain data point, and when I look at the standard deviation it is around 30 percent larger than the mean average deviation. If this is the case, should I be using median as my average value, rather than the mean?","If median average deviation is smaller than standard deviation, does that indicate that the median should be used for the average value rather than the mean?",8zg5e3,new,2,5,5,0
I am trying to make something that calculates the kr20 when you enter sample data. But what should the answer be if the test is only 1 question long since the formula would then have this calculation divide by 0? ,Calculating KR20 for a test with one question?,8zfehm,new,0,2,2,0
"I have four independent groups of binary data and I want to see if the difference in proportions (valid/all in each group) between groups A and B is the same or bigger than the difference in proportions between groups C and D.

Could someone guide me in the right direction? I can give more explanation if needed.

Thanks in advance!",Which test should I use to determine if a difference between two proportions is bigger than a difference between two other proportions?,8zehse,new,3,2,2,0
"If I conduct an exploratory factor analysis and create/identify several latent variables, can I then use those latent variables as the dependent variables in multivariate linear regression models? 

I have been looking for answers to this question online. Many seem to say you need to use structural equation modeling if the latent variables are used as dependent variables. But I'm also being told that is not the case and I could just treat it like a regular regression model. 

Is this enough information? ",Latent variables as dependent variables in a multivariate linear regression?,8ze3dc,new,4,2,2,0
"This is a fairly basic question, and I need a quick sanity check: 

Can someone confirm that if I pre-match individuals 1:1 on a set a variables, then I need to create a match ID for each matched pair and include this match ID variable in my regression model to ensure that the variables I've matched on are still accounted for in the regression.  Is this correct? 

Likewise, if only a subset of individuals from the entire dataset are propensity score matched, then I need to include the propensity score in the regression model.  It's not enough to just regression on the subset of individuals who matched without including some kind of match variable. ",Accounting for matching prior to regression,8zdzsw,new,0,2,2,0
"I am trying to use BLS data, but I can't find a key to decode the column names.  These are the ones I'm seeing and not understanding:

EMP\_PRSE	JOBS\_1000	LOC\_Q	H\_MEAN	A\_MEAN	MEAN\_PRSE	H\_PCT10	H\_PCT25	H\_MEDIAN	H\_PCT75	H\_PCT90	A\_PCT10	A\_PCT25	A\_MEDIAN	A\_PCT75	A\_PCT90",BLS data set column name key,8zdzg5,new,0,1,1,0
"Hi folks,

I have some interesting results and I'm not sure how to interpret them. I realized in my last post I wrote C instead of C' and the responses were great, I had a hard time understanding them until I realized my mistake.

Anyway, here is my output. I'm really unsure what to make of this:

[Output Results](https://imgur.com/a/5kvgs5d).

I honestly don't know how to read this and any help is appreciated. And yeah, you guessed it, I'm a mediation noob, trying to learn this stuff.",Questions regarding mediation analysis and interpretation,8zdd0a,new,3,1,1,0
"So i have a violation of homogeneity of variance in my mixed anova design. However the two groups i work with don't seem to differ anyways, so can i just proceed and interpret the repeated measure effect of the mixed anova? And ignore the violation?

Thanks in advance guys. ",Mixed ANOVA - homogeneity of variance violated,8zc6ib,new,3,1,1,0
"Learn to apply a simple single hidden layer neural network on Iris data. 

https://towardsdatascience.com/neural-network-on-iris-data-4e99601a42c8",Neural network on iris data,8zbplg,new,1,0,0,0
"Lloyd's of London predicted France's world cup victory based on insurable value of players, more than a month ago. Link below to the June 12 press release from the oracles of London.  Croatia was the 9th most valuable team and Belgium was the 5th most valuable.

[https://www.lloyds.com/news-and-risk-insight/press-releases/2018/06/dream-team](https://www.lloyds.com/news-and-risk-insight/press-releases/2018/06/dream-team)",for stats Redditors who also followed the World Cup,8zboyf,new,16,20,20,0
"Hey all,

I’ve been googling and searching this subreddit and haven’t found any good resources, so I’m reaching out to y’all for advice:

What’s the best way I can learn the statistical knowledge necessary to be a successful amateur political statistician for my local and state progressive candidates? I’m happy to start from scratch here, but was wondering if there’s a specific course out there for what I’m looking for.     

My background: I graduated with a degree in CS and Math in 2013, but only took 2 calc based stats classes. 

Thanks so much!",Best way to learn statistics for application in politics,8zbes8,new,4,0,0,0
"P-hacking (or data dredging, data fishing, data snooping) is the use of data mining to discover patterns which are presented as statistically significant, but the analysis is done by exhaustively searching various combinations of variables for correlation.

https://dataschool.com/what-is-p-hacking/",What is p hacking?,8zatqs,new,5,0,0,0
"I'm working on a research project and I need some help. I've got a list of records, with roughly every 60-70th record having an indicator variable value of 1. For each value of 1, I need to go forward (or back, it doesn't matter) between 10 and 50 records (chosen randomly for each record).

Can anyone help me with this?",SAS Help,8zakb8,new,5,2,2,0
"Assuming I have only Calc 1-3 + LA (and very minimal knowledge of prob/stats), would it be advisable to attempt a Theoretical Stats course which follows Casella-Berger?",Statistical Inference by Casella-Berger pre-requisites,8z8gx2,new,7,11,11,0
"Hello, I'm reading (Baby Measure Theory
Charles J. Geyer)[http://www.stat.umn.edu/geyer/8501/measure.pdf] posted by /u/berf a while back. 

I'm convinced I need to learn measure theory.

Are there any prereq or books recommendation? My cubical neighbor recommend ""Probability Theory"" by Chow & Teicher.

Do note I've done up to Calculus II (integration) nothing crazier beyond this and close to completing my Master program. Do I need any math prereq before tackling measure theory? 

Thank you in advance.

After this I'm going to try to follow this plan path:

https://rkbookreviews.wordpress.com/2013/01/27/a-first-look-at-rigorous-probability-theory-review/

I love this blog post and reviewer stuff.


---

Shit I got stuck on section 4 sigma algebra of Baby Measure Theory
Charles J. Geyer. I don't think the definition was clear if anybody have any baby sigma algebra book I would like that too thanks.",Self teaching Measure theory needs advices,8z5y6d,new,8,9,9,0
"Also, what minor would be useful? I'm not passionate about any industry in particular so I really don't mind which organization I work for.
Program in question:
https://zicklin.baruch.cuny.edu/academic-programs/undergraduate/majors/statistics-quantitative-modeling/

",1 What kind of internships/fields would I qualify for with a BBA in Statistics & Quantitative Modeling?,8z5ard,new,1,3,3,0
"Senior year of undergrad I did some research with a professor who I will not name at an institution I will not name. My part of the research involved writing a relatively simple MATLAB algorithm based on a result from my professor's previous work to run on problem instances. Sparing details, it went fairly decent. It resulted in me graduating with highest honors and I wound up presenting the results on my professor's behalf at an international conference that summer after I graduated. Fall comes around and I start grad school, the project is not quite at a place where it is ready to be written up as a paper and submitted yet. I decided I wanted to go in a different direction and I dropped off the project since it was only supposed to be a senior honor's thesis and I was in grad school now working on other things. A year goes by and I happen to look back at this professor's webpage and see that they have submitted the paper for publication. The MATLAB code submitted is very similar to the code I originally worked on. When I left, the core elements of the code were completed, there were still some bugs involving logistics of handling different types of formats for different dataset sources, but the ""interesting part"" of the code was the same to the point that variable names were the same I had used. In the paper, I was not cited, nor was I even acknowledged or mentioned at all. This professor is known within the department for his disagreeableness with both students and colleagues. 

I know there is a lot of room for interpretation around the similarity of code and intellectual property, but I just feel like I was used. I am upset that the year of work I put in was not even acknowledged. Am I right to feel like I deserve mention or is it normal for this kind of thing to take place? I was not on the project at it's completion, but had I not spent the year on it, the project likely wouldn't have gotten off the ground. In the beginning it was just this professor and I. Has anyone else ever experienced anything like this? I would provide further details but I do not want to reveal their identity. Should I confront this professor about it? They hold a fair amount of power in their field and I want to preserve the possibility of me continuing in the field. I am afraid that if it's their word against mine then my future in academia will be tarnished.",Advice on not being acknowledged for paper I contributed to,8z4rpg,new,3,5,5,0
"currently a senior in stats and I'm having trouble deciding which class should I take? (I can only pick one). Which class would you take or how would you rank these 3 classes in terms of interesting/usefulness? I'm interested in tech/quant/finance and healthcare sectors so I think ML and Stochastic would come in handy for the first 2 but Survival Analysis could be useful for retail/consumer analytics and healthcare. The ML class is a little skeptical since it's a new class and sounds ""buzzwordy"" while the others have a better reputation. I am open to the idea of going grad school but I prefer to work a bit after my undergrad so I want to have the skills needed in the job market. Thoughts? 

TOPIC: Machine Learning for Financial Data

DESCRIPTION: This course introduces modern machine learning techniques that are tailored for analyzing financial data. Topics include Financial Data Preprocessing, Ensemble Methods, Cross Validation, Convolutional Neural Networks, Recurrent Neural Networks with Long Short-Term Memory / Gated Recurrent Units, Generative Adversarial Networks. The emphasis is on the basics of these methods and their relevant applications with financial data.

TOPIC: Stochastic Processes

DESCRIPTION: A stochastic process is a random process that represents the evolution of some system over time. Topics may include discrete-time and continuous-time Markov chains, birth-and-death chains, branching chains, stationary distributions, random walks, Markov pure jump processes, birth-and-death processes, renewal processes, Poisson process, queues, second order processes, Brownian motion (Wiener process), and Ito's lemma.

TOPIC: Survival Analysis 

DESCRIPTION: Introduction to the analysis of time-to-event outcomes. Topics center around three main procedures: the Kaplan-Meier estimator, the log-rank test, and Cox regression. Emphasis on big-picture concepts, basic methodological understanding, and practical implementation in R.",stat course recommendation? (ML or Stochastic Processes or Survival Analysis),8z4oy6,new,7,16,16,0
"I have two years left of my Math PhD, and I’ve decided to go the industry route and pursue a career in statistics/data science. I’m not required to take anymore classes, so I will basically spending my time on writing my thesis and gaining relevant skills for industry. However, I can fit in a masters in statistics by taking 2-3 classes each semester. My question is, would it look better to have an MS in Stats on my resume with the Math PhD, or should I just put all my effort in learning the material on my own and doing some side projects? I’m leaning towards the latter because I’m honestly tired of the classroom setting, but since I’m unfamiliar with industry, I’d like some advice.",Advice: second Master’s or self taught,8z4jrs,new,6,1,1,0
I'm learning about latent variable models but they use a very different way of representing things than what I'm used to (regression models). [Please take a look at my question on Cross Validated.](https://stats.stackexchange.com/questions/357210/trying-to-understand-a-latent-curve-model-in-terms-of-mixed-effects-regression) It has the exact path model I'm trying to understand and some LaTeX with the equations I think should describe it.,Trying to understand the regression structure implied by a path model,8z4hfx,new,0,1,1,0
"So the box test of my mixed design anova is significant. (e.g. there is no homogeniety of covariances)

How bad is this? Lavenes test is not, so all good on that end,...could i just proceed?

And if not, what alternatives could i look up?

Thanks in advance",Mixed design ANOVA - box test is significant,8z1qyu,new,1,2,2,0
"I need to compare the performance of 3 Evolutionary algorithms on some Benchmark functions.

>Evolutionary algorithms are a heuristic-based approach which are used to solve optimization problems.

I have collected their best optimum value on different benchmark functions for multiple independent runs and calculated their mean and standard deviation. Would this be  suffice or I need to use some statistical tests like t-test to present some robust comparison. 

>Note: I have only collected the final best value of each algorithms not all values during different iterations

Thank you",How to compare the performance of Evolutionary Algorithms,8z0bsf,new,4,2,2,0
"In the scatterplot of two variables my data takes the shape of a triangle. Basically when X is zero Y is also zero, but when X is one Y can by anything between 0 and 1. An X of 0.5 correspond to a Y between 0 and 0.5. The shape is not exactly as clean as I described but almost. 

My interpretation of this is that X limit the max value of Y. There are strong logical reasons of why this could be the case but prior to doing the experiment the relationship was not known.

Is there a statistical test I could use to interpret this situation? I believe plain linear regression applied to this data would not make much sense. 

Thanks for the help!

Edit: I will add some info. The X axis is the probability of an event happening and the Y axis is a measurement of the event IF it happened (if it did not happen there is no measurement). Ideally there should be no relationship between the two but from looking at the data, it really seems like there is. I am trying to understand the effect the ""event probability"" have on the measurement (or vice versa). ","How to handle heteroscedastic data with a ""triangular"" shape?",8z0bex,new,9,12,12,0
"https://ntguardian.wordpress.com/2018/07/14/replication-intervals/

> ... [A] replication interval... describes how much an estimate from a replication study may differ from an originally produced estimate before the replication study may question whether the two studies describe the same process. ...
>
> ... [C]onveniently [when considering a particular case of interval similar to usual two-sided confidence intervals] the replication interval’s (RI’s) margin is $\sqrt{2}$ times the margin of error of the confidence interval. This means that if we want a replication interval and have only a two-sided confidence interval’s margin of error, we can take that margin of error and multiply it by $\sqrt{2} = 1.414... \approx 1.5$ and have the replication interval4. As an example, take a hypothetical poll I mentioned earlier which produced a 95% CI $0.41 \pm .03$. Then the (approximiate) RI is $0.41 \pm .045$. We should expect future polls with the same sample size and performing the same procedures to produce an estimated approval rating within 4.5 percentage points of the original poll.
>
> So the punchline is this: take a margin of error from a CI and multiply it by 1.5; a future, replication estimate should be within that distance of the original estimate (with the same confidence level as the CI).",Replication Intervals,8yw7qw,new,1,6,6,0
"Hi,

Ok, so let's say that:

\- out of 50,000 homes in a city, 500 are broken into per year at random.

\- As houses are broken into, a new door is installed that prevents future break-ins. The new door does not deter future break-in attempts, it just prevents a successful break-in. 

\- A single house can experience a break-in attempt an unlimited amount of times after the first successful break-in and a new door has been installed.

I need help with the following:

\- What percent of the 500 homes with new doors will experience another break-in attempt over the first year? Over 5 years and beyond?

\- Since successful break-ins are random, how many new doors are installed per year? Over 5 years and beyond?

Is there some sort of formula that can be used for this? Thank you. ",I need help solving a stats problem. Any help is appreciated.,8yv4rf,new,27,2,2,0
"We're analyzing how a media that is popular can lead to more popularity because of the making of memes. Although, it's hard to form this into data so I tried to lower it down by mentioning social media sites like twitter or facebook. I am in no luck as I am new to this and I am now having trouble trying to figure it out.  


If I say the media is just ""rap songs"" and the memes created can be ""rap memes"" how would I get the number of memes being made and how much it's shared? I honestly am on the verge of breaking down I wish I knew more about these stuff before giving the proposal.",Statistics on Memes?,8ytivg,new,5,0,0,0
"Hi all,  


Can you guys provide me with some resources to learn about different types of statistical models for research? I know this obviously all depends on a number of factors like what kind of research, what kind of models am I referring to, so on and so forth.  


I work in an Alzheimer's Disease/Dementia/Cognitive aging research lab as a research assistant primarily working with neuroimaging analysis. I would like to learn about the different types of statistical models, including but not limited to, single T-test, two T-test, single linear regression, multiple regression and the like.   


If you have any questions that may help you help me and find a better resource for me, please don't hesitate to ask.   


Thanks.",Best resources to learn about different types of statistical models for Science Research,8ytd5a,new,4,18,18,0
,How many generations till most Australians are aboriginal/TSI descendants?,8ysqjf,new,2,0,0,0
"If a report states that ""70.77&#37; of India's population has mobile phones"" how can one make sense of this value? What is 0.77&#37; of the population?

Say you have a population of 600 people...

70&#37; = 420 people. I can visualise 420 people.

70.77&#37; = 424.62. How can you have 0.62 people?  


Points for those who are intelligent enough to provide an explanation a 10 yr old would grasp. ",Understanding decimals when analysing whole number categories like population?,8ysoo8,new,12,0,0,0
"I finished AP Statistics, and in all honesty I really enjoyed it. It was the most difficult, but most enjoyable class I’ve ever taken. It required some heavy critical thinking and it was very unlike any math I’ve ever taken in high school.

Does statistics make a good job?

Does it pay well? (starting salary and opportunities to move up)

What is very difficult about it?

Is it difficult to find jobs?

How much education is required for certain jobs?

What are specific job titles and what they do?

If someone can answer any of these questions it would be greatly appreciated. Thank you.
",Does statistics make a good job?,8ysilw,new,22,1,1,0
"Hello   (I am  new to statistic, I am 20 years  old just starting out, please  don't  be so rude)

Given these  numbers:

2,  3,   4,  8,  10, 15,  22

and  here the number of days (each number  for  the correspective day, so 2 \*1day, 3\*2day, 8\* 4):

1    2    3   4   5    6     7

Calculate  the  Weighted mobile average  and  the Esponential moving average

I know  the formula  for calculating the Weighted  mobile average,  but I can't  understand the formula for Esponential moving average:

EMA (i) = EMA (i-1) + SF \* \[P (i) – EMA (i-1)\]

I don't  understand... I want  to  calculate the  EMA,  but  following this formula  I don't know  what  should  I put in EMA (i-1)   ... I don't have  the  EMA  value... I would to calculate it.

If  this  example  is not explicative, I can make the example  with  some  stock's prices:

day - closing Price

* 1 – 0,87777
* 2 – 0,88196
* 3 – 0,89143
* 4 – 0,89649
* 5 – 0,90522
* 6 – 0,89942
* 7 – 0,88975
* 8 – 0.88993
* 9 – 0.88488
* 10 – 0,89665

Left  there  are the days,  on the right  the closing  price.

How  to calculate the  EMA using that  formula?",EMA and WMA,8yruvx,new,0,0,0,0
"I’m wondering if there exists a model / approach for optimizing an ordered list from a bank of options that are defined by two (or more) different variables. 

My example would be a fantasy football draft. Not only do you have to rank individual positions in order of who is best, but you also have to take into account what position to draft in each round. Like “I’m in round 3 and I could take the #14 RB on my list or I could take the #12 WR on my list.”  Additionally, you could incorporate who is likely to be available at your pick and take that into account as a third variable. 

This is either just nonsense or there is a particular type of approach for something of this dynamic.  Does anybody know of anything or is this nothing? ",Wondering if there is a type of model or approach for a particular situation,8yr2fv,new,0,0,0,0
"I have a couple that I've thought of so far:

1. If none of the results within a study or a large area of the study end up falling within the authors' definition of ""statistical significance"", then I believe this is strong evidence that *p*-hacking isn't present in the paper. This is because I assume the main motivation of *p*-hacking is to artificially lower the *p*-value by changing the hypothesis again and again until it reaches a certain threshold so that the chances of publication, citation, and media response increase. Therefore if the *p* value within a published study isn't at a threshold that's seen as significant by most, then it's highly likely that there was no motive to manipulate it in the first place.

2. If the person responsible for statistical analysis of a given paper has a proper degree in statistics, then it's probably at least slightly less likely (on average) that *p*-hacking should occur maliciously due to an increased level of respect for the statistical process due to their formal education in the field. Along with that, *p*-hacking can't be done genuinely ignorantly by anybody with an adequate level of education in statistics, so that also removes the portion of scientific studies wherein *p*-hacking isn't done maliciously, but merely understood incorrectly.

What are your thoughts, or criticisms of my thoughts on this question? Are there are additional properties (or lack of properties) of a scientific paper for which the best explanation is that there is a high probability that *p*-hacking has occurred?

Follow-up post to this one: https://www.reddit.com/r/statistics/comments/8ybrza/are_the_findings_of_peerreviewed/","What Are the Best Ways for a Layman to Detect ""p-Hacking"" Within Scientific Papers When the Number of Hypotheses Tested Isn’t Explicitly Addressed?",8yocr8,new,8,5,5,0
"Hi, I was wondering if any of you would be able to give me your impressions on the subject areas suggested by courses entitled ""[Time Series](https://www.stat.auckland.ac.nz/courses/postgrad/2018/STATS_726)"" and  ""[Multivariate Analysis](https://www.stat.auckland.ac.nz/courses/postgrad/2018/STATS_767)"". 

For context, I am thinking of taking one of these papers, I have some familiarity with an applied treatment of [time series](https://www.stat.auckland.ac.nz/courses/stage3/STATS_326) and am not so good at maths. The Time Series paper gives the better timetable and I often follow [these](https://cdn.auckland.ac.nz/assets/stat/for/future-postgraduates/documents/course-to-careers-table-postgraduate.pdf) [things](https://cdn.auckland.ac.nz/assets/stat/for/future-undergraduates/documents/career-pathways-for-statistics-grads.pdf) when thinking about course decisions. I know all these things and have tried considered them; what I am looking for is some big picture context... some detachment quite removed from my personal concerns, which is obviously best sought from other people. 

Sorry if I seem presumptive, especially about my interesting-ness to you. Also, I mention things like having studied history, economics and statistics (with a couple of marketing courses) because while I say I want detachment, what I really want is someone to say something that gives me a eureka moment, that lets all sorts of vague thoughts collapse into something understandable. On that note, I greatly prefer things when events unfold this way and have typically found this to happen (I studied economics for seven years because too few people wanted to take German in high school and I had to change subjects).",Perspectives on (Applied) Multivariate Analysis and Time Series,8yk8kk,new,2,0,0,0
"I am a programmer and don't know that much about statistics(sorry if this is not the place to post). I work for a municipality and we are trying to figure out which sections of sewer pipes to replace first or at least identify areas that should be looked at.

We have records on all sewer lines including information like year installed, pipe type(clay, plastic, etc. ), video record inspections(someone has determined how severely to pipe is damaged)  , trees near by(roots penetrate the pipe causing blockages).  Based on these factors we would like to rank the sewer pipes from most at risk to least at risk (replace now -replace in a couple years).

Some of these factors interplay with each other for example clay pipe is more susceptible to roots breaking through than plastic pipe.

How do I rank sewer sections based on the above factors of each pipe?

Any guidance would be greatly appreciated.

**Update:** Very few pipes have visual inspections",Assessing At Risk Sewer Lines,8yk52d,new,1,2,2,0
"Hello there,

I am an engineer student, and statistics are really not my speciality. That is why I come here to get some advice from expert on my problem.

So, the project I work on is an autonomous sail boat whose goal is to cross the Atlantic ocean. My work is building a tool for decision support. For example the user might want to know what happens if the ship consumes more power, or if it goes via another route.

The way I do right now is I simulate the journey for a great amount of boat (1000 for example). But of course the behavior of each boat depends on the weather. So I have a database for each position, and for 20 years.

For example for the position lat1 and lon1, I have the data from 01/01/1997 to 31/12/2017. 

Ok so here what happens in my code :

\- The boat n is at position lat1 and lon1, the date is 13/07/2019.

\- I pick randomly in my database one the 20 13th July, let's say I pick 13/07/2003

\- The boat then moves to another position, the date may be 14/07/2019 (but it can also be still 13/07)

\- I pick randomly again, and may be I pick 14/07/2010 (another year).

And I do that for every boat.

My question is simple, is it a good way to have a good statistical sample ?

My supervisor (who is not a statistician too) suggested that maybe a better way would be to pick a random year for a boat at the start and stick to it till the end. (The if a boat start the journey in 2003, it will always be 2003).

Maybe the second options might reduces the number of simulated boats needed, I really don't know.

What do you think ?",Meaningful statistical sample.,8yk3sx,new,2,1,1,0
,Today is Friday the 13:th. Why don't we embrace this day by sharing some of our favourite experiences with people misinterpreting improbable or independent events.,8yisze,new,22,77,77,0
"This[ Linear Regression in Machine Learning video](https://www.youtube.com/watch?v=NUXdtN1W1FE) will help you understand the basics of Linear Regression algorithm - what is Linear Regression, why is it needed and how Simple Linear Regression works with solved examples, Linear regression analysis, applications of Linear Regression and Multiple Linear Regression model. At the end, we will implement a use case on profit estimation of companies using Linear Regression in Python. This Machine Learning tutorial is ideal for beginners who want to understand Data Science algorithms as well as Machine Learning algorithms. 

Below topics are covered in this Linear Regression Machine Learning Tutorial:

1. Introduction to Machine Learning

2. Machine Learning Algorithms

3. Applications of Linear Regression

4. Understanding Linear Regression

5. Multiple Linear Regression

6. Usecase - Profit estimation of companies",The Best Tutorial for Linear Regression - Anyone can understand the concept from the root,8yisjn,new,2,0,0,0
"While we are usually interested in predicting values of time series, it is often also valuable to **predict probability distribution of the next value** basing on its context - for example for risk evaluation, Monte Carlo simulations, data compression.

Standard approach like [Gaussian process regression](https://www.mathworks.com/help/stats/gaussian-process-regression-models.html) is it to predict value and its variance, e.g. assuming Gaussian distribution for error of prediction. In practice this distribution is rather Laplace, and can be essentially deformed if using context.

A basic data compression technique is to quantize contexts and estimate density separately for each bin, e.g. in [JPEG-LS](https://en.wikipedia.org/wiki/Lossless_JPEG)   image compressor there is chosen width of Laplace distribution based  on  separate estimations for 365 bins describing quantized behavior of  neighboring  pixels.

More sophisticated data compressors use [context mixing](https://en.wikipedia.org/wiki/Context_mixing)   models, which have many models and neural networks to mix their   predictions into a single final probability distribution for single bits.

Other approach is to estimate joint distribution for a few  succeeding values, e.g. by fitting polynomial, then substituting  context and normalizing to 1 we get  probability distribution for the  current value - works nicely for [\~30000 length Dow Jones](http://www.idvbook.com/teaching-aid/data-sets/the-dow-jones-industrial-average-data-set/) daily averages sequence: [https://arxiv.org/pdf/1807.04119](https://arxiv.org/pdf/1807.04119)

What are other known methods for predicting probability distribution of value in time series of real numbers?",[Discussion] Predicting probability distribution of value in time series of real numbers like Dow Jones?,8yhc8f,new,2,1,1,0
"I want to learn about different methods of estimating model parameters for LMMs and doing inference on those estimates. I'm especially interested in estimating variance components with known covariance matrices. For example a model of this form

y = U + \\epsilon

where U \~ MVN(0, \\sigma\^2 K) and K is a known covariance matrix. Is there a good place to learn about REML, MINQUE, and other methods? I'm especially having trouble finding readable discussions of MINQUE.",Good books (or other resources) on LMMs,8yg41s,new,1,1,1,0
"I know that for histogram bin size, the Freedman-Diaconis rule is designed to select a good size without the statistician having to fist look at the data and decide their best guess at what the bin size should be.

Similar to chosing the bin sizes, statified random sampling needs sizes to be chosen for the intervals of prior data that will be the strata when basing it on a numerical variable.

Is the Freedman-Diaconis rule valid for deciding this? Or is there something similar for statified random sampling? Or does the statitian just have to look at the prior data of the population and make a guess?

---

I started wondering this after seeing a [TEDx talk about sortition](https://www.ted.com/talks/brett_hennig_what_if_we_replaced_politicians_with_randomly_selected_people) and the speaker suggested we use statification in the selection process. 
I wondered how, with many continuous variables (like wealth, earnings, and age [and possibly latent spaces of things like location, race, and gender]), the intervals could be decided upon.",Can the interval sizing for stratified random sampling be done automatically?,8yfiij,new,3,6,6,0
"Hello statistic wizards! 

Say I have a cohort of N people. Among these people, they're being divided into n groups. So each group is asked to carry out a task. 

Now the evaluator has a list of 20 items to check the performance of each group and all are (y/n) type. I.E. X1,...,X20 are all {0,1}. 

After this is done, the same study cohort of N people are to take an intervention class. Then 2 weeks later they're to be evaluated again in similar fashion.  

Except... each group is made up of different combination of people. But still n groups. So again, peform task, evaluator records X1, ..., X20 which are 0 or 1. 

The goal is to see if the intervention class has significant effect on the performance of these N people. 

Me and my friend were thinking McNemar test at first because of the {0,1} and pre/post nature. However the part where each group is made of different combination of people kind of screws that up. Or can we assume the pre-intervention is independent of post-intervention? And do a Chi-Square test..? But I think this is a dangerous assumption to make.

Now I am thinking more regression techniques like binomial regression. 

So we'll have log(EY) = a + b_intervention*(X). Where X = 0 if pre and X = 1 if post intervention. 

I am not sure what's the best approach to this problem. Any advice is appreciated.

Thanks! ",How to analyze data with pre/post intervention type nature?,8yep2l,new,7,1,1,0
"Hi -- I am looking for information on the largest employers by state.  (Not the state where the company is HQed in, but rather the number of people it employs in each state.)  Is there some sort of public disclosure of payroll tax or similar?

Also, are states themselves (as employers) required to disclose the number of employees and/or job openings?",largest employers by state,8ydd96,new,4,5,5,0
"Hi r/statistics! 

So I'm currently heading into my fourth year of undergrad studying MIS, and I'm deciding between pursuing a MS in Analytics vs. Stats. I'm definitely leaning towards stats since I feel it will give me more options looking forward. After looking at the programs I'm interested in, I know I have all the prereqs under my belt (Calc I-III, Linear Algebra x2, Econometrics, Stats, Probability, CS)  so I was going to apply to programs for next fall and spend the next summer reviewing linear algebra/calc as well as playing around with R and python. However, I'm just worried I'm not going to make it very far in any program. Statistics is something I'm definitely interested in, I'm just not sure if I'm prepared in the huge jump in difficulty. How much are you expected to know when starting a MS program? How much of the prereqs are actually going to be used right off the bat, and how much should I review next summer? I really want to enter the data manipulation/data science field at some point so I feel like a stats MS is my best option at this point. 


Side question: How competitive would I be in the job market with  MIS undergrad and Stats MS, with 3 internships as work experience (up to this point)","Strongly considering a MS in Statistics, not sure if I'm cut out for it",8yc6ot,new,8,6,6,0
"If not, what are some warning signs that a layman may be able to distinguish which might server as evidence that the statistics part of a study was done badly and may therefore lead to false conclusions? 

What is the best way to evaluate the relative truth level of a scientific study that includes statistical analyses without having a formal education in statistics?

Follow-up post to this one: https://www.reddit.com/r/statistics/comments/8xtni2/i_barely_know_any_mathematics_and_need_help_using/","Are The Findings of Peer-Reviewed, Methodologically-Rigorous Scientific Studies Published by Reputable Journals and Conducted by Researchers With Proper Education From Accredited Institutions Generally Reliable, Even if They Don't Include Data Analyses From Statisticians?",8ybrza,new,32,15,15,0
"Which calculations are required to verify Doctor Strange's claim that out of 14,000,605 random possibilities, only 1 led to human survival?

Assuming that Thanos's snap was random and Doctor Strange was purely concerned with the survival of certain heroes, I feel like this should be pretty simple to check. But, I've only taken intro stat, so I'm really sure where to begin.

I think this could be a fun question to analyze. Could anyine help me out? Thanks!","Question: Avengers Infinity War, Statistics regarding results so far",8yb6qj,new,2,0,0,0
"I just sat an exam an it went rather badly to say the least. A big chunk of said exam was multiple choice. 45 Questions, 4 options apiece. Only one correct choice in each one, no minus points for wrong answers. 

I'm confident I got 20/45 correct, and I need 23 correct to pass. The remaining 25 Questions were purely random guessing on my part.

Assuming 20/45 are in fact correct, what are the odds I have to take this stupid fucking mandatory course again next year?

Thank you for your time.",Quick question to know what to expect,8ya69q,new,5,3,3,0
"I am an analyst at a big factory and they have the computers locked down right. Everything needs to be white listed. So when I try to load packages in R, the security software blocks necessary .dlls from loading. 

My boss, who is a programmer, said they wrote a step into their compiling macro to sign all the dlls called by the libraries they are using before compiling. I guess they make enough software for the company that they were allowed to whitelist their own dlls. 

Does anyone have an idea of how I could set something like this up in R? Is the load packages function a “base” package where I could work to edit the original command to include a second step?

I know this isn’t the perfect place for this but I thought some of you might have experienced it. ",Editing load packages command,8y95h3,new,2,2,2,0
"I'm a geologist, but I started learning python and now all I do is play around in data sets. I was making some plots in seaborn the other day and I just realized ""this is it, this is what I want to do"". I reached out to the dean of a program I'm interested in and he just encouraged me to take some more classes and apply. So I just signed up for linear algebra and I'm pretty excited! Are there any great books I can read or video series I can use to educate myself prior to starting an MS program. Should I also learn how to use R? Thank you!",I've made up my mind and I want to get my MS in statistics.,8y8lxl,new,18,40,40,0
,Must a statistic be a real number? Can a statistic be a 2-tuple of real numbers?,8y6zq7,new,8,0,0,0
"I'm looking at a medical study and it says ""The observed heterogeneity between pooled trials was high""

I was told ""If you look at the data that they present in this study you will see a heterogeneity score. For many of the parameters that they looked at the heterogeneity score was quite high, which essentially invalidates the analysis. That is why studies like this should never be published. ""

I know nothing about this (hopefully this was even the place to ask) but is that correct. Is a high score mean that this stuff is invalid? ","Trying to understand ""The observed heterogeneity between pooled trials was high""",8y5p56,new,2,3,3,0
"https://www.nature.com/articles/d41586-018-05664-2

From the Introduction of the study: https://academic.oup.com/toxsci/advance-article/doi/10.1093/toxsci/kfy152/5043469

>""We demonstrate a “simple” RASAR which trains a logistic regression model to predict chemical hazards from the similarity to the closest chemical tested negative (maxNeg) and similarity to the closest chemical which has tested positive (maxPos). The approach has been applied to nine of the most frequently used hazard determinations in REACH and toxicology in general. ‘Simple’ RASARs obtain cross-validated sensitivities above 80% with specificities of 50%-70%. This is on par with the reproducibility of the respective animal tests.""",Software beats animal tests at predicting toxicity of chemicals using supervised ML,8y5fe0,new,0,54,54,0
"Is it possible to do to use regression on the vector, such as each variable has an x and y. Im doing this and it seems to work but not sure if its right.

model1=glm(c(vec3x,vec3y) ~  c(vec1x,vec1y)+ c(vec2x,vec2y), data = data)

when I do the X and Y separately I get different beta values compared to the combined.

whats the right way to do this?",Regression on vectors?,8y4pk5,new,2,1,1,0
"I'm not sure if I'm posting in an appropriate place so I will delete it if it's not but...

I was wondering if it is normal have a percentile rank that is different from what a z-score is telling me. 

For example, if I got a z-score of 1, then I would have gotten a score greater than 84.13&#37; of the sample. Is it possible to also have a percentile rank that is higher than 84.13&#37;? ",Percentile ranks and z-scores,8y488h,new,0,2,2,0
"There's a study I was asked about which has 10 categorical predictor variables from a survey (n=6000), and wants to test which of them have a significant relationship with 5 (continuous measurements converted to categorical) outcome variables. Thus we have 50 tests to run.

I would normally look into PCA or a sort of factor analysis, but I get the feeling that for a publication those need to be based on prior research (i.e. you'd need to reference a study which had similar questions and which ""validated"" some factors already, rather than use the factors you found/invented yourself). The researchers also seem very interested in the specific predictors, so I'm not sure they'd be happy with telling them about the relationship between a ""latent factor"" and the outcomes.

Thus I was just going to run the 50 tests with a multiple testing correction like the Hochberg method.

But I wonder - if I tell them to pick 5 or so ""main"" hypotheses (so 5 predictor-outcome relationships that they are most interested in), could I just look at those without doing a correction? Then for ""secondary endpoints"" look at all the other survey questions and do the multiple testing correction at that stage? Or would the reviewers assume we cheated and chose the 5 ""main"" hypotheses retrospectively?","Say you have ~50 tests. Can you choose a few (3-5) ""main"" hypotheses and only do t1 error correction for the remaining tests?",8y4558,new,2,7,7,0
"Hi!

I'm not quite sure what to think of this:

It's about Gaussian mixture models and I'd like to find the optimal number of components. For that reason I programmed a loop which applies the EM algorithm for different numbers of components and computes two information criteria which, for some reason, are not implemented in mixtools. After the iterations for the first number of components I get a error message ""subscript out of bounds"" and no further results. But I can access the results manually and get all my parameters for the number of components I wanted to compute. I.e. according to the log window I only modeled with two components, but I also get my results for 3 and 4 components. Can I just carry on or should I be worried?

Maybe it is of interest that R is acting funny in general today: Instead of displaying the results I wanted it kept returning the last computed value. I was able to solve that by clearing the workspace.

I appreciate any help, thank you :)",R reports error but gives the desired results,8y3pf7,new,5,3,3,0
"I conducted a survey and got my results and am now looking to analyse it. Does anyone know of any good software that could tell me, for example, how many males said this and how many females said that etc. (in relation to my survey results)?",A question about data analysis..,8y1t30,new,2,3,3,0
"A prof posed (and proved a solution to) an altered Monty Hall problem. I'm a layman but I have what I think is a further proof and want to know if there is a mistake in mine.

**First the altered Monty Hall problem and proof**

Monte Hall, with a twist: You have prior information which indicates P(door 1 has car) > P(door 2 has car) > P(door 3 has car). You pick a door and whether or not to switch (in advance). What is optimal strategy?

Let p_i = P(door i has car). Key Lemma: if you don't switch from door i, then probability of winning is p_i if you do switch, then your probability is 1-p_i So, optimal strategy is to pick door 3 and switch. Picking door 1 and staying is worse: p_1 = 1-p_2- p_3 < 1 - p_3

Talking to someone else he said this proof is only for if you specify if you will switch in advance and he doesn't off the top have a proof for if you don't specify. That's what I think my proof (just in words) does.

**He said a proof where you can choose if you switch after Monty opens a door has to include every type of case** (e.g. Monty might always reveal a certain door when he can, or he might reveal randomly, and in certain cases you might know and in certain cases you don't).

**But I say none of these variations can lead to a better result than pick door 3 and switch** (in certain scenarios you could equal it but never do better) **and here's why**:

-Say you are playing any one specific variation^1 (of ""Monty reveals"" behavior and your knowledge of it)

-Assign letters A, B, C randomly to door 1, 2, 3 and once assigned you/player knows which is which, so A might be door 2 etc.

Now to start the game you select a door, let's say A.

If Monty opens B your choice is between A and C. Given the specific variation you are playing either A or C is the better selection and you choose it. Let's say it's C.

If instead of B Monty opens door C your choice is between A and B. Given the specific variation you are playing one door is the right choice, let's say A.

So every round you play of this variation where you started on door A you end up (final selection) on either door A or door C.

In any round you play of any variation of Monty behavior your final choice is always between 2 doors. Monty opens X, you choose B. Monty opens Y, you choose C. There is no way to have a final choice that is sometimes 3 different doors.

Because of that the highest probability of winning the game you can achieve is 1 minus (probability of the 1 door you don't select in any round).

The best of that is 1-p_3, so one of the strategies which achieves the best probability (whether you specify switching in advance or not) is pick door 3 and always switch.

1. (By any variation I mean sticking to: you pick a door, then Monty opens one of the other 2 doors and then you decide whether you switch or stay with your original. So a variation could be something like ""Monty always opens door 3 if he can"" but can't be something like: ""if you pick the correct door on first selection Monty stops the game and gives it to you but if you don't then he opens door 3 every time he can etc."")",Altered Monty Hall problem and a question,8y1fpn,new,3,6,6,0
"Can you still use a multivariate regression (in excel) if the factors are multiplied together rather than added?        
       
For example, an insurance company decides Price = Base Price * Factor 1 * Factor 2 * Factor 3. So their Loss Ratio = Losses/Price. Can I do a regression where Y = Loss Ratio and the Xs = Factors 1-3 or will that just generate nonsense?",Use of a Multivariate Regression on Multiplicative Factors,8y02yx,new,1,4,4,0
"I'm about to finish a M.S. in statistics and have been working professionally as a data analyst for about 5 years now. I've had experience working with people from many different backgrounds.

**I have never used a hypothesis test even once to make a decision professionally.** Here's how I -have- seen them in practice (or almost put in practice):

1. In one of the very first positions I was in, I had a colleague who would run a hypothesis test **over an entire population**. The populations of interest were extremely small (I'm talking something like 5-10 people). My recollection is that this person took their really small data sets, ran some *t*-tests through SPSS, and reported all of the possible *p*-values. No assumption checking or anything like that, not to mention that hypothesis testing is obviously not supposed to be used when you already have the entire population of data already available.

2. There was one situation where a hypothesis test may have been appropriate: namely an independent two-sample *t*-test. But I felt like I was cheating: both samples were very large, so chances are, the *p*-value was going to be small anyway. Also, I felt like comparing the means (which, let's be honest, that's what doing a two-sample *t*-test basically is if you want to ignore the sample size and the estimated standard deviation isn't particularly large) was a bad idea - one of the groups was only 1/3rd of the size of the other group. At the end of the day, I ended up comparing a bunch of different percentiles - and they were essentially identical. No reasonable person would have said that there was any difference between the two distributions. I wish I knew Bayes well enough to have done something more sophisticated, but that's what I knew at the time.

Usually when people talk about hypothesis testing, they usually talk about how interpretation of *p*-values is difficult, or *p*-hacking - and I think these are valid concerns. But professionally, I haven't found any use for it, and I find it hard to justify using traditional hypothesis testing in most situations. 

Now that I've gone through the Casella-Berger textbook, I understand now that what we know of as ""hypothesis testing"" is essentially mapping probabilities to 0/1 ~~in combination with the central limit theorem~~. I care a lot about education in math and stats: why is it that we ingrain to our students from the very beginning that stats is a binary 0/1 process (i.e., either reject the null or do not reject the null)? I can't fathom doing everything professionally as it was taught in the first university-level stats class, and yet, so many people go through their entire careers thinking of stats like this.

This is probably more appropriate for /r/changemyview now that I'm thinking about it, but I'd love to see any discussion on my thoughts here.",Is traditional hypothesis testing useless outside of controlled experiments?,8xyxsg,new,13,9,9,0
"I am trying to get the lognormal distribution into my head. My understanding is that if I generate a sample of random variables from a sample distribution and then calculate e\^x (where x is taken from the random distribution), what I get is a new, so-called lognormal distribution.

Given the basic parameters of my normal distribution, i.e. the mean and standard deviation, what are the parameters of the newly created lognormal distribution? Can I make this backward, too? In other words, given the mean and standard deviation of a lognormal distribution, is it possible to calculate the mean and standard deviation of a ""corresponding"" normal distribution.

Thank you very much!",What is the relationship between normal distribution and lognormal distribution?,8xy5qk,new,9,19,19,0
"I remember using SPSS to put actual measurements in an equation suggested by me and the software will calculate R value for accuracy, but I forgot how I used to do that. Anyone know the exact steps? ",Using SPSS to get R value of an equation,8xx6rm,new,2,2,2,0
"So I’ve heard and I understand why correlation does not imply causation. 

But, then how do you find a causation then? What’s the process to conclude that okay, x is the thing that causes y. ",Question regarding co-relation and causation,8xwrhn,new,6,2,2,0
"I am looking for someone to explain this to me:  

If I interview 100 individuals (50 healthy and 50 with colds) about vitamin C use, and I find the Odd Ratio is 2.0 (i.e if you are healthy the odds are 2 times greater that you took vit C),  why CAN’T  the data be turned ""forward"" and say the OR for taking vitamin C is 0.5 in terms of future sickness? 

If you can help - thanks
",A question about using retrospective data prospectively.,8xvaz4,new,0,2,2,0
"Here are all of the question frequencies: [https://ufile.io/i9ne8](https://ufile.io/i9ne8) 

Some interesting findings:

* most people want flairs (90&#37;)
* most people want mods to check backgrounds for flairs (70&#37;)
* most of us are academic statisticians (43&#37;) followed by professional statisticians (20&#37;)
* the vast majority of us are 18-35 (90&#37;)

And here is the preferred order of content:

1. Published Research and Discussion Posts
2. Links to Articles Dealing with Statistics
3. Posts Regarding Current Trends in Stats
4. Statistics Questions
5. Questions About Programming
6. College/Career Advice and Questions",2017 r/statistics Survey Results (Frequencies),8xuhs6,new,2,2,2,0
"Hey guys,

I've been trying to improve my ability to cope with ADHD by reading scholarly studies on the efficacy of medication and various behavioral therapies. However, they keep using the 'p' as a descriptor of null hypothesis probability, with p = 0.99 being 99% certainty and p = 0.01 being 1% certainty that a given finding is really just a result of the null hypothesis.

Specifically, I'm reading https://bmcpsychiatry.biomedcentral.com/track/pdf/10.1186/1471-244X-12-30%20page%201 and looking at page 5 table 1, and page 6 table 2.

The part that I find hard to grasp about 'p' is that within this double-blind, placebo-controlled, adequately randomized study with over 40 participants, they're posting 'p' values like 0.77, 0.39, 0.31, etc. A couple question about these values:

If the study was double-blind, placebo-controlled, adequately randomized, and maybe not perfect but in every other respect at least very good, how on earth can any mathematical calculations spit out values which say that in some areas of the study there exists anywhere from a 30-70% chance that the findings are actually just the result of the null hypothesis? Doesn't that seem unreasonably high?

If the specific sample of people (CBT+DEX and CBT+PLB) as well as the sample size for the groups across the tables (table 1 and table 2) stays the same, how do their 'p' values differ so drastically? The 'p' range across the tables ranges from 0.15 to 0.77, yet the sample size is always 22-23 for that group. Doesn't it seem reasonable to assume that, all other things being equal, sample size should be is the main driver of whether something should be considered statistically significant or not?

If 'p' is actually a better predictor of statistical significance than sample size alone, what other things does it incorporate that can change it's value so enormously if the sample size stays the same? I mean, what else is there other than bad study methodology that could possibly make the probability of the null hypothesis being true so high?","I Barely Know Any Mathematics, and Need Help Using It to Improve My Life. Is the Explanatory Power of ‘p’ With Regards to Statistical Significance Really Accurate?",8xtni2,new,45,25,25,0
"I have historically held courses on performing ""classic"" statistics in R, like regression models, hypothesis tests and the like. These were chiefly aimed at people who already knew a bit about R.

This fall, I have been recruited to hold a university course on R for beginners, i.e. people with no prior experience. Perhaps they have no experience with programming at all, and they may come from many different, but academic fields. Their statistical knowledge also varies.

I already have some ideas for my course, but I want to make it highly enjoyable and current (for example, I plan on teaching dplyr, data.table and ggplot2 to name just a few great ""new"" packages). The course lasts five days and will be entirely centered around actually using R; i.e. there are no big ""theory sessions"".

Therefore, I pose you this question: What would you want from such a course? Which topics would you be interested in? What is important for me as a lecturer to do, and not to do?

Thanks for your input.",What would you expect from a R course for beginners?,8xsddi,new,18,27,27,0
"*edit in title: but Freedman-Schatzkin test significant?

I'm running a structural equation model and all paths are significant.  The Aroian test isn't significant but the Freedman-Schatzkin test is significant.  I know that if the Aroian test isn't significant then there's no real reason to go further on the model, but is there any references for when this is happening in the model?",Aroian test not significant by Freedman-Schatzkin test significant?,8xs3jo,new,0,1,1,0
"Going to start off saying all of my numbers are hypothetical.

So let's say in one room I am drawing roughly 3000 kW of power.

The room is made up of machines and let's say each machine draws 10 kW and its 99% CI is (9.5, 10.5). 

I want to be able to predict the entire room's power draw after adding machines. So my question is how would calculating the confidence interval of the whole room after adding a few machines work?

I thought about something like:

+  adding 1 machine = 3010, (3009.5, 3010.5)

+  adding 2 machines = 3020, (3019, 3021)

But that doesn't seem right.",Confidence intervals and predicting values,8xrkro,new,9,1,1,0
"Hey there! I've searched around so hard for an answer to this, but I'm not sure it's out there. I want to approximate a tail end of a small binomial distribution to normal.

Essentially, my n=20 and my probability is .95. This means it fails to meet even liberal criteria for conditions to approximate to normal (np is greater than 5, but n(1-p) is not greater than 5). I get some wild values in my approximation.

I've searched for what is a essentially a way to reduce or bound the error involved with tail ends of such a small binomial distribution. Does anyone know of such method? I've found [Littlewood's approximation](https://users.cecs.anu.edu.au/~bdm/papers/littlewood2.pdf), but to be honest, I'm a social sciences person so I couldn't parse it for my own uses yet.

Any help or pointers welcome!

**Edit: to avoid an X Y Problem, my goal is to transform my probability around a specification value of .78, such that p > .78 is a positive z score and p < .78 is a negative z score.**",Approximating small binomial distributions at tails,8xpztr,new,15,2,2,0
"Hi! 

I work at a online retailer where we sell all kinds of stuff. Today we got 2000 sets of cutlery. 500 is blue, 500 yellow, 500 pink, 500 red.

We are going to sell them in assorted colours (or random). So a customer won't know what colour he gets. 

A colleague asked how many purchases he would have to make to be atleast 95 &#37; certain that he will get 8 sets of pink cutlery. 

Sorry if my english is bad :) 

If anyone is willing to help. THANKS :) ",I need help to solve a work discussion,8xp36f,new,5,0,0,0
"Im a recent college grad with an economics degree and I’m heavily considering an applied statistics masters program in attempts to become more knowledgeable in statistical analysis. I’m contemplating the set of skills I need to gain in order to be successful in grad school.

I finished really well in my stats classes but wasn’t great at calculus. My econometrics class is what pushed me to peruse this masters program and I loved it. I’ve never taken linear algebra but I’ve been taking an MIT course on my own that I found online. I’m worried that my undergrad stats classes (business stats) may not have sufficiently prepared me for more rigorous proofs and concepts in the MS applied stats program. 

Are there any rigorous undergrad level textbooks that focus on theory that anyone would recommend? I’m also really open to advice or suggestions on what topics I need to be comfortable with.
 Thanks sub!

",Statistics Text Books/ advice for prospective Masters student,8xoh7a,new,12,17,17,0
"Hi,  


I've been given two data sets containing patient data on different batterys of neuropsychological tests.  The first has about 80 patients with data on 25 variables, and the second has about 170 patients with 22 variables per patient.  Each of the data sets is quite patchy with a lot of missing data.  There isn't much over-lap in the measures used for the two data sets, so we can't combine them.   


My lab is interested in finding out if it is possible to create 'composite' scores from these data, i.e. can we condense the data down into a smaller number of 'domains'?  If we can do that, we would then look at testing hypotheses about the relationships between those domains.   


Additionally, it would be really useful to see if there are different clusters of patients with who show different syndromes on these domains.   


My understanding of statistics tells me that the two broad approaches that can be used for this type of prolem are dimension reduction or clustering.   My questions are:

1. Which of these two types of approach would be more suitable?   Is there another more appropriate approach out there?
2. Related to this - we want to see if there are clusters of patients who show different syndromes.  As far as I know, clustering can't really do this, because it realies on a distance metric, and therefore can't deal with categorical variables like participant ID.  Is it possible to put subjects into clusters?
3. Both data sets have very small sample sizes.  The rules of thumb I have read are to have at least 10 x as many participants as variables, or even more.  Of course, this will vary greatly depending on the data you have.  Given that ours is likely quite noisy, we probably need a lot more subjects.  Is there a way to assess the reliability of your model fit, given the number of participants and variables you have?  


Thanks!",Help needed - Clustering or dimension reduction and required sample size.,8xoelf,new,5,1,1,0
"As the title  say. I  have made  a  linear regression  analisy of an event, and I  got  the concentration  of  probabilities   at  a  given space  of  the  Graphic  (X, Y)  ... so  how  to extract  the odds   of  the event to  happen   by this linear  regression analisy?","How to get the odds of an event, based on its linear regression analisy?",8xl5nl,new,7,0,0,0
"I'm finishing a stats masters and my advisors gave me a topic which I feel is extremely boring and uninspired: they want me to implement a regression model for bounded discrete data with inflation at the endpoints.

I suppose that if we were to do this via ML, at least I'd get to show my skill doing a bunch of manipulations on slightly complex equations. But in this case we'll go with Bayesian inference, so all I need to do is code the likelihood in Stan and throw the data at it. My advisors think that adding a section with a few simulations showing parameter recovery and calculating changes in information criteria for a couple of link functions should be enough.

I've been seeing some papers on model averaging and variable-selecting priors, figured it might be cool to throw that on top of the original model to spice it up, but I'm not really sure.

Any suggestions? Maybe someone who thinks the original idea is fine for a masters level thing?",Looking for ideas to make my thesis more interesting,8xjjd9,new,6,16,16,0
"I've read you can't do repeated measures with two time points. I  technically have 3, but the first time point is baseline so my two groups (treatment vs control) are only different from each other for two time points.

Thus I am not sure if mixed models makes any sense for my case. Any advice?

I was going to use ""time point"" and ""intervention"" as fixed effects and ""patientID"" as the random effect. There are quite a few response variables (some continuous, some binary) that I will be running models for and checking whether ""intervention"" has a significant coefficient.
","3 time points, first is baseline (pre-treatment). Use mixed models or something else?",8xgnv8,new,12,4,4,0
"I am currently at a biostatistics REU program where my partner and I are going to need to work with MCMC, but neither of us really know anything about Bayesian Inference...our only knowledge of anything related to Bayes is Bayes Theorem.  Does anyone know of a website/link/scientific paper that would be able to thoroughly explain Bayesian inference for someone with little to no knowledge about it?

Thanks in advance for the help! Any information is appreciated. ",Bayesian Inference help?,8xfz58,new,8,1,1,0
"Can someone tell me if my answers are correct. I feel like they are, but I’m not sure? 

http://imgur.com/mtTT8mq
http://imgur.com/7k1nfjb


Thank you!!",Population proportion question help,8xeb30,new,3,8,8,0
"From what I understand the Kendall's tau test assesses whether there is a monotonic relationship between two variables. I'm using the tau b test because I have many ties in my ranks. I have a tau b value of 0.234. From what I understand this indicates that there is a positive monotonic relationship between my variables (my p-value is < 0.05 so it is also a significant positive monotonic relationship). 

I'm wondering is there is a way to assess the strength of the relationship based on the tau b value. Because my tau b value is closer to 0 than it is to 1 does this indicate that the relationship is weak? Is this an appropriate assessment?",How to interpret the strength of a Kendall's tau b?,8xe5yw,new,4,5,5,0
"Like the title asks, I have a mediation model that shows Pathway A (X > M) is not significant and Pathway B (M > Y) is significant. Pathway C (X > Y) is also significant. The overall model is also showing *p* < .05.

I've read this might be possible but I haven't actually found anything concrete stating what is going on exactly.

Any help is appreciated.",Mediation Analysis - Model significant but one path isn't. Why,8xdbwi,new,2,2,2,0
"At my job we have several machines that output parts at an extremely low defect rate (0-1 defects in the last 2500 samples [~2 years worth] for all machines) in batches of 5-6 parts per day. 
Inspecting 100% of the parts is expensive (and they have an excellent defect rate) so they need to justify not inspecting all parts. 

Cp and Cpk are greater than 1.5 for all the machines. However, the estimation for process std. deviation is so low, that all of the processes are actually out of statistical control. Even when looking at the last ~3 months of data. So, Cp (or Pp) shouldn't be used to estimate capability. 

I'm thinking of using hypothesis testing to find the minimum sample size that corresponds to detecting a shift in mean equal to 50% of the distance from the current mean to the upper spec & lower spec limits with power of 90% and alpha of 5%. However, when I did the calculations for both, it recommended that I use sample sizes of 20-40 to detect either the positive or the negative mean shift. That's of little value to the company because they have to wait 10-20 days for enough samples to know if the mean shifted. 

Does anyone know of any resources I should look at to help me with this problem? How should I change my approach?",Developing a sampling plan for a machine with extremely low rate of defects?,8xdalo,new,6,9,9,0
" I just finished collecting data for my master thesis, which is about a new organizational culture.

I built 3 groups (high culture, low culture and control). Then I asked for age, gender, leadership experience yes / no (if yes: how many years) and educational level. After that I used different questionnaires (motivation, satisfaction....)

Beside evaluating the participants data (age...) I used ANOVAs and post hocs to analyse wether the groups had an effect on the questionnaire means and which group is significantly different from which.

My professor said the results and the calculations were good, but I should look for more statistical methods, since it would be way too simple right now. I'm really fine with adding some more, but I can't imagine which methods I should use. Could someone probably name some methods that would fit my data?

I thought about a moderator analysis, however I only have access to SPSS 24 in my university and no extensions like PROCESS (also I wouldn't know how a moderation analysis would fit to my data). I would be so happy if someone could name methods I could add!",Methods for Thesis? Improve work climate in organizations,8xd4oe,new,2,1,1,0
"Hello r/statistics! I'm not too sure if this is the right place to ask this question but thought I'd give it a shot.    
    
So we have 4 of us who are going to draft 8 teams each.  Essentially we want points for wins, ties, winning the division, and winning playoff games.     
    
I am just not sure the most appropriate scoring system we should use and what is a good balance, especially with playoffs.    
    
Right now I am thinking to use 2 points for a win, 1 for a tie and 0 for a loss.  Maybe do like 3 points for a division win.    
     
As for the playoffs, I was thinking like going 3 for the wild card, 5 for next round, then 7, then 12.    
    
So yeah, my question is what is the best scoring system. Should I have more points for regular season games, different scores for playoffs, should the #1 and #2 seeds have extra points for having byes?",What point scoring system could/should we use for our NFL fantasy game?,8xbldd,new,7,5,5,0
"Hi,
I found this Nature article https://www.nature.com/articles/s41539-018-0028-7

I was wondering if this part makes sense:
> We conducted several analyses to test for statistical differences and statistical equivalence in performance, the emergence or disappearance of differences with age, and statistical differences in variability between groups. Similarities and differences between boys’ and girls’ performance were assessed using independent-samples t tests to identify statistical differences in mean performance and Schuirmann’s two one-sided tests of equivalence to identify statistical equivalence in mean performance (similarity within ½ standard deviation (s.d.) of the group data; implementation of this test for SAT-Math scores.) Testing for both statistical differences and statistical equivalence is important. Non-significant t tests only allow us to conclude that there is not enough evidence to reject the assumption that performance is equivalent between groups. However, this does not necessarily mean that the groups are statistically equivalent. By including tests of equivalence, we can determine whether the lack of a significant difference between groups reflects statistically equivalent distributions of scores between groups. To date, tests of equivalence have not been conducted on data on mathematical abilities in early childhood, but these tests are especially important for informing the “Gender Similarities Hypothesis.”


My question: is it really better to test both t-test and statistical equivalence? Why?
The way it's phrased sounds like ""we can only reject hypotheses so we need also to test the opposite to be really sure"", but maybe I'm misunderstanding..",Is this Nature article right about testing both similarity and dissimilarity of the mean?,8xb0f8,new,24,26,26,0
"I have fitted a Gamma family GLM on some training data and now I want to do predictions. However, the test/validation data has a different distribution/mean variance relationship than the training data. The dependent variable has, on average, higher values  in the testing data. What would be the best approach to ""inflate"" my predictions, other than do it manually? ",Increasing predictions in a GLM.,8xaqj6,new,3,1,1,0
"Hi all, 

I've never even heard of ROC until recently and I am asked to use it to analyze a previously done project. I was wondering if someone could ELI(am a grad student) or point me to some materials to read up on. 

From what I've learned on my own so far, I know that its a Sensitivity vs. 1-Specificity plot and I get all the TP, TN, FP, FN stuff. But what is the purpose and under what situation would a researcher feel like an ROC analysis is warranted? 

Thanks r/statistics! 

I feel like I have a lot more questions but I don't know what to ask at this point. ",Can someone help me get a better understanding of the ROC curve and its applications?,8x6soo,new,14,52,52,0
"I'll start off with stating my primary long term professional goal, which is to have statistics work be one of my primary work responsibilities, primarly focusing on applications within the environmental/geoscience field and spatial statistics.

I have a MS in geology and have worked in environmental consulting since 2011 (first few years were mainly field work). I am currently learning python (Introduction to computer science and programming using python through edx) and R (going through the text Applied Spatial Data Analysis with R).

How much do real projects matter when applying to positions where statistics is a major job responsibility. Projects would include projects worked on at my current job (environmental consulting) and independent projects (Kaggle or similar).

My MS and BS are in geology, neither of which required a heavy dose of math. Should I consider a statistics graduate certificate (Penn state has one) or just focus on my work and project experience.  thanks. I know I will also have to spend some time building up my math skills, but I don't have time right now with learning python and R. ",Are there any employment benefits to getting a statistics graduate certificate if I don't have a degree in statistics or math?,8x6cn8,new,4,0,0,0
"Hi, I apologize if this is the wrong sub for this question, but it seemed more appropriate than /r/datasets, as the question I want to ask is really more statistical in nature (I think).

For my dissertation, I'd like fairly fine-grained (one datapoint per day) data about search trends for certain companies over a number of years. Using an [unofficial python API](https://github.com/GeneralMills/pytrends), I can download this data from Google Trends automatically.

The first problem is that as you widen the time range requested, Google automatically coarsens the data it gives back to you. For example, if you request a single day, you'll get back hourly data, and if you request several years, you'll get back monthly data. So what's the problem with that? Just request one month at a time to get the fine grained data, right?

Well the second problem is Google normalizes the data so that the largest datapoint in the range it gives back to you is 100, and the other datapoints are relative to that value (a percentage). This means that datapoints from different ranges are incomparable, as they have different maximum values.

My idea to solve this second problem is to overlap the data I request, for example request January 1st-February 1st, then January 15th-February 15th. Using the half month of overlap, I should hopefully be able to figure out the relative scaling between the two ranges by comparing the same day under different ranges. If January 15th has the value 81 under the first range, and has the value 67 under the second range, then clearly we should be able to map the second range back to the first using the ratio 81/67.

The third problem is I think Google fuzzes the data a bit, as slightly different data is sent back if you repeat the same request, and the ratios of each of the overlapping 15 days always slightly varies.

The question I want to ask is: if I chain this method to create a running multiplier (mapping the third range back to the second, then the second back to the first), will the error rate eventually spiral out of control as to be unusable?

I have some very basic background in statistics, but my head seems to be having a lot of trouble reasoning about this.

Sorry for the long question!",Creating absolute values from Google Trends series data,8x2zc8,new,2,6,6,0
"I am currently working on a data science project where I am trying to model satisfactions vs # of items done. For example, how giving more loyalty points to an individual increases their satisfaction. I was wondering if there is anyway to model this relationship without having data? For example, is there a function known as the gold standard for modelling diminishing returns? 

  
Thanks. ",Modelling Law of Diminishing Returns,8x2uaq,new,9,1,1,0
"Hello everyone!

I have one categorical independent variable, and 2 continuous dependent variables. The sample size is pretty large, with 269 in one group and 371 in the other.

For my research, I need to first run an independent samples T-test on one of the 2 DVs, and then use a mixed ANOVA including both of the continuous variables. My problem is, I'm confused whether I should be checking for the assumptions of ANOVA even before I run the T-test? Or is  it acceptable to check for the respective assumptions just before I run each test? In checking for multivariate outliers, I'm afraid that it might be faulty if I checked for them separately each time just before I ran the T-test, and then the ANOVA.

(Help please, I'm desperate)

Thank you.",A mild doubt which is driving me nuts,8x0rdr,new,6,0,0,0
"Hello. I'm a PhD student in linguistics who's been convinced by one of my mentor to use Mixed Effect Models in my research (I've never studied stats or maths at higher level in my life). 
Basically, I want to observe the effect that various linguistic factors have on the number of ""likes"" obtained on Facebook posts. So firstly I used a software to annotate the posts. Then I created an Excel page, with the data organized in this way:
USER   POST   LIKES   AFFECT   JUDGEMENT   REPETITION   GRAPHICAL and so on. For this study, I only have two users, as I am conducting the research on two politicians. Then I have their posts, the number of likes (which would be my dependent variable) and the other independent factors organised in a ratio way.
In R, I see that my DV is not normally distributed, therefore I use  a log function. My mixed model is organised in this way, with the user (two of them, in this case, as random effect):
model <- lmer(loglikes ~ affect + judgement + repetition + graphical + (1|USER), data = mydataset)
The problem is that the residual plots don't seem to be random (see below), except for the distribution, so I'm violating an assumption for the model. I've read that log transformation is often useful, otherwise it means I'm dropping a significant factor in the analysis. Unfortunately the ignorance I have in this field is stopping me from understanding much of the literature out there. I hope that someone here can tell me whether I'm doing something wrong and what exactly.
Thank you anyway.

https://s33.postimg.cc/sifq3kncf/collage.png

edit:I'm not sure whether this had to be posted in askstatistics. If so, I'm sorry for the inconvenience.

UPDATE to sum up all the helpful advice I received here:

- I'd better use Negative Binomial glm as the mean and the variance of the DV are rather different (so no Poisson). In this way, I would also avoid data transformation.

- If I decide to keep only two users, I'd better drop the idea of random effect. Or I may include the control users in the model as well and decide to introduce a discrete variable to tell the two group apart, together with a random slope (maybe I got this last bit wrong).

- I should also include a time variable in order to better describe the data.

I'll create different models, experimenting with the different variables. To this regard, I wonder whether there's a way to identify the ""best"" model for my data. Any other advice is welcome, but thanks a lot anyway.

UPDATE 2: I'm literally going mad. I'm using NB as adviced, still I can't decide whether to use random effect, and if so how. In my opinion, I think I have to use random effect (I know, I have few levels) as the users in my study should be part of a larger population. Another factor I'd like to account are the tweets themselves. Although I added time as a variable (it seems to shape the residual plot slightly better), I'm still not sure whether my models are fit. AIC is always around 150.000, residual plots with random variables are oddly shaped (mostly all the dots are literally on the middle line). I suppose the main effect I'm missing about tweet likes...is the content of the tweet, but how can I include that in the model?
Another problem is that I can't understand what is the relationship between user and tweet. I have 2-4 users with several thousands of tweets each, so it's nested random effects?? I've created something like 5-6 models with different random effect relationship (taken from Bolker's faq), but it seems that really nothing changes between them.",Have I fitted this mixed effect model correctly?,8x0msc,new,24,23,23,0
"[https://highlyimplosible.com/sufficiency-completeness-and-unbiasedness/](https://highlyimplosible.com/sufficiency-completeness-and-unbiasedness/)

Here is my take on understanding the UMVUE (Uniformly Minimum Variance Unbiased Estimator)

Hope it instructs! ","Review of Sufficiency, Completeness and Unbiasedness (UMVUE)",8wzaya,new,2,1,1,0
"I am creating a survey to distribute and I am trying to decide how to best analyze my data. The first 10 questions will ask demographic information like: sex, age, profession, employment status, highest degree earned, etc. As of now the next 3 sections are divided into ""knowledge of technology,"" ""advantages of technology,"" and ""ease of use of tech."" Knowledge has 2 questions in it, Advantages has 13 questions in it, and Ease has 1. All of the questions are Likert items with a scale of 4, ""Strongly disagree, Somewhat disagree, Somewhat Agree, and Strongly Agree.""

I want to compare the correlations of the answers to the 3 sections to the demographic info. I first was thinking of doing a multiple or ordinal regression by summing each section individually then comparing them to each demographic answer, but the research I have been doing hasn't helped too much. I will be using SPSS. Without changing the format too much of the survey, what are the best ways I can run this to get the correlations between the demographics and the views of technology. Thanks!",Help choosing how to analyze my data set.,8wvsro,new,2,1,1,0
"So, I'm running an ANCOVA with 3 categorical variables and 2 covariates. One of my categorical factors has only 2 distinct groups and I know for sure that there's a difference in my response variable between the 2. I'm curious if I can drop that categorical factor from my analysis to avoid higher order interactions (which I'd drop anyways, it's not significant) and just run two separate analyses for that factor. 

To provide some context, I'm looking at growth in a species of fish and the categorical factor that I'm interested in dropping is whether or not this growth was measured during the breeding season. So...in my hypothetical scenario, I'd just drop the breeding season factor and run two separate analyses for growth (i.e., during the breeding and non-breeding season).

What would I gain/lose by dropped season as a factor? Would it just be more appropriate to keep it in and drop the higher-order interaction? Season on it's own in the model is significant.","If I've got a full model and I already know that one of my factors is going to impact my response, is there any reason to not remove that factor and run two separate analyses?",8wvskk,new,6,2,2,0
"I am interested in running a mediation analysis with 2 IVs, 1 Mediator, and 2 DVs. I am interested in relationships, not differences. So I am essentially looking for a 2x2 MANCOVA equivalent for GLM. I am unsure if this even exists, I am up for splitting the mediation into 2 separate mediation models, but I would prefer not to if a particular method exists. Any help would be greatly appreciated and I am willing to answer questions to help sort it out if need be. ",Help with Multiple Dependent Variables,8wvgf3,new,2,11,11,0
"[https://www.middleprofessor.com/files/quasipubs/change_scores.html](https://www.middleprofessor.com/files/quasipubs/change_scores.html)

A statistical breakdown of how the findings of a very highly cited and circulated Nature paper were likely bogus.",Bias in pre-post designs – An example from the Turnbaugh et al (2006) mouse fecal transplant study,8wv6c3,new,0,1,1,0
"Hi I can't find a lot of documentation on this so I was hoping someone had experience with using generalized additive models.   
  
I have data which I -expected- to have a nonlinear exposure/outcome relationship. But in fact regression trees, unadjusted plots, and GAM plots all suggest simple linearity.   
  
I read that estimated degrees of freedom can be used as a numerical test of linearity. If edf is less than 2 in a GAM, it indicates a linear parameter is more appropriate. Is this correct? I don't like the idea of writing in my analysis that I chose linear modelling just because the ""lines looked straight"".",When to reject a GAM in favour of linear models?,8wt88j,new,6,18,18,0
"In the month of  May a tennis  player  A reached  the maximum odd  of  4   every  3 matches he makes  against other players who usually statistically reached in the same month  the maximum  odd  of 3  every 3 matches..

In  june  the same tennis  A  reaches the maximum  odd of 4    every  6  matches he makes  against players that usually reach the odd of 3  every  3 matches they make

(so things changed a little better  for tennis  A)

How  can I calculate the probability that the tennis  player A when playing  with tennis players who make the maximum maximum the odd  of 3  every 3 matches, can reach himself the maximum odd of  4  when he's going to play a  match  with a tennis  player  who  reach  the maximum odd of 3  every e matches  they make?

If  I wanted  to put all those  data  into a statistical program like R,  what  should I do?  What  should  I calculate?  How could  I do?",could you help me with this problem?,8wssnu,new,0,0,0,0
"Hello! So I'll conduct a study identifying the relationship of 5 random urine samples and their different volumes towards different dependent variables. I've been told to use Factorial ANOVA, but I'm not quite sure myself. 
I'm not that sure if data coming from random independent variables can be treated using ANOVA. 
Would you guys know any good ways to treat my data? Any thoughts and suggestions will be much appreciated. Thank you!",Help in choosing the best statistical treatment,8wqhy2,new,2,1,1,0
"Hello,

I wanted to know if whether a biostatistician can work in finance, or any other quant related field besides pharma companies. Is it worth getting a phd from biostatistics or applied statistics program? What would you guys recommend for someone who wants to get a phd in statistics and wants to work outside academia? Any advice would be highly appreciated! ",Job prospects for biostatisticians,8wpw49,new,1,9,9,0
"Years ago, I was given a couple problems as part of a job interview that I still can't figure out.  


General Info:  
366 users each test 3 designs. Each user then classifies design A, B, or C as ""most preferred"" and ""easiest to use"".  


What I've determined:  
Frequency Distribution  
**Design | Prefer | Easy**  
**A** | 109 | 113  
**B** | 120 | 114  
**C** | 137 | 139

Chi Test: 0.0331  
Mode: C, C

Questions:  
Which design was preferred?  
(I'd like to say C, but the evidence suggests only a significant difference between designs, not which one is ""preferred"".)  
Please also add confidence intervals.  
(My question mainly is, how do I calculate confidence intervals for nominal data?... using the mode?)  


(\*The questions were written by someone who has published multiple stats books.)  
",Help with Nominal Data and Confidence Intervals,8wp2to,new,1,15,15,0
"Some researchers asked kids how many (0,1,2,3,4+) servings of a list of 6 foods they had for breakfast. They want to see if their health education program made a difference in healthy eating.

There's tons of other questions in the survey so I want to combine these food questions into 1-2 if possible before I run a mixed model to see whether the health ed. program is a significant predictor (there's 2 time points after the baseline survey). I'm going to tell the researchers we need to do this to avoid running too many models which would increase our odds of getting false positives. 

There's no interesting correlation between the foods so I'm not using PCA. It's rather crude (since too much of anything is no longer going to be healthy) but my plan was to to give one overall score for ""diet"", where the sugary nutrient-poor options like juice and sweetened cereal give negative points and healthy things like fruit, yogurt milk, whole grain cereal gives + points. So if someone had a donut and juice their score is -2, and if you had yogurt and 2 fruits you get +3. 

It's rather dumb (I don't really think the survey was designed well), but I'm not sure how else to do it. I'll ask the researchers if they have any input as well, since it is their study and they should decide what they want to ""see"" out of the research, but does my thinking at least make sense, and my explanation to them for why they need to think of how to combine these variables?",What is a logical way to combine semi-continuous variables (servings of food eaten) into 1?,8wljqy,new,12,6,6,0
"Random Sampling with Replacement - Expected Number of Duplicates, Triplicates, etc?

I plant trees in the City of Philadelphia. It seems to me that trees are dying in the same locations over and over. Can you help me determine if there is a location-bias or if what I am seeing could just be a result of random sampling with replacement?

There are 200 locations where trees were planted (N=200). Over the past four years, there were 147 tree deaths. There were 54 locations with no deaths, 27 locations with 1 death, 41 locations with 2 deaths, and 79 locations with 3 or more deaths.

TIA!","Random Sampling with Replacement - Expected Number of Duplicates, Triplicates, etc?",8wl7t2,new,3,14,14,0
"I'm leading a discussion on this paper in my lab next week. The focus will be on the biology but since I'm also trying to raise the bar of our statistics methods I'll also talk about the stats here. But I'm not good enough to spot errors.

[Behavioral and Metabolic Phenotype Indicate Personality in Zebrafish \(Danio rerio\)](https://www.frontiersin.org/articles/10.3389/fphys.2018.00653/full)

This is quite an interesting paper for my lab because it shows that fish raised together from the same clutch of eggs (thus all siblings) can be classified into two ""personality"" types: proactive and reactive. That's rather inconvenient since our experiments assume they are all the same, or at least their behaviours are normally distributed! 

Here, the fish were clustered based on how active they were in the 15 minutes following acute stress (being netted out of the water for 1 minute). The clustering dendogram in figure 1 shows two main groups.

The fish were then subjected to some more behaviour tests which found the ""proactive"" group was more aggressive (figure 2). Another test showed this group more frequently moved between sections of the tank (figure 3).

They then tested metabolic activity and used it as a predictor in a linear regression model, where the outcome is PC1 from a PCA of behaviours. PC1 appears to separate proactive and reactive fish on a continuous scale. This seems like an odd approach but I guess it makes sense. But wouldn't a T-test suffice?

Any thoughts on this paper or the methods they use?",Journal club: can anyone spot flaws in the statistics used here?,8wj56r,new,23,18,18,0
Hi -- I'm teaching myself to program in R and Python and was interested in validating my results via datasets / programs /output files analyzed in JMP and / or Excel. Any type of data is acceptable -- I'm mainly interested in SPC (statistical process control) but any links anyone can provide would be greatly appreciated. ,Datasets / Programs / Output Analyzed in JMP and Excel,8wgjco,new,2,2,2,0
"HI! I graduated with a math degree last year and ""wanted"" to be a data analyst/scientist, but didn't have near the skills, experience, or GPA (3.3) necessary to get an internship. I guess I liked the job description and salaries.

I'm currently a regulatory analyst for the state. Classes at any public university are free for me (up to 6 hours a term, no restrictions). I'm bored at work so I'm considering taking grad courses in statistics. If I enjoy it, I'd go on to apply to a formal program.

Am I looking at grad school wrong? If I was to be a part-time student, would my education be devalued? Additionally, I feel I'm at the back the pack because I haven't been dreaming of studying \[insert statistical topic here\] since kindergarten. I can't see myself getting funded and I don't want debt, so keeping my job is my best option.

Should I proceed?",Is a part-time masters worth it?,8wfzgz,new,37,27,27,0
"I initially posted this in r/publichealth, but I'm thinking this sub might be better to ask. Here you go:

I am at a crossroads in my career and have been offered a position as a biostatistician in public health research at a large university/hospital, and another as more of a data scientist role in market research. Completely different topics but I know I really enjoy both. Both have offered me the same starting pay but I have a feeling there is more room for growth with the data science job.

My concern is more with the biostatistician route. I have a masters degree and want to know if anyone else with a masters degree in this field has felt limited? I would be in charge of statistical analyses for a team of mostly clinicians (MDs) and some non-quantitative focused PhDs. I'm worried that this route has a ceiling which I'll reach within 5yrs or so that would require me to go back and get a PhD (which I don't want to do). Four more yrs of school making minimum wage doesn't sound appealing to me at the moment. I'd love to hear from someone who is a biostatistician with a masters degree if possible. Thanks!",Career advice as biostatistician,8wflwa,new,12,5,5,0
"Hi all. Just doing some basic 2x2 Chi-Square test for independence and came across something that I am having trouble interpreting. The chi-square test came out as NS (p =.101), but one of the pairings was significant. I believe the pairings were analyzed using Z-test for proportions. How do I interpret this? Do I just stop at the analysis once the Chi-Square is NS? I think this may be a product of the fact that the groups are incredibly imbalanced (1 of the variables has a 720 - 190 split). Is the Z-test for proportions highly sensitive? I am using SPSS for this. Thanks for your help!",Chi-Square Test Question,8wew1j,new,8,1,1,0
"What formulas or strategies can be used to tell if a statistical correlation or relationship has broken down?

For example, every time X increases by 1, Y increases by 1 at a 0.9% probability over 1000 samples. How many times would that have to not occur in order to determine that the relationship was no longer in place?

Please let me know if I need to elaborate, no formal stats education.",How To Tell If A Statistical Relationship Has Broken Down?,8wc972,new,2,1,1,0
"Hello!

I have recently been diving into count data more than usual. Both Kutner and Agresti's books are lacking detail on the different methods of handling overdispersion in Poisson mixed effects models. I was wondering if any of you have suggestions?

Quickly looking on Amazon, I found Tang's [Applied Categorical and Count Data Analysis ](https://www.amazon.com/Applied-Categorical-Analysis-Chapman-Statistical/dp/1439806241) but there are no reviews of the text.",Good Stats book on Count data,8wbjoz,new,2,4,4,0
"I was wondering if there is a web app or something where I can put in my model formula using lme4 syntax and it spits out a lovely formula with latin symbols, subscripts, and all that nice stuff.

Ive had a look around but couldnt find anything so may just stick with the good old fashion way.",lme4 syntax to standard mathematical equations,8w9ved,new,6,16,16,0
"I have always been fascinated with statistics but during my major in Engineering, we only had pretty basic courses on statistics and stochastic processes. 

Now, reading books on - Inferences, Modeling, GLMs, Bayesian, Learning, etc. are turning out to be prohibitively time and energy consuming task. 

Unlike Computer Science, statistics departments in universities are surprisingly ungenerous about sharing their resources. (fucking Canvas, et al login required everywhere)

So, please spread the joy and share slides, notes available with you or on the internet with us. Thanks.","Please share slides, notes and other resources for going beyond the introductory statistics course...",8w80rr,new,0,1,1,0
"Hi all,

I have a reasonably simple statistics question: I want to be able to determine the distribution of a variable in a dataset as part of a process of specifying a certain analysis.

The random variable represents the number of non-missing observations for each individual object (n = 702 objects); the possible range of observations per object is 0-11, but the observed range from the actual data is 1-9. 

The data is discrete (0-11 integer-valued only), the mean is approximately 6 (non-missing observations per object), the variance is approximately between 5.9-6, and the median is 7. Each of the object's potential 11 observations can take on values of 0 or 1 (or are missing).

Which distribution family does this data resemble, given the above information? How would the distribution of the above be described? I have ideas, but I'm having some trouble corroborating current theoretical coursework and applied learning to identify the distribution...",Determining the distribution of a variable,8w6q2p,new,14,7,7,0
"I have an MS in stats and have been at my first job for 3 months (biostatistician at a university). I started my stats career late in life so I'm hoping to make up for lost time (and $) by working hard and job-hopping, since it's faster than staying at my job and become a ""II"" in 2 years or whatever. I've only been at my job 3 months and I already can't help but look online at other postings. 

So I'm wondering, when can I start applying to other jobs without being seen as a flaky / unstable individual? 8 months? 1 year? 1.5 years?",When is it too soon to move on from your first stats job?,8w6ig8,new,21,10,10,0
,"I know this is quite broad, but could someone explain the reasoning behind the denominator of an independent samples t test?",8w6abr,new,3,0,0,0
"Hi all, I have two means with standard deviations:
1,236 ±143	and 1,260 ±134.
Can i just add the two means then divide by 2, then add the SD's and divide by 2? I feel like this is not right. ",taking the average of 2 means with standard deviations,8w62ml,new,11,3,3,0
"For example [538's blog post](https://fivethirtyeight.com/features/how-our-2018-world-cup-predictions-work/) on their 2018 FIFA World Cup prediction algorithm. They give an interesting problem, describe their method of modeling, and some interesting findings. I wish they went into some more technical details or had some results on how well their models actually fit, but in general this is in the realm of what I'm looking for.",Interesting blog posts on practical modeling examples?,8w620o,new,0,3,3,0
"I'm reading the chapter on GLMs in John Fox's *Applied Regression Analysis and Generalized Linear Models* and I'm having trouble understanding the notation. 

In Chapter 15.3, he describes exponential families as distributions that can be expressed in the linear-exponential form (excuse my attempt at writing these equations on my phone)

> p(y; theta, phi) = exp{((y*theta-  b(theta))/a(phi)) + c(y, phi)}

where theta is the expectation of Y passed through the canonical link function for the exponential family: theta = g_c(E[Y]). 

He goes on to write the log-likelihood of observing an individual observation Y_i:

> ln(L(theta_i, phi; Y_i)) = ((y_i - b(theta_i))/a_i(phi)) + c(Y_i, phi)

First, should the y_i on the RHS be Y_i? We haven't observed any data yet, so it would make sense that this would be Y_i. 

Second, I can't figure out why he is now subscripting theta with i. This is my thought process:

* Theta is a function of the mean of Y: theta = g_c(E[Y])

* Y is a random variable in the population, so theta is a population parameter. 

* Y_i is a single observation (one person's income) that has not yet been observed. When it is observed, it will be drawn from the distribution of Y. 

* So theta_i is a function of the expected value of Y_i: theta_i = g_c(E[Y_i]). 

* But this doesn't make sense. If Y_i is drawn from the distribution of Y, how is E[Y_i] different from E[Y]? And so how is theta_i different from theta?

I know I'm missing something but can't figure out what it is. I'd appreciate any help. ",Question on notation in Fox's treatment of GLMs,8w5q6s,new,3,13,13,0
"[distribution](https://cl.ly/3v1Y1k413I2q/Image%202018-07-05%20at%2012.29.26%20AM.png)

Looking at the distribution of the frequency of donations of a sample of individual donors, how would I figure out what distribution I am looking at? Binomial, Poisson, Exponential?",Theoretical model for sampling distribution: How do I know which one it is?,8w5md4,new,6,2,2,0
"[https://blog.unless.com/disappointed-by-a-b-testing-time-to-check-out-website-personalization-1a000b64fd4](https://blog.unless.com/disappointed-by-a-b-testing-time-to-check-out-website-personalization-1a000b64fd4)

>This means that at most 14&#37; of A/B split tests result in conversion improvements.

This sounds incredibly low?",Only 1 in every 7 A/B tests is a statistically significant winning test,8w4g1r,new,4,1,1,0
"I'm a junior doctor doing a research project and was hoping someone could give me advice on how best to analyse my data. Specifically, I'm looking to see if there is a statistically significant difference between two sets of data and not sure which test I should use - it's been a long time since I did any statistics! If anyone is interested I could PM them more details.",Need help analysing my data,8w3jtc,new,9,5,5,0
"I have a data-set, and the details are not crucial, but if it helps:  The variables are (A) country, (B) investment, (C) campaign type, (D)  external event.  Obviously A is a categorical variable, B is a  continuous quantitative variable, C is categorical, and D is a Bernoulli  variable.  I also have information about these for each year stretching  back decades.  I would like to know whether A, C, and D cause changes  in B and exactly what that causal relationship is if it exists.  

So my question is, how should I analyze this data, in broad terms?   Should I use Time Series Analysis?  It sounds like this should be a  time-series problem but given that I've never studied the subject, I'm not certain.  Is it possible to derive causal information from this data  given that it's observational?  It would be particularly great if there  were some textbook or other resource that I could read to learn about  the tools I ought to be using in a case like this.","How to analyze whether X, Y affect Z over time.",8w36iv,new,5,12,12,0
"Hey everyone, so I'm having issues figuring out exactly what I'm looking for in a study [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4191896/) . 

Specifically, --what is the percentage chance that the all-cause-mortality numbers for vegan men and vegan women are caused by increased variance from sample size reduction? 

Put another way, how much of the variance can be explained by the effect of splitting the sample into men & women vs some other factor? 

From what I can gather, n for vegan men = 2015, and 3533 for women [here](https://media.discordapp.net/attachments/459795625194815488/464080264096907284/Screen_Shot_2018-07-04_at_10.49.01_AM.png?width=591&height=669)

With the .95 CI for the base mortality of all vegans being .85 (.73-1.01) [here](https://media.discordapp.net/attachments/459795625194815488/464069021973872651/Screen_Shot_2018-07-04_at_10.04.03_AM.png?width=695&height=401)

The .95 confidence interval for women is .97 (.78-1.20) [here](https://media.discordapp.net/attachments/459795625194815488/464069075136413717/Screen_Shot_2018-07-04_at_10.04.22_AM.png?width=695&height=347)

and for men it is .72 (.56-.92) [here](https://media.discordapp.net/attachments/459795625194815488/464069040701308928/Screen_Shot_2018-07-04_at_10.04.11_AM.png?width=695&height=356)


I thought maybe this would be some type of ANOVA situation, but I'm not sure. I hope my question makes sense, let me know if I need to explain further.",Help understanding the statistical effects of sample reduction in a study?,8w2i71,new,10,3,3,0
"The question is true or false....

From a large population with µ = 10 and σ = 7 we take a random sample with N = 49. The sample mean is equal to 12. The probability of finding such a value for the sample mean (12 or more ) is equal to  .3859

I tried using z= (sample mean - population mean) / standard error, then plugging that into a z table... but I got 0.228. 

Apparently the answer is suppose to be True, and not 0.228. Why?

The answer sheet said: ""If the null hypothesis is true, than the probability of finding sample means as extreme or more compared to the observed sample mean is equal to the p-value"" 

Can anyone explain this, like.... I'm resitting my exam in a week and trying to figure this out. ","Beginner Student Prepping for Exam (Z test, or P-value?)",8w253l,new,9,1,1,0
"[https://highlyimplosible.com/finding-the-umvue-for-the-discrete-uniform/](https://highlyimplosible.com/finding-the-umvue-for-the-discrete-uniform/)

The Uniformly Minimum Variance Unbiased Estimator is sometimes trivialised. Some examples are however quite tricky!

Feel free to give me feedback! 

:)",Finding the UMVUE for the Discrete Uniform,8w2237,new,0,0,0,0
"An off quoted example of Poisson is the number of phone calls received in a period of time. Is it possible to add a further layer from the results of this? For example, if you wanted to know the probability of the age of the caller or the probability of the call duration, what further steps are required? ",Poisson distribution for dependent events,8w0ejz,new,6,3,3,0
"Hi everybody,

I have to make a forecast based on this particular set of data that ranges from 1995 to 2017 (23 data samples). The forecast if for a 3 year period (2018-2020)

[https://imgur.com/a/AyqjHIX](https://imgur.com/a/AyqjHIX)

At first I have used the expert modeler tool in SPSS which lead to the use of a Brown model. This lead to a mean average of error of around 2&#37; and a R squared of about 96&#37;. The Ljung box test was favorable.

Is there any better approach to this particular set of data?

Thanks in advance.",Suitable forecasting method,8vzx8p,new,3,0,0,0
"In my research, I am trying to evaluate the effect of the income drop (during a recent economic crisis) on general trust levels in the country. The variable that describes trust is an ordinal variable (1=complete distrust, 2=some distrust, 3=no trust nor distrust, 4=some trust, 5=complete trust). I run 2SLS regression, instrumenting the change in income by some other variable. My question is basically: **Can I use this ordinal variable as a dependent variable in the second stage (to be specific, the change in this variable)?**

One of my independent variables is also ordinal: education level (no education=0, primary education=1, secondary=2, bachelor=3, and so on). **Can I still use that variable even though the intervals between, for example, primary and secondary and secondary and bachelor are not the same?**

I would greatly appreciate any help with this, thanks a lot in advance!",Help with ordinal variables,8vzvfs,new,0,1,1,0
"I’m a senior stats major. This summer I do data analytics work by day, and I’d like to do something at night to make some more cash. Can I freelance stats advice for like $20/hr? 

I’ve helped friends out before with their stats projects. I can google stackoverflow answers effectively. What do you think, can it be done? 

I don’t want to do tutoring. 

Reasons this could work:

- I do analytics work right now 

- I’ve done stats projects in school (lol) 

- people don’t know stats and need cheap advice

Reasons this couldn’t work:

- I’m a senior in college with a sub-3.0 GPA. It’s an ivy but that doesn’t exactly scream “qualified consultant” 

- liability issues if I get something wrong (are there?) ",I’d like to make money consulting and advising students and businesses with statistics problems. But I’m a student myself. Can it be done?,8vyrdr,new,3,0,0,0
"I was doing stat questions. I wrote A which is the answer to this question. I realized later that the graph is positively skewed, not negatively skewed. As a result, shouldn't the mean be between 2 - 3?

Even when I count the data points the mean comes out to be 0.8, but not sure if I'm confusing the skewness or what. 

https://imgur.com/a/qiQgEK4","Can somebody explain this answer, please?",8vydwq,new,5,0,0,0
"Hi there,

I came across a DataCamp project about A/B testing for the game Cookie cats. You can find it [here](https://github.com/ryanschaub/Mobile-Games-A-B-Testing-with-Cookie-Cats/blob/master/notebook.ipynb). My question is why was kernel density estimation used instead of a simple test of proportions?

Z = (p1-p2)/sqrt(p(1-p) \* ((1/n1) + (1/n2))

P1 = prob control

P2 = prob prob variation

P = Overall player retention

N1 = sample size of the control group

N2 = sample size of the variation group

How do we determine when to use one over the other?",A/B testing : test of proportions vs kernel density estimation,8vybi9,new,6,21,21,0
"In my free time I’m replicating deep learning research papers that were considered breakthroughs and adding my own experiments to them. 

I’m collecting several metrics such as loss and accuracy for each experiment. 

What method is appropriate to test for statistical significance? 

For example: if I wanted to prove that a certain type of initialization results in higher accuracy - I run experiments with two different initialization schemes on multiple datasets for multiple architectures, but same hyperparameters. 

Thanks in advance! Sorry if this is a total noob question. Trying my best to learn! ",What method should I use to test for significance?,8vxcx7,new,12,3,3,0
"I am doing a large scale fitting on noisy data. I want to find the decision threshold when comparing

the null hypothesis of a multivariate normal with mu_0 and Sigma_0 vs mu_1 and Sigma_1.

I found [this](https://math.stackexchange.com/questions/1360080/log-likelihood-ratio-of-signals-with-multivariate-normal-distribution) which is identical to the test I want to perform,  but what I need is the asymptotic distribution of this Log-likelihood ratio or a strategy to simulate and find the threshold. This is probably contained in the paper talked about in the other post which I am unable to find.",Likelihood ratio of multivariate normals test.,8vw8lq,new,1,1,1,0
"I have objects that can do one of 5 things.  A, B, C, D, and E.  I have a pooled data with a distribution of these events and a percentage for each event..  I wanted to get a variance of the percentages from this pooled data and thought bootstrapping might be the way to go.  

Does anyone know of a way I could do this in R with a distribution like I have?

Thanks in advance!",Bootstrapping percentages,8vtcle,new,2,0,0,0
,Is the T-test and ANOVA just a subset of the GLM? What about Mann-Whitney and K-S tests?,8vqu7f,new,34,36,36,0
"I have a very basic question - hopefully someone can help me with it because I was not able to find the answer online.

We have a study where we measured food craving for different types of food. This was measured with a single question for each food type on a 7 point Likert scale. If we now want to compare whether our participants have a stronger craving for sweet compared to salty foods - can we use the dependent-samples t-test (providing all assumptions hold)?

We have a slight disagreement due to the fact that the dependent t-test compares means between two related groups on the **same**, continuous dependent variable.

This all makes sense if you would, for example, compare cravings for sweet food before and after an intervention. But how about in our case - where we have the same participants measured on both variables with the same format, but it's not ""the same variable""? 

In a sense, I think we could conceptualize it as measuring ""food craving"" in two different ""conditions"" - sweet and salty (although I do realize it's not exactly the same thing). And then...it would make sense to compare the means with this test. 

Am I really off track with my thinking? Or is it ok to conduct this test in our case?",Dependent-samples t-test: elementary question,8vqsal,new,1,1,1,0
"Hey guys, I just saw that there is a job opening for a Statistician at Saalex Solutions Inc. Happy job hunting! ","Statistician job opening (Temecula, Ca)",8vpn0m,new,5,0,0,0
"I ran a multiple linear regression in RStudio and got [this output.](https://imgur.com/a/mO0ypDy)

What does this p-value indicate? As I understand it, in a single regression the p-value refers to the coefficient of the independent variable, but that does not hold up here as I have two independent variables but only one p-value.

Any help would be appreciated.",P-Value in Multiple Regression,8vpk7p,new,6,0,0,0
"Hello, I am going to be starting school at the University of New Mexico in the fall, and I plan to major in stats. What are your opinions on a double major in stats and computer science versus a major in stats and a minor in comp sci? I am participating in the US Navy College Program, and I plan to make the Navy my career after school.

Thanks!


EDIT: I also recently learned that UNM offers a minor in IT with a concentration in cyber security. How does this pair with Stats and/or Comp-Sci?","Double major in stats and comp sci, or major in stats and minor in comp sci?",8vol9d,new,2,0,0,0
"I'm trying to understand LDA. I feel like I understand everything except for how to think about the between-class scatter matrix, which you can see [here](http://multivariatestatsjl.readthedocs.io/en/latest/mclda.html).

I have been creating my own numerical examples of groups with different means and computing out what between-class scatter matrix I get, and it's hard for me to understand what these outer products have to do with anything statistical...

I understand that if C is a covariance matrix and v a unit vector, that v*Cv is the variance of the projection of the data along the axis of v. But I'm struggling with this between-class scatter matrix.

Any thoughts?

Thanks.",Understanding the between-class scatter matrix in LDA,8vn2eh,new,0,0,0,0
"Anyone a or know someone that is a working self taught statistician or statistical programmer?  


I come from a programming background where employers can't being to care about your university degree.  They only care that you can code for them.  Is it similar in statistics?  I'm having a change of heart and wanting to become a statistical programmer.  Due to life reasons, going back to school will not be easy.  School is crazy expensive too.  Wondering if this is a task I can accomplish on my own?  Or should I be eating rice and beans and saving every penny?",Self Taught Statisticians or Statistical Programmers?,8vmfge,new,19,27,27,0
"A little background: I'm doing a project on analyzing the concentration of an inflammatory marker (MMP-9) in tears of patients with diseases such as Dry Eye, Graft Versus Host Disease (GVHD), and Sjogrens Syndrome (SJS). We're aiming to analyze the degree of MMP-9 reduction after initiating prosthetic lens wear in these patients. 

As a side note, patients that have GVHD and SJS are rarer than patients who have a simple diagnosis of Dry Eye. 

I have preliminary results, but by no means statistically significant, just shows a decrease in MMP-9 concentration across a number of diagnoses. 

I've run a batch of the samples I've collected thus far and want to know:

1. How many GVHD and SJS patients do I need to enroll for there to be statistical significance in the decline in MMP-9 concentration after lens wear.

2. Do I have to do some fancy backwards calculations to make my results significant and calculate the sample size of GVHD or SJS patients needed?


Thank you for your help and guidance, I appreciate it.

Edit:

Thanks for the feedback and advice everyone, you've been extremely helpful and while I understand to the statistics community my methods and or requests may be fundamentally flawed, I apologize for my apparent ignorance in this area. My back ground is in surgery and this is the first of this kind of study I've really embarked on, so its definitely a learning experience for me....we're very interested in the patients we've seen thus far in our study and wanted to enroll more....this is not a RCT for those pointing out the flaws in my inquiry. 

",How do I go about calculating sample size needed for statistical significance.,8vlubu,new,20,7,7,0
"At my workplace, we do A/B tests of our web sites all the time with Adobe Target. We usually run tests until we reach 90&#37; confidence ([whatever that means](https://docs.adobe.com/content/target-microsite/testcalculator.html)). Sometimes we get clear winners, like 30&#37; increased clickthrough rate. However, we often get results under 5&#37; that I think should be disregarded as noise, but everyone else I talk to thinks those results are just as meaningful as a 30&#37;. To me, the closer a result is to 0&#37;, the less reliable it is.

A good example of that is a recent test we did. With 70,000 results split 50/50, we got a 1.02&#37; reduction in conversion rate (-2.46&#37; to 0.41&#37;). That tells *me* that there's almost certainly no difference between the test and the control, so we can choose whichever one we prefer to implement permanently. However, other people I talk to act as if we'd be losing 1.02&#37; of our conversions if we chose to go with the test version.

Can anyone recommend a few good articles explaining the problems inherent with low-result studies? Or maybe I'm totally wrong. I'm a web designer, not a statistician. 

Thanks!",Can you recommend a good (but simple) article explaining why studies showing very low results are probably just measuring noise and shouldn't be taken as fact?,8vk1t1,new,5,1,1,0
"Im pretty new to time series analysis, and as far as analytical problems go, im also relatively new to more ""statistical"" models having worked in a more ML/ computational capacity before. 

As far as I can tell GBTMs and LCMMs are essentially the same? If anyone has experience working with both/either and explain how I should decide which to use would be much appreciated. 

Or are they completely synonymous?",Growth based trajectory modelling (GBTM) vs Latent Class Mixture Modelling (LCMM),8vjtlw,new,0,0,0,0
"I am looking at some data right now, with costs broken out by demographics.  That is, demographic attributes are row-based groupings with the average cost for the group as a column.  For example:

|Gender|Age|Cost|
|:-|:-|:-|
|Male|18-24|$1,000|
|Male|25-34|$2,000|
|Female|18-24|$750|
|Female|25-34|$1,500|

The major concern is that if I want more attribute breakouts, we start to create groupings with small sample sizes and, therefore, unreliable statistics.  However, I would like to get the highest level of granularity and simply aggregate as needed.

**Assuming we will be provided the MIN, MAX, MED, AVG, # of Obs, and perhaps the 90th percentile for each grouping, is this sufficient to then be able to calculate a weighted-average aggregate?**  For example, finding the weighted-average cost for all Males vs. Females, regardless of Age bracket.

EDIT:  I understand that a basic weighted-average mean can easily be done here.  The primary concern is that the 90th percentile figures per grouping rely on the distribution of the group.  Can we truly aggregate 90th percentiles of different-sized groups to find a total 90th percentile number?",Using subgroup sample statistics to infer aggregate group statistics,8vj9xm,new,17,4,4,0
"Hope that title wasn't too confusing. Anyways...say I'm interested in how a covariate might be influencing my response variable. Besides running it as an ANCOVA, I've also heard that you can run a regression of the covariate with the response variable. Then you can use those residuals from that regression as the response variable in an ANOVA. What's the difference between the two analyses? When is one preferred over the other? Is one typically more appropriate?",What's the difference between running an ANCOVA and running an ANOVA with the residuals (of the covariate) as the response?,8vfiil,new,1,2,2,0
"Say you are collecting strawberries to fit into a pint box.  You fill it to the brim, until there is room left for one more.  You search around to find the biggest berry that will fit without going over the brim.

If you wanted to use this sample to calculate the average size of a strawberry from this field, would you be better off dividing the sum of berry sizes by (n-1) rather than n?  Given that the last berry's size was not free to vary, but rather was determined by the size of your scoop?  ",Question about degrees of freedom in calculating variance using an analogy,8vfa64,new,0,2,2,0
"A few months back you guys seemed to like [my first video](https://youtu.be/b3HtV2Mhmvk) where I went through the four simplest spatial regression models. [Finally, here is my next installment](https://youtu.be/MbQ4s8lwqGI) where I show how to estimate all of the main flavors of models(Manski, Durbin, Durbin Error, SARAR, SLX, Lag, and Error), and talk about model selection ideas and tests (Likelihood Ratio).

I have a download link in the video description with commands and datasets so it is easy to follow along, and a couple of handouts (a command reference and a guide to the nesting of the models).

As always, I welcome your feedback, criticism, and questions or requests. Cheers!",My Second Video on Spatial Econometrics in R,8vf0si,new,7,34,34,0
"I'm not much of a stats person but I design games and realized I don't know how to solve the following:

I have a box that gives out 5 different items with varying chances. Example: A=10%, B=20%, C=15%, D=50%, E=5%.

I need 3 item As, 2 item B's, and 1 item E to do what I want. How do I calculate the average number of times needed to open the box in order to get what I need?

Thanks in advance",Help understanding process for solving this problem,8ve7ez,new,5,1,1,0
"For example, I'm working with some proportion data right now. How do I know if I need to Logit transform it or if it's okay as is?",How do I know when I need to perform a transformation on my data?,8vdnhy,new,5,1,1,0
"Hello guys, I really need your help. I'm using this plot to check for interobserver agreement of a ultrasound methodology to detect organ stiffness. 
I produced the plot on SPSS. Actually you obtain this plot on SPSS by doing analyze --> compare means --> 1 sample t test. Then you select the ""difference"" in the variables and put ""0"" in test value. I obtained this data:
Mean = 0.3824
Std. Deviation = 5.72
Std. Error of the Mean = 1.14
Sig. (2-tailed) 0.741
95% C.I. of the difference: lower -1.98 upper 2.7
After this step, you create a scatter graph and create manually the plot. I have to put an explaination of this results, but I don't know were to start. 
Can you help me? ",Bland-Altman Plot,8vdcj7,new,0,1,1,0
"Hey, thanks in advance,

I've analysed some data with a binary logistic regression and produced logits and a p-value with Chi-distribution, but the holes in my knowledge are making it difficult for me to understand what I've actually done.  I don't know how to explain what a logit is to someone else in relation to odds ratios.  I'm also trying to explain the relationship of the independent and dependent variables but I don't know what values I need to describe it.  I would like to plot the logistic function, but I only have the logits, the variable coefficients for the logits, the log-likelihood, and the P(X).  

I know that I've done a fair bit of work but I can't explain what the work means, so in explaining this, assume I don't know anything about this.

Much love,

A hopeful future researcher","Don't understand Logistic regression, can someone explain logits, odds ratios, and plotting?",8vd95q,new,8,41,41,0
"Up until my chapter three through quan I and II I was told I could use two validated surveys with no issues. Now a committee member is saying, since my audience has never been surveyed I have to go through validating the surveys. Both surveys have been used for decades and have tons of data. I simply want to compare two surveys - how do I go about doing this in a statistical manner?  I am sitting here in tears as I thought I was about ready to go to defense [we defend twice at my school - 1st prior to data gathering and then final] and now I have hit a brick wall.","Need advice, please. I am a non stat person trying to do chapter three of proposal.",8vb4vd,new,5,6,6,0
"Hi!

I am looking for some reading recommendations to brush up my statistics from basic to more advanced concepts. I'm currenty in my Masters and have followed most of the courses offered but I feel I am still way of of completely understanding everything. I know how to work with, but do not always understand the underlying theory. If possible the book should range from linear models (LM, GLM, LMM) to bayesian statistics. Tone of the book would ideally be academic yet easy to read.",Brushing up,8v9ofr,new,7,19,19,0
"As a long-time user (and lecturer) of R, I am wondering why Python seems to be so widely used in statistics.

I love Python, it's a great language. Use it all the time. But I just researched how to run a logistic regression (to do something that's a bit more difficult than just OLS) in Python and it was surprisingly complicated. In R, it could hardly be easier: Just use multinom from nnet and you can immediately use all the standard idioms that one knows from so many other kinds of models.

I then tried to look up arguments in favor of using Python when it comes to statistics, but they were surprisingly superficial. For example, it was often said that Python allows doing *more* than stats in one script. While true, is that really *better*?

Why is Python so widely used? It seems like R in conjunction with dplyr and other useful packages is vastly superior when it comes to stats. (Even though I love Python in general.)",Python vs. R,8v4zp4,new,87,73,73,0
"""The most efficient estimators are the ones with the least variability of outcomes""

What is meant by the least variability of outcomes. Which outcomes? 

Please, explain it with easy example.

NB: I am statistics newbie :)",Most Efficient Estimators,8v3db8,new,10,18,18,0
"Hey, this is my personal portfolio website ( just wordpress :P ) and i am very happy to finish this. I uploaded some of my data analysis projects and my personal CV. I would like to share it with the community for some advice and feedback. In addition, i would like to discuss any recommended project idea. Feel free to comment either positive or negative. Peace

[https://www.antonisagg.com/](https://www.antonisagg.com/)",Personal Portfolio Website,8v1sx2,new,17,4,4,0
"https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0

A step by step explanation of how to derive the ROC curve using Python examples.",Receiver Operating Characteristic Curves Demystified (in Python),8uxbuy,new,0,5,5,0
,Is positive association the same as positive correlation?,8ux8wr,new,3,2,2,0
"I have a few questions about the concept of the dangers of extrapolation.

Does it mean statisticians  shouldn’t extrapolate at all? Isn’t the whole of statistics is to find the pattern and predict? Else, how do statisticians avoid or solve such problem?

It seems like a paradox because if one can only predict within the range, then it limits the freedom of prediction.",Questions about Dangers of Extrapolation,8uwpt0,new,4,1,1,0
"I posted this in /r/datascience two weeks ago and didn't find much useful advice.  Figured it's worth a shot posting here because this sub has more people with strong statistics backgrounds.  I’ve already decided that I’m changing jobs so please don’t give me a list of reasons to stay.

I've been employed for almost two and a half years.  Graduated undergrad in 2014 and finished a Master's in Statistics from a top West Coast university in 2016.  I was the person my peers would go to for help with statistics questions and sometimes still get hit up by old friends to help them out.  I didn't do a thesis for various university bureaucratic reasons I'd prefer not to get into, and an internship I did between years turned out to have no data analysis component at all.  My undergrad was similar.  I'd take an internship or a research position where I was told I'd be doing interesting data analysis work.  Then once I got there it would turn out that I'd have no mentorship or that actually I'd be doing some other task that requires coding but no statistics, but has some vague data-related component.  Because of this I have very good technical skills in SQL, Python, and R, but not much experience applying them to solve analysis or machine learning problems.

My dilemma is I've had two data scientist positions at medium-sized companies where I've done very little analysis.  The first one was sold to me as an analysis/engineer position, but I did one analysis in the full year and a month I was there and the rest was focused on data quality control.  I was laid off because they claimed to not have any data analysis work available for me.  My current position is awesome except for the actual work.  I've been there for 18 months and I've just done exploratory analysis and dashboarding, with zero statistics.  My current company has cool ML work, but a conversation with my boss indicated that he thinks it's a giant waste of time that doesn't help the business.  He won't let me take on ML work, even though it's one area where I have a ton of interest.  We have no procedure for internal transfers and don't see any way to obtain one without his approval.

I'm at the end of my rope and want to do more meaty and quantitative work than what I'm currently doing, which to be blunt, bores the living shit out of me.  I'm a statistician by trade who has done no statistics in two and a half years years.  However job hunting has been quite difficult.  There don't seem to be many modeling focused positions even when I look for other job titles like ""Applied Scientist"".  I've been heavily targeting particular companies I want to work at and getting referrals, but have been rejected at the resume stage for many junior-level roles where I'd be doing modeling.  I've had my resume looked over by a bunch of people and none of them see anything wrong with it.  Because of my lack of professional statistics experience, my resume focuses on my ability to work with a diverse variety of professionals, gathering requirements, and presenting deliverables in a way that's useful for them.  Essentially all of the soft skills I'd expect to use in a more standard data scientist position.

Based on what I've indicated here, what kinds of positions should I be looking for?  Should I be targeting positions where I'm doing modeling and predictive analytics positions, or is my lack of experience in those areas going to fuck me over?  If I shouldn't be targeting those roles yet, what kind of roles should I be targeting to get there?  I'm currently targeting large companies because I feel like the bureaucracy will allow me to get experience precisely in what I get hired to do, rather than getting sidetracked with things like ""Oh I'm a senior person you do data please do this data task that has no analysis or statistical component.""

Additionally, how can I convince recruiters that I should be hired to do explanatory or predictive modeling work?  I'm in touch with a recruiter at a large tech company who tried to sell me a Data Scientist position that turned out would be all A/B testing and dashboarding, which would be my personal hell as a career.  When I asked him about potentially taking on a meatier role building models he said I should be looking at more basic positions that are similar to the A/B testing one.  I've had back-and-forths on LinkedIn with recruiters for some other positions that are advertised as Data Scientist positions, but when I ask how much of the role involves pipelining and wrangling for tasks other than doing data analysis, I almost always find out that they're doing a bait-and-switch and it's actually a 100% engineering role.

I'm quickly approaching the three year mark where junior careers usually end and feel completely stagnant and like I have nothing to show for it.  I have job security where I am now but if circumstances change, it's going to be far more difficult to continue on an upward career path than it is now.

If it helps I'm in the Greater Seattle Area, living downtown and working in Redmond.  There are some Meetups that look potentially nice but they're all sponsored by boot camps and the last time I went to a talk at one of them, it was basically a pitch for why you should give them money.  I have contacts at many large companies in the area, and those contacts can vouch for my enthusiasm and skill around statistics.",Currently employed as a data scientist but am worried because my statistics skills are deteriorating. What should I be doing to find a new role?,8uw5c2,new,65,90,90,0
"We are working with survival data obtained from a hospital for patients with a deadly disease. 

The data was obtained by observing all patient visits with the disease at a hospital over a 3 year period. However, many patients were diagnosed with the disease prior to the start of the study period. These patients are followed for the 3 year period (or until death). Naturally, it's reasonable to assume that there were also some patients with the disease who died prior to the start of the study, but we do not have records for these already-deceased patients.

We could remove all patients diagnosed prior to the start of the study, but that approach would eliminate too much of our data.

Is there a standard approach for correcting for this survivor bias without throwing out the pre-diagnosed patient data?

",Correcting for survivorship bias in survival analysis?,8uw3gk,new,4,1,1,0
"I have a lot of predictors, and no real expectations about which ones matter. I would like to throw them all in a model and see what sticks. I know that penalized regression is one way to do this. I also know that typical stepwise regression is frowned upon. Are there any other (simple to implement) approaches?","Other than penalized regression, what are some other options for automated variable selection for the fixed effects in a linear mixed effects model?",8uuz7j,new,11,9,9,0
"My data consists of measurements of swim bout length in larval zebrafish. In total there are 24 larvae. One group of larvae (n=12) are treated with a drug. All larvae are then observed for 15 minutes simultanously but in individual chambers. 

**My research question: does the drug cause fish to have longer swim bouts?**

Here are some figures to explain the data:

https://imgur.com/a/nmvEM1z

I'm not sure if I should be collapsing the data to per-fish means. If I don't, it means I'm sampling each fish many times (median number of bouts per fish: 684). The total observations is really high: 10036 in untreated and 5655 in treated, so then my regression model gives an extremely low P-value. Hooray! Highly significant!

If I take per-fish means, I then have n=12 per group and p=0.048. Obviously it's better for me if I don't use per-fish means. Is that sensible?",Please help me understand psuedoreplication in my data,8uu6q6,new,11,2,2,0
"Hi all!

I'm trying to analyze how well one of my company's products falls within specs, but I'm not quite the best at statistics, so I figured I'd ask for help here. 

I have a column that has two different processes - extending and collapsing - and my spec limits are maximum force and minimum force during each process, so 4 data sets total. I have a data set of about 100 columns, for which I have the min and max force for both processes.

I've been doing capability analysis on each data set separately, so max-extend, min-extend, max-collapse and min-collapse. Is there a way to analyze this for each process, so extend and collapse only, given that I only have max and min data for each process? ",Process Capability Analysis with Min & Max Data,8utqbl,new,2,5,5,0
,Which websites (not subreddits) can I find some interesting statistics?,8urrep,new,4,1,1,0
"Reading Hosmer, Lemeshow and May's *Applied Survival Analysis,* they discuss methods for determining what variables to include in a proportional-hazards model . Generally the advice was that automated methods of variable selection (such as stepwise) were poor approaches to deciding on a ‘final’ model to address the research question of ”which variables produce a good model for survival?”. The process that is discussed goes through including all covariates to begin with, opting to keep variables with *p*\-values less than some threshold (0.25 if I remember), then adding variables in and out to arrive at a model that contained the variables that had small *p*\-values. It seemed that the* *p-value was the most important aspect of deciding which variables to include/exclude, although it was mentioned that one should consult with content experts/clinicians who would also be able to say which variables are important.

My question is then, does the above method of determining which covariates to include in a model for survival time data also apply for other models? I will soon be exploring the relationship with about 30+ risk factors and the odds of outcome using logistic and GEE models. Given what I’ve learned in my statistical training, relying so heavily on *p*\-values to inform the decision to keep/drop variables from a model makes me uncomfortable. My gut feeling is that I would ask the content experts for their knowledge for what variables are important in the occurrence of the outcome, and try to thoughtfully approach the task of selecting variables for inclusion in the model.

The other aspect to this research is that nobody else has done any complex modelling before, so it feels like more of an exploratory piece of work. Prior work has only done simple Chi-squared tests to see if case/control groups differ on one variable. Fitting multiple models with different combinations of variables seems inevitable, but I want to make sure I approach it sensibly with a thought-out plan, rather than just trying all sorts of covariate combinations. Do you have any suggestions for resources that address these issues? Any experiences of your own?

This also leads to a more philosophical question – if previous research has used the *p*\-value as a criterion for variable selection, and that piece of research informs the models of future research, is the model you use based on ‘previous research’ really any different to one simply using the *p*\-value criterion?",The model building and variable selection process,8urkmw,new,7,16,16,0
"Full disclosure, I don’t understand stats that well. I’m trying to figure out a problem. So if you have a 5% chance of getting your car stolen each year, what’s the odds of it being stolen within 10 years? I think I have to do cumulative probability? But idk how :( please help! ",I am an idiot and need help.,8urk4m,new,28,0,0,0
"I did a search about this program, but I wasn't sure if anyone might help me figure out the tracks/profiles that it offers.

I'm trying to figure out which track I should take, and I've zeroed down between business statistics, all-round statistics and the EMOS program. The EMOS track needs an internship which is promising if I want to get to industry afterwards, but judging by descriptions it seems like it's the kind of track meant for people who intend to work in governmental positions.

Does anyone have any experiences with *any* of the tracks of this degree?",Does anyone know about the master's program in statistics at KU Leuven?,8uqrp1,new,7,1,1,0
"Super novice here. If I understand correctly, a BF10 of 1 means that there is no evidence for either H1 or H0. Does this mean that I can't draw any sort of conclusion unless I run more participants, or can I still infer some conclusion from a BF10 = 1 (namely, that H1 is unsupported)? 

I'm interning at a lab and the PI has tasked me with investigating this issue. I've taken introductory statistics but am just dipping my toes into Bayesian stuff--sorry if this post doesn't make much sense!",Does a Bayes factor of 1 mean you need more participants?,8uq7vh,new,2,4,4,0
"Hello, I have  downloaded  R  for UBunt  but  I can't  code..  I don't  know  R language.  and  I don't  have  time  to learn...  what  could  I do?  I have ubuntu...  Is there   any paid software?  I  would like it  have to  GUI  (user interface) on   which  I can  have statistical model predictions,  without  the need  to code...  any suggestion?",I can't utilize R,8unic7,new,12,0,0,0
"Hello ! 

I am interested in constructing a Stan model where the prior distribution of some of my parameters is a mixture of two distributions (one informative and the other quite « diffuse ») so that my posteriors will be more robust. 

Do you know if this is feasible in Stan?

I checked the manual (and the internet) and all I saw was how to construct mixture models for the data, but no mention of mixture prior.

If any of you can help me...
Many thanks !",(R) Stan users - mixture prior distribution ?,8umbp1,new,12,14,14,0
"Hi everyone. I want to do an analysis to see if raters give different scores to people depending on if they are the same or different race. I have the raters and the people being rated's race, but I'm unsure which analysis to use because people get rated by different raters, and raters rate multiple people. I'd like to break it down so that I can look at males rating males/females and females rating males/females. I have 4 different races included. Can anyone help? I'd also like to do the same for gender.

If I would have to guess, I would probably have to create dummy variables that indicate if the rater is the same or a different race from the person and then run a 2 way within-groups anova with that dummy variable and the person's race as IVs with the score as the DV.",Analysis to understand if people are giving biased ratings,8ulzju,new,8,2,2,0
"Hello, can you explain to me  how is made  quantitave  analisis  of  a bunch of  data  and  which  software do you use to  create  a  prediction model   of an event?

I do data  mining,  I  have mined  a lot  of data  and information,  which   software could I use  to build  the prediction model  of  an event?  Any paid software  would  you suggest to me? 

I want  that  this  software   will build  prediction model of  an event,  trought regression  analisis",Quantitative analisis and prediction model,8ultud,new,3,0,0,0
"Hello

I am by no means a stats expert, but I am starting to get a little better.

We have an issue where we are trying to calculate nonparametric process capability (i.e. a distribution that is non-normal and does not follow the known minitab distributions).  I was told that we use a ""minitab macro addon"" that is able to use a nonparametric estimator.  When I try reading into such a thing, I find articles such as this:

[https://www.springer.com/cda/content/document/cda\_downloaddocument/9783319415819-c2.pdf?SGWID=0-0-45-1588204-p180101503](https://www.springer.com/cda/content/document/cda_downloaddocument/9783319415819-c2.pdf?SGWID=0-0-45-1588204-p180101503)

Much of the second half of this goes over my head-- can anyone explain this briefly in layman's terms?  Is this valid to do-- if so, why wouldnt this be a default that everyone uses-- if normality isnt a requirement, it seems this is a good way to get a reasonable process capability value.",nonparametric process capability (in layman's terms),8uk784,new,2,7,7,0
"Matplotlib sometimes seems as though it's sort of ' low level ' , and I'm curious about what python users here use for plotting and why. Perhaps you use matplotlib, I'm not sure. 

Thanks :) ",Python users - what do you use for plotting?,8uhny7,new,41,8,8,0
"A man writes down either ""heads"" or ""tails"" and seals it in an envelope. He had a equal chance of writing either word and you do not know what he wrote. You flip a coin. You win if the coin matches what the man wrote in the envelope, but he will not tell you until the next week. If you win then he will tell you on a random day of the week with all days having equal chances of being chosen. If you do not win then he will never contact you again.

If you have waited through 6 days of the week and not been told that you win, then what are the odds that you have won?",A little Puzzle I made. Tell me what you think!,8ughgi,new,16,10,10,0
"Hi all, I'm a new Statistics major and I would like some advice on what class I should take for my programming requirement. 

I need to take either take Java or Python. I feel like Python is more used within the Statistics field. However, ""data science"" is a growing field so I'm thinking about minoring in Computer Programming in which case I need to take Java. I have also have to take SAS and R during a later semester so I'm really sure how much a Computer Programming minor would even help me. Here are the [requirements for a Computer Programming minor](https://www.csc.ncsu.edu/academics/undergrad/minor.php) (Edit: I also have to take the SAS class this semester along with whatever Java/Python class I decide to take to catch up. Would I have any trouble taking Java and SAS in the same semester?) So what do those of you with more experience within Statistics/Data Science recommend? Thanks!

PS: I also need a new laptop should I get a Mac or Windows for someone majoring in Stats?",What class should I take?,8ug9rq,new,7,3,3,0
"

I've seen that in forecasting sequence data by machine learning method, we often apply model based on independent and identically distributed random variables. But in my realization, i.i.d's definition is:

If a, b variables is independent to each other, and p(a) = p(b), then a and b are i.i.d.

However in a time sequence, I thought that x(t+1) is not independent to x(t)! And how does the identical distribute states for the time sequence data?

Thanks for helping!
",Is the forecasting time sequence data independent and identically distributed?,8ug68g,new,4,2,2,0
"I'm in an audit class and my professor provided the following example of calculating sample size:

https://i.gyazo.com/08290818cf2b5b3ef4d382ce6ddd9432.png

I have no idea what this is trying to say.  Can anybody translate this to plain English?",Can anybody explain this calculation in plain English?,8ufu1n,new,2,1,1,0
" The heaping phenomenon

Consider the following pair of graphs from the 2011 census on literacy in India.
In the histogram of age counts there are spikes in reported age frequencies every 5 years.  Why might this be? The size and regularity of the spike is far beyond what random variation historic events or demographic Trends would suggest.

Histogram of Reported Ages, India 2011
https://1.bp.blogspot.com/-O9wiogJxULk/WzLKfDLhEuI/AAAAAAAAAT4/Qv9_kZ8OVP0I0pRbVGcliUGpZURagHoOwCLcBGAs/s320/index.jpg

Reported Age vs Literacy, India 2011
https://2.bp.blogspot.com/-_sWsRm4s20k/WzLKfPN4qNI/AAAAAAAAAT0/ruDXQ6aawtI8TneZzGPyjag3HUSDRbYvgCLcBGAs/s320/index2.jpg

(Special thanks to sriv.org for the original analysis of this)

A key detail is that these ages are reported ages instead of actual ages. Poor conditions mean many older Indians are unsure of their exact age in years so they report on a proximate age. Some of the ages will tend towards the nearest multiple of 5 because of the phenomenon called heaping.

Also consider the graph of literacy rates (Y) over age (X). The downward spikes are a result of the same heaping phenomenon as it interacts with another variable. Respondents that heap their age to the nearest five years are also less likely to be literate. Although age heaping is an extreme example, any open question that is asked is subject to heaping.

Heaping happens to any number that is not known exactly. For example, the answer to an open answer survey question on annual income is likely to be heaped to the nearest $5,000 or $10,000. Similarly, the amount someone is willing to pay for an item is likely to be heaped to the nearest dollar or price point. This is why such questions are often asked as ordinal ranges instead of open-ended questions.

Blog post mirror (Post includes additional notes on survey questions)
http://www.stats-et-al.com/2018/06/survey-notes-sensitive-information.html",Some notes on the heaping phenomenon from a recent lecture on survey writing.,8uf5sk,new,1,1,1,0
"I know next to nothing about MBI, but it sounds like it was invented to sidestep issues with low powered studies, rather than improve our ability to detect real effects. Are there flawed methods like this floating around in other fields?

>Sports performance is a difficult thing to study. There are only so many trained athletes available for experiments, and most of the measurements required to investigate human performance are time-consuming to collect. As a result, most sports science studies are small, and that means it can be difficult to tease out the signal from the noise. In 2006, Will Hopkins and Alan Batterham published a commentary proposing a method for making meaningful inferences in such situations.

>Their method, “magnitude-based inference,” or MBI, was controversial from the start. It was rebutted in 2008 by two statisticians who concluded that it was generally unreliable and represented an improper use of existing statistical methods. In 2009, the flagship journal of the American College of Sports Medicine, Medicine and Science in Sports and Exercise, published a set of statistical guidelines for the journal that included a description of MBI, but the journal published it as an invited commentary after peer reviewers would not agree to accept it. Since then, MSSE has published two critiques of MBI that concluded the method was too flawed to be used (the most recent of which arose from reporting by FiveThirtyEight). Now FiveThirtyEight has learned that MSSE has decided to stop accepting papers that use MBI.

https://fivethirtyeight.com/features/a-flawed-statistical-method-was-just-banned-from-a-major-sports-science-journal/
","""Magnitude-Based Inference"" banned From a major sports science journal",8udr9h,new,17,51,51,0
"I have some data whose dependent variable are descriptive adverse events. What type of analysis would be appropriate for this? The independent variables are things like drug dosage, age, gender. 

Would Chi square make sense for this ? ",What category would adverse events fall under?,8udf4t,new,6,4,4,0
"Hi, I'm planning on transferring to a 4-year college. However, this school does not offer biostatistics/epidemiology at an undergraduate level. Both are only offered at a graduate level so I've decided to obtain my bachelors degree first. Which major/minor combination will be the most beneficial for a career in biostatistics/epidemiology? I have an equal amount of interest in all of the choices.

1. Human Biology major with Statistics minor

2. Statistics major with Biology minor

3. Biology major with Statistics minor ",Choosing a major/minor as an undergraduate for biostatistics/epidemiology...?,8uca8l,new,9,4,4,0
"hi there! if anyone can pls help me..

i am doing a project to look at group differences between treatment vs no treatment.

for one of my outcome variables (mood), it is NON normally distributed. we also have a significant covariate-age.

so when I ran the Mann Whitney U test (which should adjust for non normality since its nonparametric test), i got an insignificant result for group difference on outcome variable: mood

then I ran an ancova with the age covariate controlled for and now my result is significant. 

but then my stats advisor told me to transform the mood variable with log (x+.01) and re run the linear regression. i now get an insignificant result. And even w the transformation, the pp plot shows skewness still. 

so stats advisor is being super MIA and I am working on a deadline here. 

Do you guys happen to know what I should do? Am I ok to report the ANCOVA or no, because ancova doesnt take into account for non normality? Do I need to report the linear regression on the transformed mood variable?  Do I need to somehow take out the outliers (i believe I have 2 values that SPSS deemed outliers) ? but then how would I report that in my finding, is that like lying or fudging data?? I don't want to do anything wrong here. ",ancova vs linear regression for skewed data (SPSS),8ubpbj,new,3,0,0,0
"I'm doing a problem that gives me an ANOVA table. the SS and df sums are easy to figure out but I have no idea how to do the mean of squares because it says the it's the SS for that category / k-1 or n - k but I have no clue what k is. This is an online course and even with the help me solve this, it doesn't say where to get k from and I'm trying to do it now in stacrunch for f-test but I have no idea how to even do that. Please help, this is my final homework before an exam and my Professor wouldn't even get back to me by the time I have (Sunday) to do this homework and the exam (This homework isn't on the test but I'd still like to get a good grade. If someone could tell me either how to fill a ANOVA table in statcrunch or to do it by hand I wouldn't mind either way, I'm allowed both through shitty Mathlab.",ANOVA table help in Stacrunch,8uatm4,new,11,3,3,0
"I’m practicing for my stat exam and I need someone to solve a couple of questions from an older stat exam so I can check if i did it correct.
Edit: Added links to exam questions
https://ibb.co/hGWnjo
https://ibb.co/k5iNH8
",Need help with couple of stat exam questions,8u8zw1,new,2,0,0,0
"Hi r/statistics

A supermarket near me has an action where you can collect 160 different trading cards. That got me wondering, how can I check if each of these cards occur a similar amount of time and the supermarket is not holding a few back? I have collected +- 2600 cards and counted the amount of occurrences of each unique card.

Thanks!",Checking distribution of cards,8u8vhm,new,5,9,9,0
"I have a friend who is very much into sports betting and believes that he is able to consistently beat the house and earn a profit. I'm looking in to testing his claim and thus he has given me a 3000 entry dataset of his betting history. Here's a quick abstract example of the format the data is in:

|STAKE|MULTIPLIER|PROFIT/LOSS|
|:-|:-|:-|
|200|2.5|300|
|500|2|500|
|100|6|\-100|

What kind of statistical testing could I do for the data using R? I'm taking this as a learning opportunity so all kinds of ideas (even more complicated ones) are very welcome. I can easily state by taking the sum of the PROFIT/LOSS that he has made a profit, but I'd like to test for example if most of the profits have been made by a few lucky high stake bets.",Proving that beating the house in gambling is possible?,8u7086,new,7,1,1,0
"   
I'm an undergraduate student trying to understand the relationship between ANOVA and linear models. Especially the different types of ANOVAS for unbalanced data give me a hard time. Please consider a linear model with categorical predictors A and B:

* A type III ANOVA can very easily be obtained by computing a full model containing the (effect coded) variables for factors A, B and the interaction of A and B. Then you can drop one term at a time and compute the restricted models, subsequently comparing them to the full model using F-tests.
* As I understand it, one can calculate a type II ANOVA by replacing the full model with a model containing only main effects but no interactions. Similarly as above, I would compare it with the restricted models created by dropping a main effect from the full model. Therefore this type should not be used if there is a significant interaction.

Is this correct? How would a type I ANOVA fit into this pattern?","Obtaining Type I, II, and II ANOVA using Likelihood-Ratio-Tests",8u4v4l,new,4,8,8,0
"Consider the following example. 

Its the 2028 NBA finals and Golden State is up 3-0 against the Monstars. There has never been an NBA team to come back from a 3-0 deficit. The situation for the Monstars is looking grim. On game 4 they play their asses off and win the game, extending the series. The series is now 3-1, and their hope is renewed.

I'd like to update our estimate of the Monstar's chance of winning the series. Do we now give them the same odds we would give any other team trailing 3-1, or does the fact that they started the series 3-0 affect our estimate? In other words, do all 3-1 series give you the same information about the match up or does the sequence of wins and losses affect your estimate?",Best of seven series: are all 3-1 leads alike?,8u4lul,new,18,20,20,0
handy T table calculator while prepping for my stat exams [http://statcalculators.com/student-t-value-calculator/](http://statcalculators.com/student-t-value-calculator/),T table calculator,8u3y8q,new,2,0,0,0
"Hello,

I was studying the Wilcoxon signed-rank Test and the U-Test the other day and I am confused about why H0 (null hypothesis) rejection. 

So, in short, the signed rank test and U-Test are both tests that, using ordinal data, want to show if two lists show no significant difference (h0) or in fact show significant difference (h1). 

So, you run your test and get a critical value. 

My question is: why does your test statistic have to be larger than the critical value to have H0 (which means that there is no difference) confirmed? 

While I was about to click ""submit"" I remembered something: 

In the U-Test, you take into account degrees of freedom (here, df is: sample size of one of the two samples -1 (both have the same size anyway)) when looking up the critical value. So if both lists have roughly the same sum of ranks, they're both bound to be somehwhat high, considering the degree of freedom (also after calculating the test statistic). Since we are working with sum of ranks (to be precise: sumofranks - ((n*(n+1))/2)) and IF the both lists show no significant difference, their test statistic is bound to be somewhat high. 
Thus, they are required to exceed the critical value. 
Is my thought correct?",Understanding two-tailed statistical hypothesis tests,8u3qtx,new,27,7,7,0
"I'm working with data that deals with time and several data (X, Y, Z). I am trying to forecast values of X but notice an association between X and Y. How would I go about modeling this? Given a possible dependence how would I test this association/correlation and go on to forecast? Thanks!",Forecasting dependent variables?,8u3bnm,new,6,4,4,0
"Gradient Descent is an optimization algorithm to find the local minima of a function. Code it yourself :

https://medium.com/@rohanjoseph_91119/implement-gradient-descent-in-python-9b93ed7108d1",Implement Gradient Descent in Python,8u3b7u,new,11,6,6,0
"Hello guys, i'm doing segmentation of the population of Portugal for my master thesis on GIS and I've came across this phase in a segmentation paper:

""Graphical descriptions of the 12 clusters according to the original variables. On the graph of each variable, the cluster sections and remaining sections were compared. The bar representing the frequency of each modality has been given a more intense colour when the frequency in the sections belonging to the cluster is greater than that of those that don't belong to the cluster, to make it easier to understand. In this respect, the order of variables has been determined according to Pearson's Χ^2 statistical test, used to measure homogeneity.""

After some research I wasn't able to find (or understand) the method for knowing the order of importance that each variable had to the clustering (if that makes sense).

Statistics isn't my field so if someone can point me in the right direction or advice some works i'd appreciate it, thanks!",Variable order according to Pearson's chi squared,8u35wp,new,0,3,3,0
"Hi! Anyone knows any web about sports statistics? I´m interested in serious statistics, not just the data of the newspapers.  


Thank you.",Sports statistics,8u1csv,new,2,0,0,0
" Can someone give me a 101 and/or point out resources/tutorials (videos or otherwise) for data normalization, rationalization, and transformation techniques?","Data normalization, rationalization and transformation?",8u1953,new,4,5,5,0
"Hi, I have two categorical data sets that I wish to compare. The data set consists of a probability reading vs. a letter reading. This image illustrates it with the red and green lines: https://imgur.com/a/Sf2OjOa

What suggestions do you have to compare the two data sets - the area overlap perhaps? Any other ideas? Thanks.",Comparing Categorical Data,8u0jtp,new,3,5,5,0
"Say you create a model and it turns out the residuals are correlated.  Obviously, an e(t) vs e(t-1) plot would show the correlation. But would an e vs y-predicted plot show it as well? The e vs y-predicted plot would show what the expectation and variance of the residuals are, for the range of y, but I'm not sure if it would show if they were correlated.  Any help is greatly appreciated! ",Regression: could you tell if residuals of a model were correlated given the residual vs predicted plot?,8u05nl,new,1,1,1,0
"Hi. I am extracting substance use outcomes for a meta-analysis. Currently, I plan to include bio-metric measures, which are dichotomous, in a primary analysis - this bit is straight forward. But I would like to include self-reported days of use as a secondary outcome. 

the issue is that different studies report days of use over different time periods - so one may report use in the last week, another may report use in the last month, etc. Therefore the maximum number of days of use differs between studies. 

Would standardised mean difference be an appropriate statistic to report for self-reported use?

thank you in advance!",how to include data using different time periods in a meta-analysis,8u03pr,new,0,2,2,0
[https://arxiv.org/abs/1806.08324](https://arxiv.org/abs/1806.08324),Countdown regression: new approach to survival prediction using tools from meteorology and recurrent neural networks,8tvefh,new,2,3,3,0
"Hello guys, my statistical knowledge is less than basic. I'm a newbie. I am doing a medical study (as a medical student).  I want to correlate spleen stiffness values which are a scale of value in kPa (from 10 kPa to 60 kPa) and the presence/absence of esopagheal varices expressed in 0 (absence) or 1 (presence). What is the best statistical test that I could use to see if there is a statistically significant correlation? I'm using SPSS. ",What's the best correlation test?,8ttyzx,new,16,19,19,0
"Hi,

I'm working on the data analysis for an experiment and wondering if/what type of transformation to use. I have unbounded percentage data that doesn't fit a normal distribution. Wondering if I need to transform the data because of the non-normal distribution but not because it's unbounded?

Thanks in advance!",Biostats help-Transformations,8ttphk,new,3,1,1,0
"Okay, so I am looking for help with creating an ANOVA with a Tukeys Post Hoc of the attached data, either on minitab or on SPSS

This isn't homework, its for my masters thesis, so any help offered would be great.

If any other details are needed I can try my best, but I'm at a dead end.

TIA

https://imgur.com/5FxZhf2 ",MSc Thesis data,8tt681,new,3,0,0,0
"I  have taken Calc 1 & 2. Also, I have taken biostats 1 and 2 

I plan on taking linear algebra and multivariable calc. What else should I take? Thanks! ",What should I do to prepare for PhD in Biostatistics while in MS in Epidemiology program?,8tsv46,new,4,2,2,0
Why is there no intercept in a linear regression model equation with standardized coefficients? ,No intercept in a linear regression model equation with standardized coefficients?,8ts8pt,new,7,8,8,0
"I am using General linear mixed effects models in order to test the effects of my two categorical IVs on my continuous DV. These variables are experimentally defined. I also have potential covariates which I will also be testing.

I know that a lot of people use stepwise comparison of models, starting from either a bare model, or the most complex model. I tried this and it turns out that the simplest model, without my fixed effects is the best model fit. The problem is that I have read that when one is running an experiment, the simplest model is not a model with no fixed effects, but rather a model with only the experimentally defined fixed effects i.e. my IVs. Is this then the right approach?

When I compared models with a null model which included no fixed effects, I found that it performed better than my model with my experimentally defined fixed effects. This mean that my fixed effects are not good predictors. There also seems to be no difference between the groups when I plot the results.

My question is, which model should I report? Just the model with my independent variables as fixed effects, and then show they are not significant predictors?

Thank you in advance.",Using GLM in experiment with predefined independent variables,8ts7w6,new,2,0,0,0
Can I show mathematically to someone that consistently playing the same set of numbers has a higher probability of winning over time then picking random numbers to play?,Probability and lottery question,8trsic,new,9,0,0,0
"Hello,

today in class our lecturer mentioned that we should use the G-Test when an observed value (in cell) is below 5. 
Unfortunately, he had no slides about the G-Test but I've looked it up and I pretty much understand the formula. 

Here are the questions though: 

What exactly does the G-Test do?
And: What do I do after I've calculated the G-Test statistic? I get the formula but I cant find what to do afterwards or what to do with the statistic. 

Thanks in advance",G-Test instead of Chi-Squared Test,8tq8o3,new,7,9,9,0
,What are the best pop math books with an emphasis on statistical topics?,8tq3k1,new,12,11,11,0
"Trying to identify statistical difference in testosterone levels in a longitudinal study.  Group of 12 (n=12) measured (standardized) at 15 points (k=15) throughout the year.  Been a while since I've tackled stats, so working through some of the options I have.  Just looking for some advice, pointers on best options to analyze this data (I'm using an old version of SPSS).

IV: Time
DV: Testosterone level
What's my best strategy for identifying statistical difference between the 15 time points??

Originally looked at repeated measures ANOVA, but there a two individuals with missing data points (one of 15 points missing each, so I don't want to throw the set of data out).  As well (correct me if I'm wrong), normality has to be found in each of the time points, which isn't the case with this data set, so, my understanding, assumptions of RMANOVA are violated??

Would really appreciate some direction, thanks.

",Best options for repeated measures analysis?,8tmwbt,new,5,5,5,0
"If I have a dataset which is drawn from multiple sources - part hospital records, part telephone calls, and part nurse visits - would it make sense if I said the missing data is both missing completely at random and missing not at random?   

I have a variable from the survey portion where the missingness is correlated with the variable itself, a case of MNAR.   

But some data is missing from the hospitals which I know has nothing to do with the survey, and appears completely at random.  

Can I say the data is a combination of MCAR and MNAR? It makes sense to me... but i have never seen it in the literature.","Can data be a combination of missing at random, missing not at random, mcar?",8tjz0f,new,1,11,11,0
"If I understood counterbalancing correctly I have found the right number of participants for my research.
I have 2 prototypes, that they need to test.
The prototypes has the same amount of conditions 3.
I want to reject that gender (male and female) has anything to do with the result. 
Therefor i would say its 12 participants i would need.
2*2*3 
I this correctly understood?",Counterbalancing data sets,8thidc,new,1,6,6,0
"Hello everyone.

This  is   a difficult post... take  your time  to read   please, and I am  sorry if  I have  not explained myself  very well

I would like to calculate the  average frequency of  an event  in a tennis match: For example, let's say that Rodger Federer in the most of the  tennis games he won,   he won the game in this way: 15-0, then 15-15 then 30-15, then 40-15 . I would like to calculate (and report into a graphic ) the frequency of those kind of games and all statistical parameters that could give me a better understanding of what will happen to the next game against another player and how far  he  will  be from the average frequency  of ''  the ways he used to win a game ''

I don't know if  I explained myself ... I want to calculate the arithmetic avereage of the most  ''usual way''   .. the ''frequency''  of ''how''  Federer wins his games ... for example: If he wins more frequently like this 15-0, 30-0, 40-0 ... or if he wins more frequently like this 15-0 15-15, 30-15. 40-15 and in which  ratio ... I would like to calculate the frequency  and   put it  into  a   software  that  will draw  a graphic,  and see how much the graphic   '' deviates '' from the average frequency of the way he  (usually)  win  the games

I  want to  know  also if  there  is  a  sofwatare  that  could automate  this...  without  me  making all the calculations...  thank you",Statistic software to calculate this?,8tgtzh,new,15,0,0,0
"Hi,   
I've got 10 experts who answered single time on 14 question in Lickert scale way.  Inter-rater agreement is a mess.  Kappa Fleissa=0,17; Alfa Krippendorfa=0,18.   
Any other ideas how to approach the interpretation?",Has anyone worked with Delphi panel interpretation?,8tgjv1,new,4,13,13,0
"I'm trying to draw figure 5.2 from Bayesian Data analysis 3.

The authors describe how to make the plot

>To create the plot, we β
first compute the logarithm of the density function (5.8) with prior density (5.9), multiply- ing by the Jacobian to obtain the density p(log( α ), log(α+β)|y). We set a grid in the range
β
(log( α ), log(α+β)) ∈ [−2.5, −1] × [1.5, 3], which is centered near our earlier point estimate β
(−1.8, 2.3) (that is, (α, β) = (1.4, 8.6)) and covers a factor of 4 in each parameter

I *think* I've been following that correctly, but can't seem to reproduce the plot.  If anyone has read that book, would someone mind showing me how to make that plot in R/python?
",Plotting contours for posteriors,8tfs0x,new,0,3,3,0
"Not sure if this is the best place to ask but I don't have a ton of time.

Hey, so I just make a major mistake and failed the statistics class I was taking at my community college. I need basic statistics in order to graduate college and really don't want to have to take at my university because my experience with stat there was awful. So does anyone know somewhere I can start taking it like right now to hopefully transfer credit? The easier to pass the better.",Looking for an online basics statistics class that can easily transfer to a US 4 year,8tbrux,new,2,3,3,0
"I want to learn more about statistics and probability theory, especially in the realm of gambling.

My background is basically this... I have a Ph.D. in CS and have written some Monte Carlo simulation stuff before. I get the basics of things like standard deviation, confidence intervals, and normal distributions, but that's about it. Things like t-tests, Poisson and Bernoulli distributions, p-values, hypothesis testing, and Bayesian probability are things I'm vaguely familiar with from prior use but no longer have a grasp on. Markov chains I definitely have a very weak understanding of.

I also do not have any experience with statistical programming languages like R but would like to learn.

What websites, books, online courses, Youtube videos, etc. do you think would be useful for me?",Suggestions for materials to learn more,8tb6r5,new,6,2,2,0
I had a question on here a few weeks ago and this group did a great job in educating me on answering it well.  Thanks!,Thanks /r/statistics,8tb2nd,new,1,0,0,0
"I'm trying to calculate a confidence level for a split test.  My stats knowledge is limited and I've been searching online for the answer to this with no avail.  I'm coming across examples, but I'm not sure how to fit them to my use case.  I'm hoping someone might be kind enough to talk me through an example and provide a formula I can apply to the software I'm building.

An example:

The goal is to determine which price offer produces the most value for the company.  

\- 10,000 people were shown an offer to purchase a product.

\- 6,000 of those were shown the offer with a price of $10

\- 4,000 of those were shown the offer with a price of $15

\- 600 people purchased at the price of $10 giving a value/opportunity of $1.00 (600 \* $10 / 6000)

\- 300 people purchased at the price of $15 giving a value/opportunity of $1.13 (300 \* $15 / 4000)

The $15 price provides more value for the company, but how can I calculate the confidence level in this example?",How to calculate confidence level for a split test?,8tavoh,new,6,4,4,0
"Hello,

I just started studying statistics and I've 'accidentially' taken a masters course in Statistical Programming. In this course I have to code a Linear Regression in Python but I can't use any packages such as scipy scikit or panda (numpy and random is allowed though).  I merely know how to calculate a Simple Linear Regression by hand and know I have to code one with specific requirements. I don't know if this is the right subreddit for something like this or of anybody is able to help me but if somebody was kind enough to give me at least some clue where to start at i would be very grateful!

The assignment goes as follows:

    Write a class my_reg for linear regression (i.e. $y=X\beta+e$). For initialization of a new instance should take a matrix $X$ of covariates and a vector $y$ corresponding to the dependent variable as input and a keyword argument ""intercept=True"".<br>
    The class should include methods/attributes for obtaining<br>
     - fitted values,<br>
     - the resiudals, <br>
     - $R^2$ <br> 
     - a summary of t-tests for the coefficients and an overall F-test<br>
     - methods for plotting the fitted values vs. the residuals and fitted values vs. standardized residuals <br>
     - a call method for predicting $y$ given a new $x$ vector of covariates",Linear Regression in Python,8tapg2,new,4,1,1,0
"I'm currently going some of the data camp tutorials (currently foundations of inference), but using R for statistics is not really clicking. I think I would learn better from a book that I could sit down and read, in combination with practice exercises. I've come across ""Introduction to Statistical Learning: with applications in R"", is this a good recommendation?

I understand basic statistics (basic probability, confidence intervals, central limit theorem, linear regression, correlation, etc). For some reason, I'm finding python code easier to understand, so I may consider focusing on using python for statistics. ",Book recommendations for learning statistics with R,8t9ud5,new,23,54,54,0
"Hello, I am  a noob in the statistic  field, sorry if  the  question may  seems  the classic ''noob  question''

But  usually  when  you make a  statistics/mathematical modelling graph based  on likehoods  of  an  event to happen which method   do you use?  Montecarlo analisis?  PDE?  A combination of  both?

Are   there on line  software  which  help  people  whi didn't  study statistic to  create a  modelling  graph based on data  without  having to  make   the calculation by myself ?",Modelling,8t96j7,new,3,0,0,0
"Hello,

I've stepped away from a paper for about a year and my mind is very rusty. I have a significant interaction in a path analysis but I can't figure out how to compare the means and detect where the statistical difference(s) lie. It should be very simple. The predictor has two levels and the moderator has two levels as well. The outcome variable is continuous. Any pointers? A way to calculate something similar to a Tukey's HSD? Or maybe that is exactly what I am supposed to calculate? 

TIA.",Interpreting interaction,8t5els,new,2,1,1,0
"I'll try to keep this as short as possible.

I am trying to make a compelling case for a sports-related problem using the various recorded statistics throughout NFL history.

The problem lies in the fact that it's important that these various samples are compared to the state of the NFL from which they are being taken, instead of comparing them with the contemporary setting of the NFL..

The problem being posed is that professional football has gradually grown in size, from 9 teams in the AFL in 1966 (my first sample) to 32 teams in 2018. I would like to compare and contrast the rankings of each team through a given set of years, but a 4th best rank when there were only 9 teams to a 4th best rank when there is a much larger sample of 32 teams are not equivalent at all. I've attempted to do percentile scores as I thought that would be more accurate but once again, a first place ranking with only ten teams is only putting them in the 90th percentile, whereas a first place ranking with 32 teams would place them in the 97th percentile. I still can't use these calculations to accurately compare and contrast different subjects with such different sample sizes.

So I don't know exactly what equation I need to correct this gap, or if one can even exist. I don't know if this should be a percentile problem, or one of standard deviation from the mean of the league, z-score, or something else entirely.

[Here is the link to my work so far](https://docs.google.com/spreadsheets/d/10BNVn75VLpUm_eRfj1fQt4Z37ZK_WJExsMwbe8wLlFw/edit?usp=sharing), with all of the statistics and averages compiled. What needs to be done to make the best and most accurate comparison?",How do you compare rankings in multiple datasets of varying sample size?,8t56jx,new,0,1,1,0
"the statistician in me is excited. but the statistician in me now wears a tin-foil hat for clicking that link.

https://www.fbi.gov/file-repository/pre-attack-behaviors-of-active-shooters-in-us-2000-2013.pdf/view",FBI released a report on some trends of behaviors of mass shooters. cool and frightening.,8t4hms,new,20,94,94,0
"There are many types of measures in binary classification problems. Some of them are:  

 - sensitivity (recall)
 - specificity 
 - Positive predictive value (precision)
 - Negative predictive value
 - F1 score
 - accuracy
 - etc...

I mean, I do now their definitions and how to calculate them given a confusion matrix or model results. But when should you look at one or the other to decide what to optimize for? What is your internal monologue when analyzing and presenting them?",How do you guys internalize performance statistical measures for binary problems?,8t45iy,new,1,1,1,0
Are both basically the same? I want to pursue Data Science as a career if that info is needed.,MS in Math with Stat concentration vs MS in Pure or Applied Stat. What's the difference?,8t3os3,new,5,0,0,0
"I would like to see if certain influx of certain costs (air conditioning, lawn mowing, etc) can predict an active property to go inactive. The spreadsheet I have to work with is two separate spreadsheets, one active properties one inactive, over time and they show the costs they accrue on a monthly basis. Any tips on how to go about doing this? ",I have a list of active and inactive properties that detail which costs come to each,8t3kvx,new,0,1,1,0
"I'm going to try to describe the problem without diving too much into the biology.

I did a pilot study (N of 5) and discovered that my treatment had a pretty high effect size (cohen's d was about .9-.12 for 3 different genes from a gene expression assay). From the sample size calc using 80&#37; power, I determined that a final sample size of 10-12 would be required to get statistical significance for this data.

So I repeat the experiment to get a final N of 12 and the data is not significant, though for two of the three genes, they are relatively close to statistical significance, as they were in the pilot study. I'm wondering what the cause of this could be, is this just more biological variability that wasn't captured in the pilot study or is this a type I error? 

Would it be advisable to repeat the experiment from scratch, requiring 24 more animals, and burning more reagents, or would it be better to conclude that the effect isn't really there and move on to something else?",How would you interpret non-significant results in the final study after your sample size for the final study was calculated by a pilot study? [Biological Experiment],8t28ab,new,26,4,4,0
"Hi all,

I have survey data that includes the raw survey responses as well as indexes made by other researchers who I am not in contact with. Right now I am working to recreate all of these indexes from the raw data so I can be sure my analyses have been done correctly.

I was able to complete this task for most of the scales included in my study using the codebook but a few have me stumped - It seems there was some sort of transformation that is not noted.

For example, I have a scale composed of six dichotomous survey items with 1=yes and 0=no. Normally what I would expect to see is for these six items to be summed or averaged, but this scale has a range of -5.02-5.98. It has a mean of -3.xx so I don't think it's just been standardized.

Does anyone know what might be going on here?",Could use some help reverse-engineering a measure,8t27eo,new,7,2,2,0
"Can someone explain likelihood to me like I'm a first year student?

I think I have a handle on it, but I think some good analogies would help me further grasp it.  


  
Thanks,",Likelihood ELI5,8t230k,new,21,7,7,0
"I have created a json file of every single city and data about it, including crime rate, cost of living, income, home values, etc. Would researchers buy this data for say, $20? Where would I sell this data?",Selling data to researchers,8t1h0k,new,5,0,0,0
"Users at r/math suggested I post here instead:

I'm doing research for the summer, specifically in biophysics (I'm a math/physics double major).

We are studying organelle size regulation. I have a matrix where each column is a cell, and each entry of that column is an integer (10 - 200) that represents the size of one of the organelles found in that cell. So the length of each column is equal to the total number of organelles found in that cell.

I realized today that I have a very large matrix, where each (column) vector holds information for an individual cell.

I haven't taken a rigorous Linear Algebra course yet, so I was wondering if anyone had ideas for useful applications of Linear Algebra to this situation. My data is very conveniently arranged, and it would be awesome if I could use rigorous mathematical principles to derive interesting (and new) information.",Practical Applications of Matrix Properties (Basic Linear Algebra),8svwro,new,11,6,6,0
"Hi fellow Statisticians! 

I am currently completing a Masters in Statistics and have just started my dissertation, which is on Monte Carlo Markov Chain methods. I have enjoyed learning about MCMC and other Computational Methods a lot. However, when looking for jobs there seems to be a saturation of 'Medical Statistician'/'Clinical Trial Analyst' kind of jobs which do not interest me greatly. Does anyone have any advice on what kind of jobs would involve using MCMC and any recommendations on other computational methods I should focus on honing to help take this career path.

Thanks! 

Edit: Thanks for the great advice and suggestions everyone!",Jobs which Utilise Monte Carlo Simulation,8svwr1,new,20,45,45,0
"I have two populations of objects on a line of fixed length.  Objects A are placed on the line at some frequency, while Objects B are placed on the same line in a different frequency.  How would I got about determining if either A is dependent on B or B is dependent on A or the distribution is completely random.

Thanks in advance!",How to determine if two populations are significantly close to one another (aka non-random distribution),8svls3,new,6,2,2,0
"I work as a statistician in a factory. Most of the time I create statistical reports on sales. I use invoice data. Since all sales are recorded in the company database, I'd say that I don't have a sample, but the population itself. Do you agree with this statement? As a consequence, I've decided to just do descriptive statistics and visualization. As far as I know, statistical tests and confidence interval are meaningless with population data since I know the true values of the population parameters. In other words, there is no inference. Is it correct? Am I ignoring the fact that sales can be thought as a random process of which we only observe a realization? I'm so confused. I don't even know if is it legit to use regression. I understand that if the aim is to predict, regression techniques can be used, but I don't know if there are any other circumstances in which the use of inferential methods is convenient.

Thank you in advance.",Statistics on invoice data: do I have the entire population?,8sv4ph,new,2,13,13,0
"So 3 of my coworkers are golfing tomorrow and none of them want to share a car with the random person to round out their group. They decide to settle who will share a cart with the random guy by picking straws. After the short straw is picked first, we started debating what the odds of picking the short straw had it been picked second. 
I’m convinced no matter when you pick, you have a 1/3 chance of picking the short straw. Is this correct or are the odds 50/50 after one has been revealed? ",CoWorker Debate,8suw6h,new,4,2,2,0
"I am working through the book ""An Introduction to Mathematical Statistics and Its Applications"" and I am having a problem with the Central Limit Theorem. The problem states: A fair coin is tossed 200 times. Let Xi = 1 if the i-th toss comes up heads and Xi = 0 otherwise. Calculate the central limit theorem approximation for P(|X-E(x)| <= 5).

The way I solved it:

P(|X-E(X)| <= 5) = P(-5 <= X-mu <= 5)

mu = np = 200\*(1/2) = 100

Var(X)= np(1-p) = 50

P(-5 <= X-mu <= 5) = P(95 <= X <= 105)

Z = (X-mu)/(sqrt(Var(x))/sqrt(n))

P(95 <= X <= 105) = P(-10 <= Z <= 10) = 1

To me this makes because the large value of n creates a very small standard deviation, but the book gives the same answer as simply summing over the probability from 95 to 105?",Calculating Probabilities with the Central Limit Theorem,8sudhi,new,10,4,4,0
"I am not great with maths yet but I am doing my best to learn. I have come across something that I don’t quite understand and can’t seem to reason out why it works this way.

I look at risk adjusted readmission rates for hospitals which is Observed/Expected Readmissions. 

For the year so far I have a rate of 0.65 but when I break it down by month I have 
Jan - 0.50
Feb - 0.81 
Mar - 0.81 
Apr - 0.58 
May - 0.61 
June - 0.21 (so far) 

I expect the average of those months to be About the same as the yearly rate but the average of those 6 months is 0.59

What am i missing? The numerators of each month equal the numerator of the yearly rate. Is this expected because the rate is a calculation so you shouldn’t be averaging them together anyway? Just recalculate with all the info?",Understanding rate calculations,8subo9,new,2,3,3,0
"When conducting a statistical test to see if two skewed distributions are significantly different, is it better to perform the test on the medians of the two distributions or the means of their log transforms? I'm guessing this depends on the problem at hand, but I'm just wondering in the general sense if there is a ""best practices"" approach to this. Thanks.",Better to compare the medians of skewed data or mean of their log transforms?,8suakr,new,2,3,3,0
"Hello all, i would like some advise please.  First however, id like to state off the bat that im not schooled in traditional statistics but rather quantitative psychology.  Ive recently got accepted to start data science in health sciences for grad school. So im trained in more applied statistics rather than statistical theory.

I am currently writing a paper on robust statistics and learning about the differences between OLSR vs robust regression, my question is are there any R datasets that you guys recommend in order for me to do a comparison where OLSR wont really work (outliers or heteroscedasticity) and a robust regression would?

Any advise appreciated, take care.

",Learning about Robust Statistics,8ssobq,new,10,11,11,0
" Hi guys, I'm facing [this problem](https://stats.stackexchange.com/questions/351573/calculate-probability-of-disease-appearance?noredirect=1#comment664067_351573) and I don't know how to approach it. I am thinking of Bernoulli Naive Bayes but I am not an expert, so any advice is welcomed. ",Calculate probability of disease appearance,8srxef,new,8,7,7,0
"I'm thinking about exploring post-conflict outcomes for a project, but I'm unsure if I could run some sort of model in which I have countries A-Z with variables A-Z but only for 10 years after the cessation of conflict for each country. 

For example:
Country A with all variables from 1990-2000
Country B ""  "" from 1983-1993
Country C ""  "" from 2003-2013
Etc. run as a time series regression

Would running a regression like this have low validity? Are there certain controls to put in place that can allow for this, or do I need to have data for all countries over all years covered and align them?

I googled it, but all the results are flooded with normal time series regression explanations. Thanks",Time series regression with different starting years but equal time span?,8sqx59,new,6,9,9,0
"I’m doing data analysis over something that has happened over three years. 
I want to see the most occurring spoken language among some people over 3 years. 
Should I look for the average or relative frequency while calculating? What’s the difference ",What is the difference between the average and relative frequency?,8squ5c,new,1,2,2,0
"I have computed two different intraclass correlations for some data I'm working on. Both sets have an ICC of 1. 

In Rstudio the output has a column labelled 'F'. Each data set has a different value for this. 

I'm really unclear what this value means. Any help? 

I have to decide between the two datasets but as they both have an ICC of 1 I'm stuck. 

Thanks in advance. ",ICC & F values?,8sohuh,new,0,1,1,0
"I have a linear model (excerpt below). To my understanding, to calculate the SSE would be summarize(sum(.resid\^2)), but the data camp exercise is telling me that SSE is calculated by summarize(SSE = var(.resid)). I can't understand why the equation changed from the presentation to the exercise

\> glimpse(mod\_null)

Observations: 507

Variables: 32

$ bia.di     <dbl> 42.9, 43.7, 40.1, 44.3, 42.5, 43.3, 43.5, 44.4, 43.5, 42...

$ bii.di     <dbl> 26.0, 28.5, 28.2, 29.9, 29.9, 27.0, 30.0, 29.8, 26.5, 28...

$ bit.di     <dbl> 31.5, 33.5, 33.3, 34.0, 34.0, 31.5, 34.0, 33.2, 32.1, 34...

$ [che.de](https://che.de)<dbl> 17.7, 16.9, 20.9, 18.4, 21.5, 19.6, 21.9, 21.8, 15.5, 22...

$ che.di     <dbl> 28.0, 30.8, 31.7, 28.2, 29.4, 31.3, 31.7, 28.8, 27.5, 28...

$ elb.di     <dbl> 13.1, 14.0, 13.9, 13.9, 15.2, 14.0, 16.1, 15.1, 14.1, 15...

$ wri.di     <dbl> 10.4, 11.8, 10.9, 11.2, 11.6, 11.5, 12.5, 11.9, 11.2, 12...

$ kne.di     <dbl> ",Confusion on calculating sum of squared errors in r with a linear model,8snyp4,new,0,10,10,0
"I'm going to make up an example because I think it's easier to communicate than what I really did which involves MCMC and Bayesian inference.

Assume you have data on peoples' earnings, their age, and shoe size. Assume the only relevant peoples are people aged 1-10, and the only possible shoe sizes are 1-10. These are categorical variables you use in a model.

Let's say you have a data set consisting of exactly 1 observation for each case where age <= show size. So for example, you have the income of a person with shoe size 10 at ages 1-10 (10 observations), the income of a person with shoe size 9 at ages 1-9 (9 observations)... down to a person with shoe size 1 at age 1 (1 observation). Thus you don't have any observations for people with an age greater than their shoe size, but you want to be able predict the income of shoe size 3 when he's age 10 for example. Let's say in all your observations, your salaries are all below 20,000 and you know (both from experience and the data) that for an given shoe size, a higher age makes more money.

You construct a model where log(salary) = age + shoe size, where age and shoe size are categorical variables (binary variables each to account for every possibility). You estimate coefficients and now you can predict salary for any combination of age and shoe size.

You create another model and to compare the 2 models you look at an information criterion like AIC (Aikaike Information Criterion); in my specific case I'm looking at DIC (Deviance Information Criterion). Let's say the AIC for model 2 is SUBSTANTIALLY lower than model 1.

However, when you use your model to estimate the salary of someone with shoe size 3 at age 10 like mentioned earlier using model 2, you get an estimate of 850,000, way higher than you know is possible in my fictional world where pretty much everyone is making less than 20,000. Also when you get the predictions based on model 1, you don't get any estimates above 30,000 i.e. much more reasonable predictions.

Thus I definitely can't use that model for my prediction because as a subject matter expert I know it's not feasible but from a statistical standpoint I feel at a loss because the DIC was so much lower (19 compared to 60) which is a huge difference for DIC.

TLDR; what to do when statistics point overwhelming to a model that isn't producing feasible predictions.",Model selection when AIC selects nonsensicle model,8smiua,new,3,9,9,0
"I had the pleasure to hear the results of some research today where the speaker was sharing results of an ordinal categorical logistic model.  Model was trying to guess income tiers of individuals using several independent variables and their known incomes.  It had a concordance ratio of 78% but only 64% of the guessed categories matched reality.  I’m told that the concordance ratio seems to be accepted better as a measure for goodness of fit but, as mostly a layman, I want to reach for the plain old % correct and so I’m feeling a little, uh, discord here.  Any input?",Ordinal categorical logistic regression: concordance ratio vs plain-old percentage correct,8smd8e,new,2,6,6,0
Hello! Starting to learn SQL and beginning to wonder why you would need that if you know R. Seems like you can do most things you do in SQL in R so far. ,Why use SQL if you know R,8skx2x,new,13,0,0,0
"There is a previous study in which participants made a binary decision. A follow up study is having them make a decision that taps into the same construct, but will be on a likert scale (from 1-7).

I am wondering how one would use the effect size from the logistic regression (i.e., the beta value), to conduct an a priori power analysis for a study that will be analyzed using linear regression.

In other words, what does a beta value of X in logistic regression equate to, in terms of a beta value for linear regression? Is there a sensible way to make this conversion?",How does one use a logistic regression based power analysis to motivate sample size for a linear regression?,8skq0h,new,2,4,4,0
"I am currently trying to solidify my major. I’m going into my sophomore year so it is important that I have it decided. My main issue is there is very little info to be found about what you actually DO in the real world once you graduate with your degree in whatever field. I loved the statistics classes I took in high school, and I’ve come to be very interested in the subject. However, I’d like to hear from real people out there about their jobs. So if any of you who are statisticians or work in the stats field would be willing to tell me what exactly you do at work, I would really appreciate it. Thanks.",Careers in Statistics? Any info appreciated.,8sknd7,new,8,4,4,0
"I am having some difficulty figuring out how to calculate the above. I am currently using this online calculator (link below), but cannot figure out what formula its using/how it's arriving at an MOE without having to know the population standard deviation. Thanks in advance!

https://www.surveymonkey.co.uk/mp/margin-of-error-calculator/",Calculating margin of error of sample mean when population sigma is unknown?,8sjykh,new,5,1,1,0
"Hey all, 

I am drafting materials for a study that will look at the effect of an intervention on group processes. One condition will have the intervention before the group, and the other will be group-as-usual. 

Participants will be rating the processes, and I anticipate we will have somewhere between 150-250 participants. I am imagining a total range of 25-40 groups, so around 12-20 per condition. However, I would also like to have an observer rate each group around the same variables. 

Because observers will be rating at the group-level, I will only have as many datapoints as I do groups. We will have multiple observers rating, but only one observer per group, so it could be like 10 observers are responsible for 3-4 ratings each.   
  
I assume that this data is not subject to HLM, because it's technically independent. Therefore, am I right in thinking that this just a simple t-test power analysis will help me see what effect size/N I need to detect differences? 

Thanks.",HLM and observer ratings,8sju0t,new,0,1,1,0
"Also, what minor would be useful? I'm not passionate about any industry in particular so I'm having trouble deciding.
For those of you who don't know what a BBA is, a BBA is a Bachelor's in Business Administration, meaning I have to take a bunch of business classes alongside my Stat classes.",What kind of internships would I qualify for with a BBA in Statistics?,8siy3b,new,3,1,1,0
"https://www.m4.unic.ac.cy/the-m-competitions-and-their-far-reaching-contributions-to-the-theory-and-practice-of-forecasting/

- The combination of methods was the king of the M4. Of the 17 most accurate methods, 12 were “combinations” of mostly statistical approaches.

- The biggest surprise was a “hybrid” approach that utilised both statistical and ML features. This method produced both the most accurate forecasts and the most precise PIs, and was submitted by Slawek Smyl, a Data Scientist at Uber Technologies. According to sMAPE, it was close to 10% more accurate than the combination benchmark.

- The second most accurate method was a combination of seven statistical methods and an ML one, with the weights for the averaging calculated by an ML algorithm that was trained to minimise the forecasting error through holdout tests. This method was submitted jointly by Spain’s University of A Coruña and Australia’s Monash University.

- The most accurate and second most accurate methods also achieved an amazing success in specifying the 95% PIs correctly. These are the first methods we are aware of that have done so, rather than underestimating the uncertainty considerably.

- The six pure ML methods that were submitted in the M4 all performed poorly, with none of them being more accurate than Comb and only one being more accurate than Naïve2. This supports the findings of the latest PLOS ONE paper by Makridakis, Spiliotis and Assimakopoulos.

Edit: A paper with preliminary results is now available at https://www.sciencedirect.com/science/article/pii/S0169207018300785

and the presentation slides: https://www.m4.unic.ac.cy/wp-content/uploads/2018/06/ISF2018_presentation_Spiliotis.pdf

Edit2: The test data is now available:

https://www.m4.unic.ac.cy/the-dataset/

Note: 
> A special issue of the IJF is scheduled to be published next year that will cover all aspects of the M4 in detail,describing the most accurate methods and identifying the reasons for their success. 
",Preliminary results of the M4 forecast competition: hybrid approaches and combinations of forecasting methods produce greater accuracy,8sh855,new,18,41,41,0
"Hi,

Medical physician working with my PhD here, familiar with basics statistics but considering myself in general very average at statistics (at least calculations!). Grateful for any help :) 

In our research group we are planning on launching a field trial to validate a novel technique for pap smear analysis (screening for cervical cancer; big problem especially in many low-resource areas!). This technique could potentially improve the cancer screening significantly in areas lacking adequate screening.

The research question/hypothesis is that our technique is comparable to traditional diagnosis - e.g. microscopy analysis of samples, for the detection of high grade pre-cancerous lesions.

The problem is that I am trying to calculate the amount of patients/samples needed for the study to confidently be able to say that our technique is not significantly worse than the golden standard, i.e. traditional microscopy analysis (reject the null hypothesis). 

So for the data we can assume that the prevalence of pre-cancerous lesions we want to detect is about 5% in the study population. Light microscopy, to which we are comparing our method, has a sensitivity of about 60% and a specificity of about 90% for the detection of these lesions. For the alpha parameter, the traditional 0.05 value is good, and for statistical power 80 % would probably be enough (beta = 0.2).

I apologise if the question is too simple, but for a more ""clinically"" oriented person, I'm having a hard time figuring out what would be the best way to estimate the sample size required, performing power calculations etc :) Would it make sense for example to try to compare the methods with kappa statistics, say assume that the agreement is better than moderate (k > 0.4)? 

Thank you so much if you can help explain what would be the most sensible way to solve this! Any help appreciated :) Have worked mainly with Stata, but apparently power calculators etc. are also available online..? ",Help (a dumb clinician) with sample size calculation for clinical field study,8sh7hn,new,8,6,6,0
"Hello r/statistics!

I've recently graduated with a bachelor's in Civil Engineering, and I'm considering applying for an MSc Statistics. Will my background be a deterrent in my application? Also, if you could suggest some prerequisites that'd be awesome!",Applying for an MSc in Statistics with an engineering background.,8sdtbb,new,4,2,2,0
"[Fisher, Neyman, and the Creation of Classical Statistics](https://link.springer.com/book/10.1007%2F978-1-4419-9500-1): Interesting read that tells the story of how much of classical statistics came to be and how the terms we commonly use today were first mentioned in canonical papers.","Fisher, Neyman, and the Creation of Classical Statistics",8scve0,new,2,7,7,0
"Hello, I am a total beginner at statistics. As a software engineer, the following analogy makes sense to me, so I will use it:

Imagine a very small empty flash drive has a total space of just 100 bits.

There is no file system on the flash drive.. it's completely empty.

I have a program that will attempt to set a random bit to 1.

Initially, it is very easy for the program to set the bit on each attempt.

But as the drive gets filled up, there are less and less successful bit updates.

I figured out, the formula for this is the following:

    Chance of changing a Bit = EXP(-Number of Attempts / Total Bits)

Here is my problem:

This equation assumes that we started with a completely empty drive.

But what happens if the drive already had 30 bits set to 1 already?

How would this equation be updated to take into account Used Bits?",Trying to understand Exponential Distributions better,8sc62o,new,8,1,1,0
"Last fall I too a time series course in which we predominantly used SAS over R because it has superior built in features for this type of analysis. I thought it would be a good personal project to build something similar using Shiny, and while it has a long, long, long way to go, I think I'm at a point where it's worth sharing. So if anyone would be willing to take a look at it and give me feedback, that would be great.

[https://jimac.shinyapps.io/TimeSeriesModelR/](https://jimac.shinyapps.io/TimeSeriesModelR/)

Basically it works like this:

1. Upload a .rds file with the data (I'm planning to include other file formats eventually too)
   1. If you don't have a .rds file laying around, you can use the Sunspots or Lake Huron data from my GitHub: [https://github.com/jimacDS/TimeSeriesModelR](https://github.com/jimacDS/TimeSeriesModelR)
2. Choose the series/response variable, the time/obs variable, and the length of a season
3. Use various diagnostic plots to guide making a few simple transformations
   1. Make sure to name a new variable before transforming it, the app doesn't handle errors on this step yet
   2. Something is funky with the Box-Cox plot
4. Build ARIMA models using transformed variables from step 3
   1. Make sure each of (p,d,q)X(P,D,Q) has a value, including 0, before submitting
5. Compare models using supplied output
6. Use preferred model(s) to make forecasts that are automatically plotted and are downloadable (again as .rds files for now)

Again, any feedback or advice would really be helpful if anyone has the time/interest to play around with it. This is my first foray into Shiny, so I'm sure there are a lot of ways to break the app. 

Thanks!",ARIMA Modeling Shiny App,8sbdgy,new,3,3,3,0
"Hi everyone! I'm doing a stats class and I have to make up data having to do with a topic I chose. I decided to do whether or not that giving free breakfast at schools helps with standardized test averages, my claim is that schools that give free breakfast have higher test averages than schools that don't. My combined data p-value is .0004 so I know I am rejecting the null hypothesis so does that mean that my claim was right or that it was wrong? 

Help! :(((","Help with a stats project? Rejecting the null, but what does it mean???",8sbc3q,new,13,0,0,0
"I've read posts from a few years ago when most of these MAS degrees were brand news, but I haven't seen anything recent. Does anyone here have an MAS or work with MAS degree holders? What is your perception of these programs with regards to curriculum and career prospects?",What are the pros and cons of getting an Masters of Applied Statistics (as opposed to a traditional MS)?,8sadji,new,6,6,6,0
"Greetings fellow redditors, 

I would like to to run statistical tests (t-tests, ANOVAs, correlation and regression) in a sample of about 2400 participants. Within this dataset there is a subgroup (A) of participants who are not very well studied in the current research and I would like to run some comparisons between it and the rest of the cases (subgroup B). The problem is that this subgroup A is disproportionately small and my advisor told me that there might be some ""power analysis issues"". What is this and what implications can it have on statistical tests?

Also, a colleague suggested randomly selecting 10&#37; of the subgroup B and use that for comparison. It seems quite easy to do in SPSS, but, as is often case with this piece of software, clicking the wrong buttons will give you a result that can be very misleading. What are the pitfalls I should avoid? 

Thanks!

TL;DR: 1.Independent samples of 200 vs 2200 cases - how can this be tricky when doing stat tests? 2. Randomly selecting from the 2200 is ok?",Help with confronting two independent samples (200 vs. 2200 cases),8sa44a,new,4,1,1,0
"Here's the [old post](https://www.reddit.com/r/statistics/comments/8pju48/how_can_i_learn_the_linear_algebra_matrix/e0bw73x/). 

Here are my updated recommendations. **You only need to read one book per bolded section below.**

**Introductory Linear Algebra** (i.e., starting from square one - you should cover **everything** in these books):

- [*Linear Algebra and Its Applications*](https://www.amazon.com/Linear-Algebra-Its-Applications-5th/dp/032198238X) by Lay

- [*Introduction to Linear Algebra*](https://www.amazon.com/Introduction-Linear-Algebra-Gilbert-Strang/dp/0980232775/) by Strang

**Linear Algebra, with a focus on what you need for statistics** (pursue **after** you have mastered the introductory linear algebra):

- [*Linear Algebra Done Wrong*](https://www.math.brown.edu/~treil/papers/LADW/LADW.html), Treil. I would recommend focusing on all of Ch. 1, all of Ch. 2 (skip 2.8), Ch. 3.1 through 3.5, all of Ch. 4, Ch. 5.1 through 5.4 (5.4 is *extremely* important). The only disadvantage of this book is that it isn't specifically geared toward statistics.

- [*Matrix Algebra*](http://www.amazon.com/Matrix-Algebra-Computations-Applications-Statistics/dp/0387708723) by Gentle. Does *not* cover proofs, but it is a nice catalog of methods and ideas you should know for a stats program. Chapters 1 through 3 are essential material. Depending on the math prerequisites demanded, chapter 4 is nice to know. I would also recommend 5.8, 5.9, 6.7, 6.8, and 7.7. Chapters 8.2 - 8.5 are essential material, along with 9.1 - 9.2. This includes the linear model material as well that you will find in a M.S. program. All of the other stuff is optional or minimally covered in a stats program, as far as I know. There has since been an [updated second edition released](https://www.amazon.com/Matrix-Algebra-Computations-Applications-Statistics/dp/3319648667/).

For good reference material (I wouldn't recommend trying to learn from this unless you have *a lot* of time, but it's been extremely useful as a reference):

- [*Matrix Algebra From a Statistician's Perspective*](http://www.amazon.com/gp/product/038794978X/ref=pd_lpo_sbs_dp_ss_2?pf_rd_p=1944687602&pf_rd_s=lpo-top-stripe-1&pf_rd_t=201&pf_rd_i=0387708723&pf_rd_m=ATVPDKIKX0DER&pf_rd_r=08XWF6K2CZT7PYNEMFKW) by Harville. This does not cover any of the linear model material itself, but rather the matrix algebra behind it. It is the most complete book I have found so far on linear algebra for statistics. For the most part, you should know Chapters 1 through 14, 16-18, 20, and 21.  

I haven't read [Searle's text](https://www.amazon.com/Matrix-Algebra-Useful-Statistics-Probability/dp/1118935144) yet, but I've heard good things about the first edition.

**For linear models** (pursue **after** you have mastered the introductory linear algebra. You don't necessarily need to have read the statistics-focused linear algebra texts before pursuing these linear models texts, but the stats-focused linear algebra texts are nice to have as a reference):

- [*Plane Answers to Complex Questions*](https://www.amazon.com/Plane-Answers-Complex-Questions-Statistics/dp/1441998152/) by Christensen. START ON THE APPENDICES FIRST, and THEN proceed to chapter 1.

- [*Foundations of Linear and Generalized Linear Models*](https://www.amazon.com/Foundations-Linear-Generalized-Probability-Statistics/dp/1118730038/) by Agresti

- [*A Primer on Linear Models*](https://www.amazon.com/Primer-Linear-Chapman-Statistical-Science/dp/1420062018/) by Monahan

I haven't read [Searle's text](https://www.amazon.com/Linear-Models-Wiley-Probability-Statistics/dp/1118952839) on this yet, but I've heard good things about the first edition.","Because I've had to reference my linear algebra recommendations post several times now, here are my updated recommendations.",8s9ql7,new,24,156,156,0
"Hi,

Going straight into it, I have two groups of people, one with an adverse outcome and one without.  I want to report the differences of the people in age, sex, and physiological parameters.  The project hinges on the difference in one qualitative data point associated to each subject.  I'm not sure how much this one value will differ between the two and I want to analyse this difference.  The two groups do not contain the same number of people.

What is the most appropriate test or set of tests for my problem?

I'm not an epidemiologist (clearly) and have a good high school understanding of statistics.  Thank you for your help in advance.",What is the appropriate test in comparing two groups in this scenario? My first medical research project.,8s9njx,new,7,3,3,0
"Reason I'm wondering is because I think I'll just be able to take one of them (next semester). 

For computational statistics, we're using the book: Statistical Computing with R (Chapman & Hall/CRC The R Series

For statistical learning we're using the book: An Introduction to
Statistical Learning with Applications in R by Robert Tibshirani, etc.

I found this from wikipedia under computational statistics: 

""Computational statistics, or statistical computing, is the interface between statistics and computer science. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education.""

So it seems to be having some decent growth atm, and this under statistical learning:

""Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis.[1][2] Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, bioinformatics and baseball.""

Which one would make me the most ""well rounded"". I know that generally speaking just a single course difference won't really mean much, but I'm curious. Also, would your recommendation still be the same if I was a CS guy instead of a Stats guy? (I'm not, just curious). ",What is currently seeing the most growth? Statistical learning or Computational statistics?,8s8fxk,new,6,6,6,0
"Hi r/statistics - I'm hoping for some advice about the best way to analyse some data. I'll preface this by saying I'm (slowly) learning biostatistics but I've still got a long way to go.

I'm designing a study where we're looking at a series of cross-sectional surveys (one per year) examining vaccination rates and prevalence of disease. Although we will have subject-level data, the expectation is that the prevalence will decline in both vaccinated and unvaccinated subjects, due to reduced transmission between both groups. For this reason I'm not looking at a repeat chi-squared test or something along those lines as I don't want to treat the vaccinated and unvaccinated as two separate groups.

So what I'd like to do is take a more ecological approach analyse the proportion of people vaccinated each year (independent variable) and the proportion of people with disease (dependent variable) and make some determination if the vaccination program is reducing disease in the total population (vaccinated and unvaccinated). Although there may be some of the same people each year, it is expected that year on year the samples will be largely independent. Across multiple years, I would like to determine the effect of the proportion of vaccinated people on disease prevalence (what happens to the disease prevalence as the vaccinated rates change by x%). 

Conceptually this seems very straight forward but the proportional nature of the two variables and the repeat measures across years has me doubting the best approach. I've looked at sample size calculations for proportions in two independent samples - while I can no doubt determine if there is a significant difference in proportions in two groups this doesn't seem to take into account the proportional change of the independent variable, so this seems to be missing a key component. I've been looking at logistic regression but that seems to suggest using the year as a categorical variable which, again, doesn't take into account the magnitude of changes between years. I'm not sure if proportions are suitable for independent variables in logistic regression?

I'm going to stop now as this is getting long - if anyone can point me in the right direction I'm happy to hunt down the methods paper and do some further reading to work this out!",Appropriate test for repeat measures cross-sectional study,8s85w8,new,3,1,1,0
"Just like the title says, why doesn't my negative binomial GLM have a p-value for the model...like an ANOVA/ANCOVA/etc. would?",Why don't GLM's have a p-value for the model?,8s5pw2,new,11,5,5,0
"Hopefully the last time I'm bugging you guys about school.


https://online.stat.tamu.edu/course-list/
 

I've picked out Bayesian Methods, Time Series, Multivariate Analysis, and ﻿Categorical Analysis as important electives/emphasis courses. Of Sampling, Flexible Regression, and Applied Analytics, which do you believe are most beneficial?

 ﻿

I would greatly appreciate your help. Thank you for any input!﻿",Most important topics,8s5lgl,new,2,4,4,0
"Hi all, 

I am trying to understand what type of experimental design this study is and what type of statistical analysis to use, but I'm stumped. 

There were 32 patients, 16 received real treatment and 16 received sham treatment (controls). Measurement of interest was taken as baseline (before treatment) at first follow up, second follow up, and third follow up. 

I want to find out:
1. Changes in subjects over the course of the treatment 
2. Differences between the two groups

I'm pretty sure a t test won't suffice here, but I'm not quite sure what type of ANOVA is valid. 

Thanks in advance. ",What experimental design is this study?,8s5blc,new,7,3,3,0
"I asked this same question on askscience but i figured this would be a more appropriate place to ask this. So I was looking through this survey: [http://cdn.cnn.com/cnn/2018/images/06/18/rel6a.-.immigration.pdf](http://cdn.cnn.com/cnn/2018/images/06/18/rel6a.-.immigration.pdf) where questions were asked to gauge the sentiment toward current political conditions. Specifically it asked about the approval of the policy of separating children from their parents when detained at the U.S. border. The CNN article that I was reading about this survey was stating the findings by sub group (Republican, Democrat, Independent). The entire sample size however was 1,012 interviews conducted over phone. This sample size was just barely short of the 1,068 participants that is suggested by this sample size calculator: [https://www.checkmarket.com/sample-size-calculator/](https://www.checkmarket.com/sample-size-calculator/) , assuming a total population of 325,000,000 (population of U.S. according to google) and a 3&#37; error margin (survey had +- 3.7&#37;) and 95&#37; confidence. But after googling the total population of the republican party I found a gallup poll that states roughly a quarter of the population identifies as republican ([https://news.gallup.com/poll/15370/party-affiliation.aspx](https://news.gallup.com/poll/15370/party-affiliation.aspx)) so I used the sample size calculator to test what sample would be needed to infer meaning on this sub population of 81,250,000 people (325,000,000/4) with the same levels of accuracy as before and it recommends the same sample size (1,068). Also I should note that in the survey regarding the immigration question, 25&#37; of the participants identified as republican so about 253 people. Which this is significantly less than the calculators recommended size to gauge an inference on that population right? So I guess my question is can these claims be made about the subgroups with the same confidence or would surveys need to be conducted treating that subgroup as the population and require the recommended sample size. Also is this recommended sample size accurate? why would it need the same sample size for a population 1/4 the size? Let me know what you think, thanks! ",Can subgroup inferences be accurately determined from a survey that was measuring a larger population given its sample size?,8s4vfm,new,11,1,1,0
"Hey guys,

I created an Android application that has probability distribution calculators and flash cards for several common distributions.

The app is useful for students enrolled in probability courses, or for those looking for a quick way to calculate certain metrics from probability distributions

[https://play.google.com/store/apps/details?id=com.bignerdranch.android.exampapp](https://play.google.com/store/apps/details?id=com.bignerdranch.android.exampapp)

Hope this helps a few people out.

If you find it useful, it would be great to hear some feedback!",Free Probability Distribution Calculator App for Students,8s2s1l,new,13,36,36,0
"Hi guys. Either Google is broken or IBM is really trying to hide this information :) my laptop broke and need to buy a ""new"" system that can run SPSS for the next few years.",spss 25 system requirements hardware?,8s2dst,new,4,1,1,0
"I was trying to make a regression analysis with neural net model in R language from this guide: [https://www.r-bloggers.com/fitting-a-neural-network-in-r-neuralnet-package/](https://www.r-bloggers.com/fitting-a-neural-network-in-r-neuralnet-package/) and the results are diffrent (the weights for every neuron are diffrent )  even when i am reloading the same code. As far as i analyse the problem the mean standard error is fortunetely the same every time i reload but not the same as in guide( nn:15, lm:21 vs nn:10, lm:21  the diffrence is rather significant).

So, here is my newbie question: Is there any randomness in process of training neural nets or the sample i took was so random that it change my model?",Why my results of neural net model is diffrent although i am using the same code in R,8s24d6,new,8,2,2,0
"I have a nonparametric distribution of inter-vector distance measures, between a bunch of pairs of known vectors, that form my null distribution... I would like to take a single inter-vector distance measure between a known and unknown vector and obtain a p-value for the null hypothesis that the unknown vector comes from the pool of known vectors. What would be the test for this? I've been looking into Wilcoxon tests, but they all seem to be for ordinal / paired data, which this measurement isn't.

Thanks very much for your help.

P.S. If anyone could comment on the suitability of a Kolmogorov-Smirnov with only one measurement, I would be ever appreciative.",Hypothesis test for single measurement on a nonparametric distribution,8s1sw3,new,11,1,1,0
"Hello everyone!

I need to prepare myself for an ispection of the models I created, but I don't know what to be prepared for. I try to look for ""statistical models audit"", but it just shows me how to use statistics in auditory. 

Can you help me?",How to audit a model,8s0czb,new,4,3,3,0
"Say I run a regression model, not including the order of the data points, and it turns out that the Durbin-Watson indicates a strong positive or negative trend.  I could probably conclude that the order is significant, and then I run a new model include order as an independent variable. Would the Durbin-Watson for the new model still indicate a trend, or should it be closer to 2, since the order has been accounted for (residuals independent)?","Regression - if it turns out the order of the data points in a regression model is significant and included in the model as an independent variable, should the Durbin-Watson for the residuals be closer to 2, or would it still show a trend for the order?",8rzt7d,new,0,2,2,0
"Hello everyone!

Do you know if there's a method that allows me to detect if a variable (A) gives me wrong information of a variable (B)? I'll explain myself... 

I have two variables A and B, and all of them are related to products. Variable A tells me the industry where a product is fabricated (it's a finite set), and variable B tells me which product it is (virtually infinite set of products). I know for certain that some products are wrongly classified in variable A. I want to know if there's a way for me to determine statistically which they are. 

Thank you.",Detect wrong variable labeling,8rzrwq,new,1,1,1,0
"Hi, just wondering if anyone has a good example of a research/article or post of someone running drawing conclusions from regression coefficients? 

",Any example of drawing conclusions from logistic regression (or any regression) coefficient analysis?,8rzoh7,new,7,2,2,0
"Hey guys, I have a question.

So, we have collected data for a within-subject research design. Participants perform 10 trial in each of 3 conditions. Also, in one of the conditions there are 5 versions of the trial, which are assigned randomly. So, in that specific condition a participant does 10 trials, and that could be 4x version 1, 3x version 2, 2x version 3, 1x version 4 and 0x version 5 for example. There aren't always 2 of each.

But now, we want to know whether the version of the trial affects task performance on the trial in this condition. So task performance is the dependent variable and trial version is the independent variable. Sounds like a chi2 question to me, but here's the thing. The trial version is sort of a repeated measure, but not really. Repeated measures ANOVA requires each participant to participate in each version an equal amount of times, but that's not the case.

So, what should I do? This question is not really fit for a chi2 test, I think, but also not for RM ANOVA. Do you have recommendations?

Thanks!",Chi2 or repeated measures ANOVA?,8rymqo,new,3,0,0,0
"Hi all,

I have survey data, which asked respondents to rate the importance of three factors in 8 different cases. For these three factors I want to test which one is the most important one, second most important and least important.

Using an ANOVA does not really work here in my opinion, as I have to put in a factor, which I'm confused about, because I shouldn't really need one for this analysis.

My approach right now: Using three different paired t-tests, one for each combination (A-B, A-C, B-C).

And maybe a more fundamental question. Because respondents rate the importance of those three factors for 8 different cases, is there maybe even something incorrect with calculating the aggregate means for each factor (calculating the mean by summing up the importance value of all 8 cases and divide by 8)?  If possible I would like to analyse the means without doing that, but as of right now I have no idea how to do so.

Does maybe somebody of you have an idea how I can test if the aggregate means are equal?",Beginner Question: Comparing three means SPSS,8ryju3,new,0,1,1,0
"Dear Redditors,

As the title suggests, I try to calculate the interrater variability (Cohen's Kappa) in SPSS. However, the tool that can be used to do this in SPSS, Crosstabs, assumes that the dataset consists of one column per rater and variable. However, in my case each row belongs to either one or the other rater. To illustrate this I have made the table below:

ID | Rater | Variable A | Variable B
---------|----------|----------|-----------
#1 | 1 | 0 | 0
#2 | 2 | 0 | 1
#3 | 1 | 0 | 0
#4 | 2 | 0 | 0
#.. | .. | .. | ..

How can I calculate the interrater variability in SPSS with this setup? As in, I want to know the interrater variability between rater 1 and 2 for any of the variables.

Hopefully my question is clear. If not, please let me know, then I will try to clarify it.

Thanks in advance!
",Calculating interrater variability with one rater per row in SPSS,8ry1en,new,1,2,2,0
"Say you start with 10,000 variables and you want to use a few of them to predict some outcome. You use forests or lasso or whatever to find a few variables (in my case these are genes) that do well in several datasets for predicting the outcome (disease in my case). 

Now, imagine you don't really know WHY those variables help predict your outcome. It's sort of backward from regular modelling, because in this case you found some variables that seem to predict your outcome, so now you have an incentive to study those variables and see why they affect your outcome, as opposed to the usual of having a set of variables that you have some theoretical justification for, then selecting the ones that perform best.

In traditional stats we learn this is not good, that you should have a theoretical justification or your model. Is there some terminology / math / theory I can read about that discusses this concept in depth? I only really recall it being mentioned by profs/books, but not explained in much detail. I'd like to read about the drawbacks of not knowing why your variables predict your outcome and perhaps a few examples of when (if ever) it can still result in a good model. ",Using variables whose relationship to the outcome is not well understood,8rwbu9,new,2,3,3,0
" Do economic sociologists ever employ econometrics into their studies? How many other sociologists can understand the [Gini coefficent in calculus form?](https://en.wikipedia.org/wiki/Gini_coefficient) Beyond hypothesis testing and calcultating standard deviation, how much more complicated can sociologists make their mathematical formulas.  ",What quantiative methods do sociologists use beyond an introductory course?,8rw4kv,new,2,1,1,0
"So this is a cry for help because I'm a dumb-dumb trying to do grown-up math. I'm a physician assistant in a doctoral program with a cockamamie idea and no background in statistics whatsoever.

What I am trying to accomplish is this - I'm trying to run a Monte Carlo simulation creating subjects on a medical intervention. I'm using 3 independent variables with sample sizes and ranges that I've obtained from medical literature. I've either obtained or calculated means, standard deviation (from reported medians & ranges - method per Hozo et al. 'Estimating the mean and variance from the median, range, and the size of a sample'. BMC Medical Research Methodology. 5:13. April 2005). The independent variables are continuous data. I was thinking of pooling them from the literature and obtain sort of a ""meta-distribution"" so I can determine if it's normally distributed or not. I have a strong feeling that they are not normally distributed from cursory reading. From these variables, I want to run a simulation for my dependent variable - survival to discharge (dichotomous - yes/no). Survival to discharge is generally about 40 to 60&#37; in the literature. There are no known mathematically defined relationships between the independent variables and dependent variables.

Evidently I don't know what I'm doing, but I have this deep visceral feeling that I'm doing something outrageously wrong. Your expert opinion is requested. My questions are as follows:

1) Is it legitimate to calculate mean, variance (and subsequently standard deviation) from the method that I found?

2) Does it make sense to ""pool"" the data from many studies to create the range & distribution for a Monte Carlo simulation? Assuming my independent variables are a, b, and c  AND that some studies report some of the variables and not all, is it still fair to pool the data where available?

3) The studies are inconsistent on whether or not they report individual data, even though their sample sizes tend to be small (n<20). Am I able to determine the distribution based off mean, standard deviation alone? 

4) If I can accomplish questions 1-3 without raising most eyebrows, how do I perform a Monte Carlo simulation with multiple independent variables with non-normal distribution, a dichotomous dependent variable?",Idiot Doing Monte Carlo Simulation,8rvqcm,new,16,47,47,0
I just bombed my 2nd exam. I used to have an 86% but now its a 71.5% Im a little nervous. What advice can you give me? And did you guys go through the same thing. Im on mymathlab,Im scared about failing my Online Stats Class?,8rvkiq,new,18,0,0,0
"There is a column I am supposed to fill for the 12 observations given to me. I know how to take the average and took it for 12. But how do you fill the average for each row? For observations A, B, C....I know for first observation the mean is A itself. Second mean is (A+B)/2... third mean is? Why is it not (A+B+C)/3?  
I don't know how to get that dynamically changing standard deviation and mean.

My excel file attached [here](https://1drv.ms/x/s!AjW7nYJVGMD-iX7j5mbu085aGyvx) and the graph that I expect attached [here](https://1drv.ms/u/s!AjW7nYJVGMD-igFsW1pRRwRdleCw). The formulas to be used are [here](https://1drv.ms/u/s!AjW7nYJVGMD-igFsW1pRRwRdleCw).  


So in others words, I don't know how to obtain those zig zag dashed limits. That's why I drew a  y=c line but that c is only valid for the 12th observation at end. I tried searching about means online but I couldn't find anything that describes this. What kind of a mean is this? I am supposed to have studied it but I can't easily find it only. I only find Arithmetic, Geometric and Harmonic means online which are not relevant to my case.",How to find the Mean at each observation in the sample?,8rtdn0,new,2,0,0,0
"When we say that the binomial distribution converges in distribution to the normal distribution as n gets large. Are we referring to the number of trials in the binomial distribution e.g. if I flip a biased coin with p = 6 and I have n = 10 then if this n increases from 10 to infinity does that imply conversion to normality? Or are we referring to the number of samples e.g. X_1, X_2, ... , X_n and as this n converges to infinity Bin -> N(mu, sigma^2 )?",Central Limit Theory: Adjusting simulation size vs. Sample size,8rslac,new,17,4,4,0
"ANOVA says the model is significant.

It tells me that one of my independend variables is significant (age), all of the others are not. However, the difference that this variable is causing is suuuuper small, the regression coefficient is very small. It just shouldn't be signficant. I don't understand.

[Here is a link to the results if that helps](https://imgur.com/a/MUR2qk0) (sorry it's in German) The coefficient of the age is 0,008, but to have any impact it should be at least 0,2.

I'm using SPSS.",Multiple linear regression gives me strange results,8rrjmf,new,14,7,7,0
"So I have a between subject matched pair design (matched for nationality, age and gender) for 3 treatment conditions (positive, negative and no treatment) and I am testing performance on various cognitive tests (verbal reasoning, abstract reasoning, etc) each has sub scales as well and i computed a grand total. This was done in 3 experiments were the set up is the same just the positive/negative treatments were modified.

Initially I ran multiple one way Anovas to test for differences on the cognitive tests and grand total which turned out non significant (some violated assumptions of homogeneity of variances so Welch’s was conducted). 

For the sub scales I ran a one way Manova excluding the already tested cognitive totals as they were highly correlating. Wilk’s Lambda was non significant. Treatment condition was also non significant for all sub scales and post hoc confirmed the same. 

To see if age was interacting I added it and ran a two way manova. Again Wilk’s non significant. But between subject effects turned out to be significant for treatment condition on subscales. Age also turned out to be significant on most sub scales and the interaction of treatment and age as well on most. However, in the post hoc some subscales that were significant became suddenly non significant.

So I ran a t-test on that subscales considering only positive and negative treatment and again it turns out significant. 

I am totally confused by this. Can anyone let me know if I am doing something wrong or how I am supposed to interpret this mess? 

Thanks so much","T-test, Anova and Manova give contradicting results - I am going crazy",8rnxox,new,17,18,18,0
"Hello,

  I have a side project that I have hit a snag on and I am wondering if FIML will be useful. I have used FIML before in hierarchical structural models with continuous indicators to deal with missing data. And I largely understand how the estimator works. However, it wasn't until I hit this snag that the implications of FIML started to click a little for me. 

  A brief summary of the project:

I have 7000 participants in mental health programs at 60 sites across the country. At the site level, each could choose to implement up to 5 programs for program evaluation. So, some sites only did 1 program, some 3, some 5, and in between. I have dummy variables for each program that says : ""Did the site implement program A?"" and so on for all 5. Then I have a total variable that capture how many programs overall they a site implemented. So far, so good. Not hard to understand or analyze, although you might be able to guess what my problem is. Say a site only implemented the first 2 programs, they have NAs for the remaining programs (for a number of reasons, it can't be 0. it's odd, so yes, if they did NOT implement program 3,4, or 5, it's NA for each variable). I then have a number of quality of implementation variables per program. So, again, if the site did not implement programs 3,4, and 5, those are NA. 

Here's the thing. I am looking to see if different metrics of implementation quality moderate participant outcomes. If I went with complete cases, based on the way the data are structured, I would only be using participants at sites that implemented all 5 programs. And I do not think I can recode certain metrics as 0 if a site did not implement the program. Looking at my example site above, they have an NA for ""number of facilitators for program 4"" because they didn't implement it. That is fundamentally different, I think, than saying 0 facilitators. 0 is poor implementation, not necessarily abstinence from implementation.

Now to my FIML question. Because FIML uses all available information for a parameter estimate, doesn't that mean I can run a simultaneous-entry regression like the following:

participant outcome=b0 + qualityofprogram1 + qualityofprogram2 + qualityofprogram3 + qualityofprogram4 + qualityofprogram5,

where ""quality of programx"" are all the same **type** of metric per program, and not get biased results? For example, my fake site above that only did the first 2 programs would only contribute information to the first 2 predictors, and nothing to the remaining 3. And for sites that implemented only the first 4 programs, I can draw inferences about how the quality of the 4th program impacts participants, controlling for the first 3 programs, because those sites aren't contributing to the parameter estimate for the 5th program.

Am I missing something? ","Clarifying ML or FIML, depending on how you refer to it",8rnqwm,new,0,1,1,0
"I am currently working as a medical coder for physician’s group of a hospital with a CPC. I have a nursing degree from a 3rd world country.

I intend to get an MPH in Applied Epidemiology from CSULA while still working as a medical coder. After I graduate, I am hoping to apply to USC (their PhD is funded so I’m hopefully not going to be in debt) for PhD in Biostatistics. My long term goals are to be a chief biostatistician, project scientist or chief epidemiologist.

I’m sure I’d fare better as an epidemiologist but biostatistics seems to have better job prospects & pay more, which is why I’m interested in it.

I have neither research experience (with the exception of one nursing research & community health research subject in undergrad) nor a strong math background (relevant courses taken in undergrad - college algebra, biostatistics, physics).

Questions:
1.) What are some ways to increase my chances of getting accepted at USC?

2.) Should I quit my job and find one where I’m more likely to get an epidemiologist/biostatistician job? If so, what jobs are great to apply for and where can I best find them?

3.) What are job prospects of people with MPH vs. PhD? What kinds of jobs would await me post-MPH vs. post-PhD?

4.) Would [CSUN’s MPH in applied epi curriculum](https://www.csun.edu/sites/default/files/MPH%20Info%20Sheet%202017.pdf) help me towards my goal? 

Thank you very much!",Need career advice from PhD holders in Epi or Biostats,8rl2t7,new,10,1,1,0
"Hey folks, I'm a programmer by training, and I've always been fascinated by randomness. More recently, I've been getting into information theory and randomized algorithms, and have been doing a bit of statistics by necessity - not that it isn't fun, it's just the angle I'm coming from.

I did take two classes on statistics over my first two semesters of university, but it's been a while, and we certainly didn't cover what I'm currently thinking about: randomized permutation tests. I first learned about those from [this talk](https://youtu.be/Iq9DzN6mvYA?t=7m44s), which doesn't really cover all the math behind them. So I went to my university's library and checked a whole shelf's worth of statistics books, only one of which even covers the exact kind, but not the randomized tests. Soo I've been trying to do the math myself.

Most of it I find fairly straight-forward. Given two samples of size `n1` and `n2`, respectively, there's `N = (n1+n2)!/(n1!*n2!)` ways to reassign labels. Let `T` be the test statistic; compute `T_critical` for the original samples, and count the number of permutations `z` for which `T >= T_critical` (or `T <= T_critical`, depending on H1). Now `p = z / N` gives the probability of the measured difference between groups occuring by chance.

When `N` is too large to do this exhaustively, we check `M` random configuration of group labels, and count `zs` configurations with `T >= T_critical`. We can then estimate `p ~= zs / M`, which is what the code shown during the talk does.

__So here's my question__: Randomization introduces a new source of possible errors, so if we reject H0 when `p < p_critical`, isn't the probability of error _greater_ than `p` (and therefore potentially greater than `p_critical`)? Should we not apply some sort of correction?

`zs` follows a binomial distribution (`M` trials with probability `z / N`), so even when `zs / M < p_critical`, there's still a chance that `z / N > p_critical`, and when `zs / M > p_critical`, it's still possible that `z / N < p_critical`.

My best guess at the correction function would calculate those probabilities using the inverse of one of the [standard functions for estimating confidence intervals](https://en.wikipedia.org/wiki/Binomial_distribution#Confidence_intervals) and adding them to or subtracting them from `zs / M`, depending on which case we hit. How far down the wrong path am I?

Previously I considered simply returning the upper bound of an estimated confidence interval, but that still leaves some errors unaccounted for, no?",Quantifying the probability of error in randomized permutation testing,8rk8s5,new,4,10,10,0
"Hello gentlemen and -women. Not native English speaker so excuse mistakes.

I am doing a pretty simple study using multivariate linear regression, however I have encountered a problem I can't find reflected in any of my textbooks or literature research. Likely a stupid question but here goes:

I have a hypothesis that time delay is a key aspect of the effect of X on Y in the base model, which simply has the dependent variable and a single independent variable (that can incorporate time delay in different ways). For example one operationalization is the stimulus during the past observation period, and another is the average stimulus in the past 6 observation periods. I have a total of 12 different delays that make practical sense.

Before I continue with multivariate regression I would like to somehow select a single delay variable, since the point is not actually to compare the effect of different delays, but to analyze the complete model later. But what criteria make sense to select the operationalization of the delayed variable? 

I am considering comparing F-statistic, R-squared (same variable number so adjustment should not be necesarry) and Akaine's informationcriterium (AIC). 

So my question is: Does it make sense to compare models with different delay variables under these criteria, and use the variable with the best ""model fit"" going forward? Or am I making a looking at it in a wrong way?
",Using comparisons of base model to select one of multiple ways of operationalizing key explanatory variable?,8rimka,new,1,8,8,0
"Hey, I am trying to graph my grades through school and highlight moments etc, and I want to have the dates at the bottom (horizontal axis iirc) and grade, but sorted where it goes like this:
0-5-4-3-2-1.

It is because in my country, 1 is the best and 5 the worst, and I dont want 5s to be at the top of the graph, as the top should be the best grades.

I tried excel, but it nearly gave me stroke because I just can't understand the program.

Any advice on my listed problems and also advice on the program?

Thanks for any answers!","Which free graph tool should I use? Also, a bunch of axis questions (Line graph)",8rifun,new,6,3,3,0
"In regards to mediation analysis in which the c prime pathway is stronger than the original c before considering the mediator, is there a term for when this happens?

In all my previous understandings of mediation, c prime is usually weaker compared to c. If it is stronger, is that called something?

Thank you!","In regards to mediation analysis and c prime being stronger, is there a term for it?",8rhf4m,new,1,4,4,0
"Apologies in advance for the vague title.

I'm working on a personal data science project analyzing the performance of prospects in the Canadian major junior hockey leagues. Basically, I'm calculating what % of a team's points a player contributes to his team in each game he plays (alongside goals, primary points, powerplay points, etc.). 

One of my biggest gripes with the project so far is the lack of information about time-on-ice for players, so I can't adjust player's metrics based on this (since players with more icetime will score more points).

Luckily, I am keeping track of each play a player makes. So if player A scores a goal, I keep track that player B and player C assisted on that goal. Since I do this for all plays in the leagues, I can see that, for example, player B assists on 70% of player A's goals, and player A scores on 70% of plays that player A assists. 

I'd like to use this information to scale players' metrics. For example, if player A and B are high scoring good players and always pair in each other's plays while player D is a modest scorer but has no high-scoring player pair on his plays, I'd like to give player D's metric a boost to account for the fact that it's more likely that he is playing with less skilled players on his line than players A and B. 

Obviously this won't be a perfect correction, but I still feel that it's better than nothing. I have a pretty basic stats background (sophomore/junior level applied stats class at my university) and don't really know where to start, other than calculating what % of plays have pairs of players.

I'm also curious to know if there's a way to tell which of player A or B are the driving force in their plays. If player A is an amazing goalscorer, player B doesn't have be an amazing playmaker to put up a lot of points. Likewise, if player B is an amazing playmaker, player A doesn't have to do much to score goals. Unfortunately, I don't think this is possible due to the limited number of games played for prospects in the juniors and the lack of variation of teammates from game-to-game (player A may only play in 20 games that player B was absent from, so I don't think that's enough to be able to show a significant rate of scoring unless it's a quite drastic change).",Calculating significance of paired players in hockey plays?,8rh05b,new,0,1,1,0
Is there a theorem that states if we have X_i's i.i.d random variables with E(Xi) = 0 and V(X_i) = sigma^2 (but finite variance) then V(X_i^2 ) is also finite?,"Looking for a theorem, but I am not sure it exists",8rgda5,new,8,4,4,0
"For datasets with 20 observations:
=======================
* What does it mean for the data to be Normal?
* Why does it need to be Normal? (meaning, why do some statistical tests require it?)
* What is the consequence of Non-Normality?
* How do we fix Non-Normality? (meaning, how do we select a transformation?)


Residuals for OLS Regression:
=======================
* Why do the residuals need to be normal? 
* What happens to the regression model if they are not normal?
* Is there clear distinction between constant variance and normality of residuals?","Normality - often used, but seldom explained. Help a noob out.",8rfke5,new,5,0,0,0
"Hi! 



I would like to find out the significant diffirence between two samples ( Inhibitor vs Control) after I follow up the cell proliferation in my cells over time. Is there a way to compare the curves in same graph? Can I do it one by one using t.test for each time point since it is followed up for at least 30 days? 

I've linked an example (it is figure C).[Graph](https://www.researchgate.net/figure/Cell-proliferation-and-xenograft-model-The-graph-shows-the-doubling-time-of-mock-and_fig1_278046885)

If you could explain a simple way I would appreciate it very much!

PS: It says polynomial analysis but I am not very good at statistics. Can I do it on Excel? ",How can I calculate P-value in a cell proliferation analysis?,8re3dc,new,10,17,17,0
"I'm finishing up my master's, and I have one job that could be considered semi-relevant (apart from my TAship). My time between undergrad and grad was mostly spent in either unemployment or crappy minimum wage jobs. Should I include any of these in my resume?",Resume: Should I include old minimum wage jobs?,8rdv5f,new,10,5,5,0
"Hi everyone,

I'm looking at prevalence rates of dementia in a specific population. The problem I'm having is very simple - the data on the population demographics is split into 3 age ranges that I'm interested in (45-64, 65-84, 85+), however the prevalence rates are split into 4 age ranges (45-54, 55-64, 65-74, 75+). I was wondering how I would go about combining the prevalence rates into the demographic categories. My first thought was to just add them together but this doesn't seem right. I'm even unsure what the term for this is so I'm not sure what to search for.

Can anyone shed some light on this? Apologies if this is a daft question, I'm very new to all this. Even if you can provide me with the name of this operation I'd be grateful.

Thanks",Complete Beginner - simple problem but unsure of the term for it,8rcw5v,new,3,6,6,0
So I conducted a survey and I am trying to understand how I could possibly use a regression to understand how much gender influences the outcome of specific questions for which I used a likert scale. I valued the likert scale from 1 to 5 and now I am not sure which regression model to use. All the explanations I found on the web use numeric variables only and not binary like gender.,How can I use a regression to understand how gender influences a likert scale?,8rbgh6,new,10,9,9,0
"I got my acceptance letter yesterday, I'll be starting the online Masters in Statistics program at Texas A&M in the fall.  I've got a BS in Computer Science and 20 years experience in all things data (DBA to Data Engineer and everything in between).  On the advice of the school, I've been refreshing my math knowledge as I haven't touched calculus in 2 decades.  I'm fairly proficient with Python, but the school is an R/SAS shop.  I'll be brushing up my R skills (I've played with it before), but at least they expect to teach me SAS so I don't need to self learn that one.  Unfortunately, that means I'll need a Windows machine and I haven't had one of those in quite a while.

Beyond the Calculus and R - is there anything I should be looking at to be most comfortable come the fall?  I'm really excited, and I haven't been excited about school in a long time.",Starting a Masters in the fall - Any advice?,8raxw5,new,45,11,11,0
"I am a university student writing a paper on the impacts of particulate matter 2.5 (PM 2.5) between two neighborhoods, that are 2.25 miles apart. I am trying to prove a correlation between diesel PM and asthma. I used two census tracts data, one had a Diesel PM of 35, while the other had a Diesel PM of 11.4. The asthma-related hospitalizations were 5.5x higher in the community with the higher Diesel PM rate.

What statistical tests can I run off of this to prove the significance and/or correlation between diesel PM and asthma. What other data do I need to obtain?
","What tests can I use for a particular spatial problem, given the datasets that are available.",8r8ooy,new,6,5,5,0
"Hey all! I spent sometime trying to build a decent pipeline to go from start (a new dataset you've never before seen) to finish (a well-constructed logit) in R. It's a pretty quick read, but I'd appreciate any and all feedback you have! Most of this is pretty simple, maybe with the exception of the Lasso method. 

(link below to the GitHub markdown with extensive comments)

[https://github.com/pmaji/r-stats-and-modeling/blob/master/classification/logistic\_regression.md](https://github.com/pmaji/r-stats-and-modeling/blob/master/classification/logistic_regression.md)",Feedback on new pipeline for logistic regression model-construction?,8r8au2,new,0,1,1,0
"Hi,

I'd like to assess a predictive model with gini coefficient and KS, but I'm really unsure how to evalutate these criterias?

e.g. I get KS as 0.30 and gini as 0.39. Is it good/bad/okayi'sh? And what does it tell me in terms of how good my model is at predicting?

Cheers",Question about gini coefficient and kolmogorov-smirnov,8r7t0n,new,8,4,4,0
,In what ways Statistics is still useful/preferrable compared to Machine Learning?,8r71tv,new,47,8,8,0
"Hi Folks. I'm in a bit of a statistics conundrum and could use your help. Assume I gave a standardized test to a group of K-12 students and received scores in the following fashion:

Grade   Average   Standard Deviation

K ............. 120 ............. 15.02

1 .............  142.4 ........... 11.4

2  .............  156.3 .............  13.2

3  .............  190.5 .............   12.2

Lets assume that the raw average score goes up with each grade level and that the scores are normally distributed. Here's my question: How do I take this data and determine a clear cut off for what constitutes a K score, or a 1st grade score, or a 2nd grade score, etc?

For example, I could take the mid-point of 120 and 142.4, which is 131.2, and simply say that everything below 131.2 is a K score, and that everything at or above 131.2 is a 1st grade score. But I feel like this mid-point method isn't very scientific, especially with the standard deviation information readily available. How would you go about finding the cut off points between each grade-level?

Any advice or thoughts are greatly appreciated! Thanks.",Question about Standard Deviation,8r6m4l,new,10,2,2,0
,Is statistics on it's way to be an automated position? I see projects such as Automatic Statistician on the rise,8r60xc,new,12,9,9,0
"Edit: To give more context. Let's say I am comparing a CRM database that has a list of customers vs non-customers and I wanted to use a Chi-Squared test to compare a categorical variable between the two groups. 

Hello, I have a few questions about samples and populations for business.

* Is all the current customers for a company the population or sample of all potential customers?
* I have learned that sampling for a Chi Squared test must use replacement to establish independence between variables. However, say I own a company with 1000 customers currently, would just using the 1000 people for a chi squared test be okay (without replacement)?",Chi Squared Test for Homogeneity on Customers?,8r5ymj,new,2,1,1,0
"If I have a model with one exposure along with a handful of adjusted variables, eg outcome \~ exposure + a + b + c...  

My standard error and confidence intervals for the exposure are quite narrow.  

But adding an interaction term: outcome \~ exposure + a + exposure\*a + B + C .... 

Blows up my SE and CI for the exposure. The actual interaction term is nonsignificant. I just dont understand what happens to the SE of the main parameter, why does it increase so much?",Why does adding an interaction term increase the standard error of the main parameter in multivariable regression?,8r54je,new,3,2,2,0
"http://www.askamathematician.com/2010/07/q-whats-the-chance-of-getting-a-run-of-k-successes-in-n-bernoulli-trials-why-use-approximations-when-the-exact-answer-is-known/

Includes *most of the derivation, as well as some code and an approximation for N>>K. Other posts have shown another approximation which isn't exact like this, nor easy to compute IMO.",Interesting derivation for the probability of at least K wins in a row,8r4mfw,new,0,2,2,0
"Hi all, I have a simple question that is causing me a headache.

I  conducted negative binomial regression model to assess the association  between X (predictor variable) and Y (response variable). When I am  describing the results, I am not sure if I should name the predictor  variable first (e.g. We found a positive association between X and Y) or  name the response variable first (e.g. We found a positive association  between Y and X), or if this really doesn't matter.

So, what do you think is the correct way?",Does the predictor variable goes first when describing an association?,8r44b9,new,0,1,1,0
"The [Hack for the Sea](https://hackforthesea.tech) Crew is proud to present this year's challenge statements and data sets.

They are, as follows:

* [How does a changing coastal watershed impact coastal waters?](https://hackforthesea.tech/GLO/challenge/1)
* [Can you predict where and when cod spawning will occur?](https://hackforthesea.tech/GLO/challenge/2)
* [Can an individual whale be identified based on its blowhole?](https://hackforthesea.tech/GLO/challenge/3)
* [Can you design a mooring that's both eelgrass and user-friendly?](https://hackforthesea.tech/GLO/challenge/4)

The event is all ages and open to anybody who is ready and willing to provide their skills to help the oceans. Also, while the summit will be held in person, the community is open and involved year round. Join us!",Data Sets and Challenge Statements Released for this year's Hack for the Sea,8r416w,new,0,2,2,0
"Thanks for reading. Some months ago or so, there was a Reddit question that I failed to answer despite trying a variety of approaches... perhaps I didn't even understand the question to begin with. Anyhow, if I recall the problem correctly, it went something like this: there is an Olympics-like competition event in which various countries are participating. USA is sending 5 players, Canada is also sending 5, Mexico is 4, Brazil is 3, England is 2, France is 1, Italy is 1, China is 1, and Japan is 1. For this event, there is first place, second place and third place. Countries may win more than one place (assuming they have more than 1 player, of course). How many possible combinations of countries winning*? I'm hoping an approach exists, other than the brute force way of listing them all out.

*allowing for duplicates to cover different players",counting question that has bugged me for a long time,8r3nm1,new,4,6,6,0
"I'm studying the relationship between variables X and Y.

At low values of X, Y is low as well. However, at high values of X, Y can be high or low.

So clearly, X has some association with Y, but not one that a simple linear model would be able to describe. What statistical analysis could I do in this situation to try to quantify the relationship? Is there a certain statistical technique which is appropriate for this kind of situation?",What kind of statistical analysis can I do in this situation?,8r3du1,new,16,4,4,0
"I have data for a client on whether people had a stroke when they got back to a hospital and a score from 1-10. 

So it looks like 

person1 8 stroke 

person2 6 not stroke

person3 4 stroke 

for about 100ish clients 

So to get the score, the EMS people would have a checklist and the score is calculated from how many boxes they would check off. For example 1) was the person fat, 2) was the person conscious and ect up to 10 items. So if the EMS person answered yes to 1 and 2, the person would have a score of 2. (These are made up questions. I don't remember what the actual questions are)

In theory, one person could have a score of 3 from items 4, 5, 6 and another person could have a score of 3 from items 1, 2, 3 and they would have the same score. I cannot differentiate between the scores because the client doesn't have that data. Something about IRB approval. 

I did a t test in about two seconds and found that there was a significant difference between people who had a stroke and those who did not for their score (it was like 1 point difference) I also did a logistic regression and that also found it significant. 

Is there another test I can do? I'm not sure what else I can say about this project. Any guidance would be helpful. I have to write like 3 pages on this. ",Help with Analysis for a Project,8r2u22,new,2,2,2,0
"Hey r/statistics

I am just a lurker here and I’m not sure if this is the correct place to post this but I’m in the process of taking a data analysis course through an online masters program and I find myself really struggling. 

I’m learning probability theory right now and I feel myself falling behind due to a lack of familiarity with the calculus concepts (namely, integrals) that I’m running into. I haven’t done calculus since high school (almost 6 years) 

Anyone have any tips on where to start here? I’m not sure what kinds of calculus that I need to know to fully understand this material. 

Any help here would be much appreciated. ",Struggling with probability theory,8r2epc,new,26,22,22,0
"I am trying to optimize some weights for a factor model. I have three return streams that I want to apply weights to and create a single index and compare against a benchmark. However I don't simply just apply the weights. After applying the weights I do some normalization and then I have the index returns, and then do a cumprod in order to get the index price.

Is there a way I can do this for a variety of weights to find the maximum R-squared? I don't want to use a basic linear regression, because I'm not just simply applying the weights to my streams. I have some other things going on before it spits out the final stream. Doing this in python if that helps. Any help is appreciated.
",HELP - Optimize weights for a custom function?,8r22bw,new,0,3,3,0
"I am working on interpreting a survey I conducted with reddit users and ran into what I guess is a common problem of statistics. I get significant changes in my results if I filter by gender, but males tend to be younger and based in Europe while females are older and based in the USA. How could I possibly say for sure if the changes are due to one or another? And there might be other factors in play too that I havent even considered.","How to know for sure if result changes are due to gender, when they could be due to other factors like age, country, ...",8r0ih4,new,14,7,7,0
"I am creating am using the R function glmer to create a model with the gamma family, and I am not sure how to check the model fit? How should I check the residuals and how should they be distributed? I have searched far and wide but can only find good answers for the Gaussian family as those residuals should be normally distributed. Thank you in advance!",How to check model fit of Generalised linear mixed effects model with gamma family?,8r07ch,new,2,6,6,0
"I have to calculate ratios of cell densities from 2 separate cultures, which will be used later in a formula. 

The problem is that the formula is normally executed with continuous data, but my data set is completely categorical; there are intervals such as 1-5 cells (labeled ""."" ), 6-30 cells (""..""), 31-40 cells (""...""), and 40+ cells (""....""). Intervals from a separate data set that I would like to combine include different ranges, such as 1-10 cells (""."") and 11-15 cells ("".."").

I realize one of the challenges is that the intervals have unequal ranges, so is there any logical way of scoring these into numerical values for use?",Scoring categorical data for use in formula,8qxqte,new,0,1,1,0
"I asked this question in r/askmath and someone suggested i post it here.

I was posed this question on an entrance quiz for a course:

 Suppose yi \~ N(μi,σ\^2), where i = 1,...,n 

and

 μ \~ MVNn(0,\[ω\^2\]R), where R is a known fixed correlation matrix. 

Find p(y1,...yn|σ\^2,ω\^2) up to a constant of proportionality. Show your working. 

I, straight up, don't know how to do this question. I know that yi \~ N(μi,σ\^2) means that y is normally distributed.... and I know that μ \~ MVNn(0,\[ω\^2\]R) means μ is a zero\-mean multivariate normal distribution.

But I dont know where to begin with the solution? Any help at all is greatly appreciated.","Find p(y1,...yn|σ^2,ω^2) up to a constant of proportionality?",8qxncc,new,3,2,2,0
"I’m trying to check the probability that I got a 5 on the APCS test. The stats were given
24.7% got a 5
21.3% got a 4
21.7% got a 3
11.8% got a 2
20.5% got a 1

I want to find the probability that I got a five, given that I didn’t get a 2 or a 1. I took AP stats this year but for some reason I just can’t figure it out.",Probability of B given A,8qx0ch,new,17,12,12,0
"Let's say that all I have the ""Species"" variable from the `iris` dataset (i.e. none of the other variables are available), and I wish to sample from the posterior distribution of the proportions for each of setosa/versicolor/virginica, with a vague prior with equal proportions for each category. How would one go about doing this in R? My google-fu on how to do something that sounds so simple is weak on my part.",Establishing the bayesian posterior distribution of a multiple category variable using R,8qwq6k,new,6,1,1,0
"I'm trying to model the effect of drug A in terms of dependence on drug B. Lets say A is cannabis and B is heroin. First I created a DAG, with:

MJ use > tendency to use drugs > Heroin use > heroin dependence

I had a number of covariates (education, income, sex, etc) feeding into 'tendency to use drugs' but the main issue is the complex relationships imply bidirectionality.

- alcohol use > tendency to use drugs > MJ use (here the relationship goes BACKWARDS)
- income > drug tendency, mental health > drug tendency (but income and MH also feed into each other)

All the examples I've found seem to be unrealistically one-directional (such as warming up on ingame injury). I've though about path analysis and SEM, but again, all the examples seem to inexplicably contain only unidirectional relationships. Since DAG/SEM is used in social sciences often, I'm confused how researchers consistently have unidirectional relationships. Am I missing something fundamental here?

You can see an example DAG here: dagitty.net/mIqMh3b",Confused with DAGs and path analysis,8qv14g,new,4,1,1,0
"I got the widely-suggested book 'An Introduction to Statistical Learning' and am going through it right now. I'd like to spend a bit more time learning the theory behind linear modeling since the book didn't dive too in-depth. Stuff like t-statistics, p-value, and whatnot. 

I'm going through Coursera's Linear Regression Model course as well but it's really really cursory and not entirely useful IMO. It hasn't mentioned anything aside from R2 in the first week's worth of lessons... 

So I'm looking for resources (books, pdfs, online classes, etc) that teach the theory a bit more but not at a grad-school level...",Looking for resources to learn linear regression modeling,8qu18j,new,7,7,7,0
"I am looking for some help checking my work on the following problem.

I need to determine the system mean and system standard deviation of a LED light fixture so I can model what the expected total light output would be (in lumens). The components of the system are as follows:

1 driver board that supplies the LEDs with current.

100 LEDs that each vary independently in lumen output (regardless of input current).

1 Lens that reduces the light output from the LEDs by a certain percentage.

Here’s what I have so far:

Assuming the nominal input current from the driver board, the individual LED output in lumens, assuming normal distribution as follows:

Mean (LED): 58.57 (lumens)

Standard Deviation (LED): 3.14 (lumens)

The driver board has a nominal current value that can vary \+/\- 5&#37;. This variation in current linearly affects the output of the individual LEDs. I have made the assumption that this is normally distributed to a 3 sigma confidence, which gives me the following:

Mean (driver): 100&#37;

Standard Deviation (driver): 1.666667&#37;

The total light transmittance through the lens (amount of light let through) is as follows:

Mean (transmittance): 64.5&#37;

Standard Deviation (transmittance): 0.85&#37;

From this, I get a system mean for total light output as follows:

Mean (system): Qty of LEDs \* Mean (LED) \* Mean (driver) \* Mean (transmittance)

Mean (system): 100\*58.57\*1.00\*.645 = 3777.9 lumens

I used RSS to calculate the system standard deviation and calculated as follows:

Convert driver board standard deviation to lumens:

Standard deviation (driver): .016667\*58.57 = 0.976 lumens

Convert transmittance standard deviation to lumens:

Standard deviation (transmittance): .0085\*58.57 = 0.498 lumens

Then I complete the RSS:

Standard Deviation (system): 100\*SQRT(3.14\^2 \+ 0.976\^2 \+ 0.498\^2) = 332.6 lumens

Does any of this look wrong?  Thanks for checking!",Check my work on this problem,8qtj0k,new,0,0,0,0
"Working with census PUMS data, I have 9617 observations. For reliable results I’m supposed to keep the number of sample observations above 400. But say for example I have 0 observations for popsicles. I am not able to note that there are 0 popsicles in my sample? Do you have to ignore zeros?   And do you have to ignore observations below 400? Or can you include margins of error?",Understanding Census Sample Data,8qsisl,new,2,1,1,0
"I have a problem concerning batch effects in survival data of C.elegans. I read through a few forum posts and a paper but I am still not really getting it. I'm majoring in physics and never had a formal statistics course. 
In the experiment the effect of different interventions of lifespan were tested in different batches. The experiments were conducted on different dates in a machine which automatically captures the time of death. The problem is, that the temperature in each batch can not be controlled 100% and C.elegans is quite sensitive to changes in temperature. In the end I wanna put all the data of the batches together and fit a Gompertz function to them using MLE. My data has the following from:

Batch/ Temp/ Time_of_death

1 20 1.5

1 20 1.7

1 20 1.9

1 20 1.4

2 20 1.3

2 20 1.2

2 20 1.6

2 20 1.7

I know about the R function Combat but I am not sure how I would implement my data into it to get rid of batch effects and what the batch covariates exactly are. Any Help would be very much appreciated.",Batch effect in survival data,8qs84n,new,8,2,2,0
"If you have 7 locks and 50 keys, each lock can be opened by one key only, what would be the fastest way to open one lock?
A) Try the keys one at a time on a single lock.
B) Try one key at a time on all locks one by one.

Personally I think B sounds like the right choice, but it is time consuming.

What do y'all statisticians think??",Question that's keeping me up at night,8qr2iq,new,20,34,34,0
"Hi r/statistics,

I'm graduating from a 4 year mathematics degree this summer. I focused mainly on abstract algebra during this time. In a post exam epiphany, I realised all the interesting mathematics I have missed out on. 

I'm very much motivated to pursue a career with a quantitative element, given I have the sufficient background to well understand statistical content. So, I've set aside the next few months for my personal development in this respect. Im currently working through ""Basic Probability and Statistics by Example"" by Kelbert and Suhov. This seems to be a recurring text for first and second year undergraduate courses at my university. What follows this is exactly where I need some help! 

A friend of mine has recommended texts on both Multivariate Statistics and Monte Carlo Methods. What other interesting areas should I focus on next? Are there any up and coming areas of statistics that are particularly exciting? Big data and AI are terms slammed around a lot, where should I start with these?

Any ideas would be much appreciated! Either general areas that might be worth me investigating further, or specific texts that helped you!

Thanks! ",[Question] A Mathematician Who Neglected Statistics,8qqlp8,new,14,3,3,0
"I was doing some statistical analysis of police\-to\-population ratio of various cities in the year 2014 and I got the following results:

**Singapore:**

Total Population size : 5,612,300

Total Police size : 13,487

Policeman to Population ratio :416.1266405

How many groups of 100,000: 56.123

Police\-man per 100,000:240.3114588

**Hong Kong:**

Total Population size : 7,112,400

Total Police size : 28,261

Policeman to Population ratio : 251.6683769

How many groups of 100,000: 71.124

Police\-man per 100,000:  397.3482931

**Tokyo:**

Total Population size : 13,130,762

Total Police size : 43,305

Policeman to Population ratio : 303.21584

How many groups of 100,000: 131.30762

Police\-man per 100,000:  329.7980726

**New York:**

Total Population size : 8,335,697

Total Police size : 34,500

Policeman to Population ratio : 241.6144058

How many groups of 100,000: 83.35697

Police\-man per 100,000: 413.8826063

**London:**

Total Population size : 8,170,000

Total Police size : 32,855

Policeman to Population ratio : 248.6683914

How many groups of 100,000: 81.7

Police\-man per 100,000: 402.1419829

 On first glance, it seems as if Singapore has the highest police\-to\-population ration, at 416. But when I start comparing by Policeman for every 100,000 residents in the population, it becomes the lowest. I am not sure what to make of it. For practical purposes, what does it really tell me if I am analyzing if Singapore's police force is overmanned or overstretched? ","Why do ratios look so different when we calculate by per 100,000 residents?",8qoykt,new,3,2,2,0
"Hello. I am wondering how to use Minitab for what I want to calculate how effective can one brand of gloves used at my facility would perform the best.

So this takes place at a manufacturing facility for bbq kettlegrills

Basically, I want to see if given 1000 units, how many defects would two brand of gloves perform. I believe this can be achieved using a negative binomial regression, but correct me if Im wrong.

Here’s more info just to explain better what I am trying to do.
Once the grills are assembled, they go through an inspection line where two processes happen:
1.- Inspection: grills are checked by operators for any defects (in this case, craters on paint) and if any found, they input them on a computer system. This is where the variable data comes from in the sheet im sending you (number of defects on total units)

2.- Repair: Operators check their computers on each grill and if they find any defects they look for them in the grill and either repair them (good unit) or send them back to repaint as a rerun (defective units) This is where the attribute data comes from in the sheet im sending you (defectives from a sample)

Before this inspection line, there’s a few prep, finishing and assembly lines, where operators wear gloves to protect themselves and the units when manipulating them. We suspect these gloves might be causing craters.

[QuickSummary](https://drive.google.com/file/d/1YLhRSrMO4hw4omTFQZbqMLp9OZ2Hnt3Z/view?usp=sharing)

[Raw Data](https://drive.google.com/file/d/1SvvmHti6t7XugUbvFqG4BowjRYNIpYTA/view?usp=sharing)

So as you see in the data (attachments), from Feb 20th to March 26th operators were using Vargas Brand. And from March 27th up until June 9th, operators used Ansell Brand.

I want to see if there’s statistical evidence to prove gloves did or didnt have and impact on craters, I could do a 2 Sample T test for a difference of means, but I also would like to simulate what would happen running 1000 units as I sateted before.

I really need to use Minitab for this as is the only statistical software I’m allowed to install on my computer.

Thanks everyone for your help in advance.",Negative Binomial Regression on Minitab?,8qow5j,new,8,1,1,0
"Here is my situation:  Students from 4 different classes are told to describe a specific phenomenon under two different contexts C1 and C2, in a written response.  They do this as a pre-test and a post-test after instruction in that topic.  Each response is graded based on having 6 of the essential concepts (A1, A2, A3, A4, A5, A6) and 3 of the misconceptions (B1, B2, B3), yes/no or 0/1 for each.  Ideally, every student would have all 6 essential concepts and 0 of the misconceptions, for both of the concepts.

As an example, lets say on the pre-test for context C1 the student used the ideas (A1,A3,B1,B2) and for context C2 the ideas (A1,A4,B1).  Then on the post-test for context C1 they used the ideas (A1,A3,A4) and for context C2 the ideas (A1,A3,A4,A6).  So, for both contexts the student gained essential concepts while eliminating misconceptions, and they also have a closer agreement between the 2 contexts. (for the topic at hand, the student should give the exact same explanations for the different contexts).

I want to find either: (a) student-level network measures that indicate their ideas are becoming more complex in each context and similar across contexts, then I will compare these measures for ~400 students in each class, or, (b) a class-level network measure that aggregates all of the students from each class, and compare these class-level measures across the different classes.

What I'm having difficulty with:    
1) My networks are very small, only 9 possible nodes in each context, or 18 possible nodes if I combine contexts.    
2) I want to make sure that a connection between Ai-Bj is seen as a bad thing, since an essential concept and misconception should not be linked together.    
3) My network is undirected, and always complete, if a student has Ai, Aj, and Bk, they are automatically all connected with no direction.",Looking for sources on network statistics that fit my situation.,8qnf9n,new,1,1,1,0
"The recent work of the National Center for Health statistics to look at suicide rate trends by state presents a troubling indication of the challenges policymakers face in addressing an important policy challenge.  At the same time, the analysis itself and the fact that it could even be completed is a testament to the value of the National Vital Statistics Program. More [here ](https://bipartisanpolicy.org/blog/new-suicide-statistics-demonstrate-value-of-data-coordination-for-vital-statistics/)from the Bipartisan Policy Center: [https://bipartisanpolicy.org/blog/new\-suicide\-statistics\-demonstrate\-value\-of\-data\-coordination\-for\-vital\-statistics/](https://bipartisanpolicy.org/blog/new-suicide-statistics-demonstrate-value-of-data-coordination-for-vital-statistics/)",New Suicide Statistics Demonstrate Value of Data Coordination for Vital Statistics,8qn90p,new,0,9,9,0
"
I am trying to create a generalised linear model with random effect. I have a small dataset, with longitudinal data of 4 subjects during different years. The data I obtain from them is a frequency data, and for one of the subjects all of the data points are 0. So when checking the normality and the residual plots the distribution is not normal. [Residuals plot](https://i.stack.imgur.com/BifAH.png)

I tried transforming the data in different ways but the plot remains to look the same.

Is there any model or transformation I can use for this type of data, where one of the subjects shows no variability?

m=lme(Freq ~ Time, random=~ 1|Subject,  data=my_data, method='ML')

Thank you",Zero-inflated generalized linear model ?,8qn8yb,new,0,2,2,0
"I'm making sample size determinations for a future study. The book I've been referencing is called ""Sample Size Calculations in Clinical research"", and I've been using the corresponding R package, TrialSize. The comparison we're doing is a t-test (for a non-inferiority trial). The author gives equations for known and unknown variance, and briefly mentions that equations for known variance can be used in place of the unknown ones when n is sufficiently large. 

I suspect that's related to the t distribution approximating a z distribution at higher sample sizes. However, it appears that the author only uses calculations for known variance in his R package. Is there a reason for this, and is this a problem when the equations are yielding very small sample sizes (n < 10)?",Power Calculations: Should I be worried about this?,8qn86m,new,3,1,1,0
,Estimating population ratios from a survey?,8qm8md,new,5,2,2,0
"Hey all, Andy Field's Discovering Statistics Using R is targeted toward people who are new to R *and* Statistics itself. This is my situation. But I don't really like all the jokes that Andy includes in the book. I prefer dry reading that's to the point. Is there a similar book for the same target audience but without the bullshit? Thanks. ",Book like Discovering Statistics but without the fluff?,8qm2hz,new,2,1,1,0
"Is there terminology that goes along with this type of action? Say I have a lookup table

Group | Match | Weight
---|---|----
A | Dog | 1
B | Cat | 1
C | 4 legged | .5
D | Fish | 1
A | 4 legged | .5
E | Plant | 1

and I wanted wanted to find matches for ""4 legged, dog"". Keeping

Group | Match | Weight
---|---|----
A | Dog | 1
C | 4 legged | .5
A | 4 legged | .5

Then aggregating the rows by group

Group | Weight
---|----
A | 1.5
C | .5

Then saying

Group | Distribution
---|----
A | 75%
C | 25%

Just wondering if there is any terminology for that type of subsetting, aggregating, describing distribution process.",Is there a name for this type of sampling?,8ql52i,new,8,0,0,0
"I'm a few years out of a math undergrad (concentration in abstract algebra) and have been working as a software engineer, primarily in the healthcare domain, ever since.

Recently been seeing a lot of jobs similar to what I do now, but with more of an emphasis on analytics and stats, and thinking I'd like to try my hand a it.

Can anyone recommend some material for

* Getting a quick refresher for my rusty prob/stats skills?
* Learning what kinds of statistics are useful in healthcare and where to start studying them?

Thanks everyone! ",Statistics for data engineering work?,8ql30e,new,0,1,1,0
"Hey guys, I'm having a hard time solving or wrapping my head around a certain probability problem and was wondering if someone could help me out by explaining the methodology needed:

If I draw 32 cards out of a standard 52 card deck and then out of those 32 cards I draw 8 cards. What are the chances I have exactly 4 cards of the same suit within those 8 cards. Meaning the other 4 cards can have a maximum of 3 of one of the other 3 suits. 

I understand how combinations and permutations work but can't seem to figure it out. ",Help needed with odds on follow up card draw,8qk346,new,4,1,1,0
"Hello!

I am currently working on a study on household data in which I want to find the covariance of two specific variables. However, I am currently not sure about how to calculate the confidence intervals. Below is a simplified description of what I’m trying to do:

\-First, I stratify households into three different bins. 

\-For each bin, I calculate the mean value of variable A of the households in the bin. Let’s call the resulting value variable X. So for each bin I now have a value for variable X.

\-Also for each bin, I run a regression to find the regression coefficient of variable B on variable C. Let’s call this regression coefficient variable Y. So for each bin I now have a value for variable Y.

\-I take the three bins together and find the covariance of variable X and variable Y. 

How do I find the 95&#37; confidence interval for this covariance? Had I not worked with bins I would have bootstrapped the sample 100 times at the household level. However, since I am working with bins I’m not entirely sure how to proceed here. Could someone help me out?",Finding the Confidence Interval when working with bins?,8qjxbm,new,1,0,0,0
"Hello,

I'm doing my MSc in Forensic Psychology. I've spent about a week trying to do my analysis but I don't think I'm doing the right thing, or I can't seem to compute things correctly in SPSS. Any advice would be greatly appreciated. 

The study is looking at personality, prejudice, identity and experiences of hate crime.

The measures used are:

>Participants will initially be asked to provide demographic information, which includes their religious identity (if any) and gender. This study will utilize the Big Five scale for measuring personality (Goldberg, 1992).   
>  
>This study will also use an adaption of the ISSP 2013 National Identity III Basic Questionnaire (ISSP Research Group, 2015) for assessing identity or potential self\-perceived “in\-group” or “out\-group”.   
>  
>For assessing prejudice, this study will be adapting a number of scales from Velasco González, Verkuyten, Weesie & Poppe’s 2008 study on prejudice towards Muslims. This includes a feelings thermometer, a measure of social distance, and a measure of stereotypes. This study will also ask about participant’s exposure to hate crime.

For religion, individuals have been coded into Muslim (indicated by a 1) or Non\-Muslim (indicated by a 2). 

The Big Five scale uses a likert scale and the questions are divided into five tests of personality. 

The ISSP measure I've simply used as a total number \- it hasn't been divided into any sections. It uses a likert scale.

The feelings thermometer is rated between 0 and 100.

Social distance uses a likert scale.

Stereotypes uses a likert scale.

Exposure to hate crime is a series of simple yes/no questions which I'd aggregated into a total score for each individual.

The hypotheses are: 

>  The data will be comparing Muslims and Non\-Muslims, and comparing the sub groups of male and female Muslims. Based on previous research, it is hypothesized that:   
>  
>(1) Experiences of hate crime will correlate with lower levels of belonging to the “in\-group” (i.e. “Britishness”).   
>  
>(2) Muslim individuals will report greater hate crime experiences than non\-Muslims; Within the Muslim sub group, women will report greater hate crime experiences than men.  
>  
>(3) Overall, Muslim individuals will report lower levels of prejudice than non\-Muslim individuals.   
>  
>(4) Overall, across both the Muslim and non\-Muslim group, prejudice will negatively correlate with openness to experience, agreeableness and neuroticism.

For the first hypothesis, I used a spearman's R correlation test.

For the second hypothesis, I used one\-tailed independent samples t\-tests. 

The third I used a one\-tailed paired samples t\-test.

The final hypothesis I used a spearman's R correlation test for each individual personality type.

Whenever I tried to conduct ANOVA's the output was always empty \- so I'm not sure what I've messed up or where.

Any advice? Do these seem like the appropriate tests to use?

Many Thanks!",How to test these hypotheses in SPSS? Struggling with MSc dissertation analysis.,8qj6og,new,2,1,1,0
"Please can anyone advise me what sort of test I should do to find out whether a particular frequency distribution differs from expected? The expected distribution is known, not unknown. 

Specifically, I'm trying to test whether two dice rolled together are weighted, when only the sum of the two is written down. I have written down the results of a few hundred rolls, and want to see whether this frequency distribution differs significantly from the expected for the sum of two n-sided dice.

Thanks for any advice in advance!",Not sure how to test a particular hypothesis,8qhs1h,new,15,8,8,0
,Difference between theoretical statistics and mathematical statistics?,8qfy1u,new,5,6,6,0
"Let's say that X and Y are strongly negatively correlated.

Let's also say that X  and Z are weakly positively correlated.

What can we then say about the correlation between X and Y, when controlling for Z? What are the possibilities, and what would have to be the case for those possibilities to be true?",Correlation question,8qfi4g,new,3,0,0,0
"I want to A/B test two separate back\-end recommendation logic (A and B) but I'm not sure which method works best (from a statistical standpoint). Here's how my application works:

1. User goes to my application.
2. Application recommends 5 widgets to purchase (using either Logic A or Logic B)
3. User can choose to either buy or not buy.
4. User can use this application over and over again.

I narrowed down to 2 methods of A/B testing.

* The first method is to randomly divide the users up into bucket A and bucket B. Bucket A will contain Logic A and bucket B will contain Logic B. **Example:** If Bob gets bucketed to Bucket A then Bob will always get Logic A.
* The second method is to randomly serve up Logic A 50&#37; of the time and Logic B 50&#37; of the time to the users. **Example:** Bob could get Logic A the first time he visits the application. But then he could just as well get Logic B the second/third/fourth time he visits the application.

In both cases, I can keep track of which logic the user books or doesn't book from. Is method 1 or 2 better to see which logic works best?",Best way to implement A/B testing?,8qecis,new,4,0,0,0
"Hello, I was wondering if anyone would please indicate me packages in R that enable you to do fairly basic linear algebra, such as: calculating the rref of a matrix, characteristic polynomials, etc... I already know of the functions det(), solve() and eigen().",Packages for Linear Algebra Using R,8qe2jk,new,4,2,2,0
"I have been teaching a statistics class for a few years.

One of the most difficult concepts to teach students is the idea of a sampling distribution (and how it is different from a distribution of data)

When I introduce the idea of a sampling distribution, I ask students to imagine that a person is taking a random sample of 2 people from a population of 4 people and that the ages of the population members are 20, 22, 24, and 26.

I tell the students that the person is going to record the ages of the 2 people and calculate the mean of the sample.

Next, I ask students to figure out the probability that the sample mean will be 21.

To solve the problem, I have the students calculate the means of the six possible samples, plot the sample means on a number line, and then count the number of sample means (out of 6) that are equal to 21.   

I teach it this way so that students can easily see that there are different possible samples that could be selected, that each possible sample has a sample mean, that the sample means have a distribution.

Later, we move into more realistic examples in which I draw a normal distribution and I tell the students that the normal distribution is an outline of the sampling distribution. 

However, I have a few concerns.

1. Students might be getting confused because it seems like an artificial situation that would never happen in a real life.
(probably no one would ever randomly sample 2 out of 4 people)

2. As we get further into the topic, students might be failing to make the connection between the distribution of 6 sample means and a sampling distribution that is shown as a curve.  After all, they might have already forgotten the example with the 6 sample means by this point.
If they fail to make that connection, then then there is really no point in starting with the ""small population example.""

3.  Students might be confused by the small population example at first because they can't see the big picture of why the topic is important.  For example, they might not fully understand the ideas of sampling variability and sampling error until we move into more realistic examples.

I was wondering if I you think that my approach might be failing to accomplish its purpose because of the reasons I mentioned above.

Also, I was wondering if I you think that I should continue with this way of introducing the sampling distribution concept or if you think something else would be more effective. 

I am trying to avoid introducing the idea of repeated sampling from a population because I think students will become fixated on it and have difficulty understanding that a person only takes one sample from a population.  Also, students are likely to confuse things like sample size and number of samples. 
",looking for advice on teaching sampling distributions,8qdww7,new,2,4,4,0
"So, I'm interested in running a linear regression that takes into account a covariate. When I think about it, I'm essentially interested in running an ANCOVA but instead of a categorical predictor it's continuous. The covariate and the response variable are still continuous as well. Is this type of analysis possible and if so, what's it called? ",ANCOVA with a continuous predictor variable?,8qdh63,new,3,2,2,0
"Hi, what is the best way to showcase your work (online) for an employer looking to learn more? Thanks.",Best way to showcase statistics work?,8qcrao,new,1,1,1,0
"Hi,

I've been having issues interpreting the appropriate p,d,q values based on these [ ACF and PACF](https://imgur.com/a/l1fyFZi) plots. I've been trying to mess around in STATA and I  get the best Wald result with (0,1,1). The data originally had a trend which I tested with the Dickey\-Fuller test and after that i took the first order of difference which would indicate why d is 1, right? 

Could someone explain how I should interpret these plots?","Deciding ARIMA(p,d,q)",8qbv6q,new,0,1,1,0
"One way to choose lambda for LASSO is to cross-validate (via k-folds) different values of lambda and then choose the highest lambda value that is within one standard error of the lowest resulting MSE.

My question is if it makes sense to iterate this process: run the k-fold cross-validation X number of times, and average the resulting MSE values for each lambda. And then, from this distribution of lambda values and *average* MSE values, choose the highest lambda resulting in MSE within one SE of the minimum. If so, does this R Code accomplish that?

    set.seed(3)
    IV1 <- data.frame(IV1 = rnorm(100))
    IV2 <- data.frame(IV2 = rnorm(100))
    IV3 <- data.frame(IV3 = rnorm(100))
    IV4 <- data.frame(IV4 = rnorm(100))
    IV5 <- data.frame(IV5 = rnorm(100))
    DV <- data.frame(DV = rnorm(100))
    
    data<-data.frame(IV1,IV2,IV3,IV4,IV5,DV)
    
    x <-model.matrix(DV~.-IV5 , data)[,-1]
    y <- data$DV
    
    lambdas = NULL
    for (i in 1:500)
    {
      fit <- cv.glmnet(x,y)
      errors = data.frame(fit$lambda,fit$cvm)
      lambdas <- rbind(lambdas,errors)
      r2[i]<-max(1-fit$cvm/var(y))
    }
    # take mean cvm for each lambda
    lambdas <- aggregate(lambdas[, 2], list(lambdas$fit.lambda), mean)
    lambdas<-as.data.frame(lambdas)
    
    #select those within 1 se of the mininmum mse
    onese<-std.error(lambdas$x)
    min<-min(lambdas$x)
    low<-min-onese
    high<-min+onese
    
    lambdas<-subset(lambdas, x>low)
    lambdas<-subset(lambdas, x<high)
    
    #from those, select the highest lambda value
    
    bestindex = which(lambdas[1]==max(lambdas[1]))
    bestlambda = lambdas[bestindex,1]
    
    # and now run glmnet once more with it
    fit <- glmnet(x,y,lambda=bestlambda)","When choosing lambda for penalized regression, does it make sense to...",8qbtig,new,2,1,1,0
"Just graduated with a BSC in Statistics in Canada and I’m looking for what’s a good baseline in terms of salary for me. I would appreciate it if we can gather some info on this :) 

1. What was your first job out of university? (Or what industry)

2. Which country did you get your degree in?

3. Where was your first job? (Country and city)

4. What was your salary? (Include currency)

Thank you!","BSc in Statistics graduates, what was your first job out of university, and how much was your salary?",8qbots,new,20,18,18,0
"I do advertising analytics work for a small company, and I'm currently doing ad testing, and I want to know if I'm using the best test to discern statistically significant differences. 

I'm currently using the default function ""Prop.test"" in R, but my proportions are really small (ranging from .02% to .15%), while my sample sizes are large (between 10,000 and 100,000)  


Is there a better way to run this analysis.


",Best Test For Analyzing Differences Between Really Small Proportions with Large Sample Sizes.,8qbnq5,new,2,1,1,0
"I’m unsure if I should take my university’s series in Real Analysis, as its 12 units. I understand the application but I’m not sure if it’s too theoretical for what I want in my career. ",How important is Real Analysis for career in Computational Statistics or BI?,8qbbyl,new,8,2,2,0
"Hi Guys,

Let's say I want to conduct an experiment in my company: I want to see if the introduction of activity B affects A. For example, will (activity B) asking each SalesPerson to call the customer at least one time instead of just communicating via email positively affect (activity A) number of successful Sales? 

I guess one simple method but not entirely accurate would be to measure the average number of (activity A) successful sales for a period of time before the (activity B) minimum one call rule was implemented and measure activity A for the same period of time after activity B was implemented. 

The best method is probably Randomized Controlled Trials but it won't work if the sample size is too small, which is the case for my company of 20 workers in Sales. 

Your help will be greatly appreciated by me and hopefully others :)",What are some ways to accurately estimate the effect of a new policy?,8qb9a7,new,12,8,8,0
"I've taken an intro to statistics and intro to probability course in my school and it didn't go too well, I managed to pass but just barely. Over the next few months I'd like to improve my knowledge and prepare for the courses I have coming up.

These are the two courses I will have to take:

* Stochastic Process - finite dimensional distributions and the existence theorem, discrete time Markov chains, discrete time martingales, the multivariate normal distribution, Gaussian processes and Brownian motion.

* Regression Analysis - Orthogonal projections. Univariate normal distribution theory. The linear model and its statistical analysis, residual analysis, influence analysis, collinearity analysis, model selection procedures. Analysis of designs. Random effects. Models for categorical data. Nonlinear models

I've searched around this sub and people reccomend books and in another thread I look at, the book seems to be put down, so I'm not too sure.

So here's what I'm looking for: A book that goes over the intro level stuff (since I didn't retain much) and the stuff I mentioned up there. I love examples, I don't like books that are too abstract where they just explain something and throw a bunch of problems at you without explaining anything. I have taken math courses up to multi variable calculus and linear algebra so I'm comfortable with math. The intro to stats and intro to probability course I took in my college was pretty math heavy I think, so I assume the rest of my courses will look like that.

Thank you!",Statistics book that isn't too abstract but also is thorough and has good examples,8qb7ac,new,4,1,1,0
"I am statistics student and have just finished my first year. What books would you suggest to go more in-depth into statistics? What books should all statisticians read?

I am fine with books being a little maths heavy.

Edit: Thank you for all the suggestions. I have already placed orders for many of the suggestions.",Book suggestions for a student pursuing a degree in statistics?,8qaik1,new,18,46,46,0
I am doing a survey in Algebra 2 Honors class where we ask the students to rate our teacher. What would be the best way to conduct the survey with as little bias as possible ,I am conducting a survey...,8q9mx6,new,6,0,0,0
And why do incompetent people feel so confident in talking about this branch of science? ,Are you annoyed by armchair statisticians IRL or on the internet?,8q9baw,new,4,0,0,0
Thank you in advance ,Hi guys I was wondering if someone could explain the difference between lowercase p-hat and uppercase p-hat.,8q72bp,new,2,0,0,0
,[CONTEST] - The Lowest Unique Integer Game | x-post from /r/math,8q5qlh,new,0,19,19,0
"I'm trying to finish up my research paper but I am struggling with the statistical analysis of my information. I took a few statistics classes about 4 years ago so I have a little (rusty) familiarity with the SPSS software, but I have absolutely no idea which tests to run for my data. Any help would be really appreciated!

My project has to do with examining 11 websites and theorizing which websites would have the largest number of users based on certain features. I am not actually looking at how many users are subscribed to each website; this paper is solely theoretical.

So I have 11 websites picked out to examine. I also have 10 features picked out that I want to place on a rating scale. These features are things like buddy chats, language selection, keyword lookup, etc. Essentially, my theory is that websites that have these features will have a high number of users. Therefore, I've tried to look at this data in several ways.

First, I want to rate the priority of each feature. I want to say that the buddy chat feature is considered a high priority (coded as ""1"" or ""high""), the language selection is irrelevant (coded as ""2"" or ""neutral""), and language selection is a low priority (""3"" or ""low""). Although writing this out, it seems like I should decrease my coding to a ""1"" or ""2"" (high or low).

Then, I have to look at each website and decide which features it has. I list the feature as either present on each website or absent.

Finally, I want to compare all of that data to decide which website has the most number of ""high priority"" features. At this step, I want the websites rated against each other, so I could essentially rank them 1 through 11. 

Any thoughts? Even constructive criticism on my approach would be appreciated at this point. ",Struggling with Statistical Analysis for a Research Paper,8q42sv,new,1,2,2,0
"What's a good textbook for somebody comfortable with set theory to learn probability theory\* from that will have applications in it and not just pure measure theory?  I'm a third year pure math/computer science student and have taken an introduction to real analysis, an introduction to abstract algebra and have some experience with R.   

Book we're using in class is Mathematical Statistics with Applications by Wackerly, Mendenhall and Scheaffer and they seem to be afraid of set notation, rigour, and formality. As a specific example they spend almost two pages defining this thing called the ""mn-rule"" which is just ""the order of the Cartesian product of two sets is equal to the product of the orders"" but they spend about two pages drawing boxes and talking about it instead of just giving the simple, concise definition. It doesn't appear to be a bad book (I've been working through the questions in it and they've all been effective in teaching me what's necessary), the non-math majors in the class quite like it, but it lacks the rigour and formality I prefer in my textbooks and I find it kind of confusing because they're (poorly) redefining all of these things I've learned in a more formal manner.  

I've learned a little measure theory, and whilst fascinating, all the books and references tend to veer away from it's application to probability and focus mostly on pure abstraction which isn't all that useful for an application based stats course.   

If there's a better place to ask please feel free to direct me in that direction!   

\*The course syllabus specifies the following:  
Elements of probabilistic modelling, Basic probability computation techniques, Discrete and continuous random variables and distributions, Functions of random variables, Expectation and variance, Multivariate random variables, Conditional distributions, Covariance, Conditional expectation, Central Limit Theorem, Applications to real-world modelling",Suggestions for a different textbook for an introduction to probability course,8q3ofa,new,6,17,17,0
"Hi All, 

I was reading this paper (https://arxiv.org/pdf/1106.4513.pdf) which address the question of defining the performance period and timeframe in credit risk. Since the paper didn't provide any code, I thought of writing the code in R to see if I match the results which the author has presented in table 4 (page 7). Unfortunately, I am not able to validate the results, would someone please help me verify R code and guide me on what I am doing wrong? Here is the code: 

#Step1: Create table 2: Transition Matrix from real data
stateNames <- c(""Closed"", ""Current"",""X"",""30"", ""60"", ""90"",""120+"")
ds <- matrix(c(1,0,0,0,0,0,0,0.02,.66,.31,.01,.0,0,0,0.04,.17,.71,.07,.0, .0,0,0.04,0.04, 0.15, 0.45, 0.3, 0.03,0,0.06,0.01, 0.02,0.03, 0.33, 0.49,0.06,0.03,0.02, 0.01, 0.01, 0.02, 0.26,0.66, 0,0,0,0,0,0,1),
              nrow=7, byrow=TRUE)
row.names(ds) <- stateNames; colnames(ds) <- stateNames
ds

#Step2: Create the canonical form of the matrix
ab <- c(""Current"", ""X"",""30"", ""60"", ""90"",""120+"", ""Closed"")
aa <- matrix(c(0.66,0.17,0.04,0.01,0.02,0,0,
               0.31,0.71,0.15,0.02,0.01,0,0,
               0.01,0.07,0.45,0.03,0.01,0,0,
               0,0,0.3,0.33,0.02,0,0,
               0,0,0.03,0.49,0.26,0,0,
               0,0,0,0.06,0.66,1,0,
               0.02,0.05,0.03,0.06,0.02,0,1),
              nrow=7, byrow=TRUE)
row.names(aa) <- ab; colnames(aa) <- ab
aa

DW <- matrix(aa,7,7,byrow=TRUE)
DW #Canonical Form of transition Matrix

#Step3: Create matrix for transient states
DWmc <-new(""markovchain"",
           transitionMatrix = DW,
           states = c(""1"",""2"",""3"",""4"",""5"",""6"",""7""),
           name = ""Drunkard's Walk"")
DWmc

P <- canonicForm(DWmc)
P

# Find Matrix Q
getRQ <- function(M,type=""Q""){
  if(length(absorbingStates(M)) == 0) stop(""Not Absorbing Matrix"")
  tm <- M@transitionMatrix
  d <- diag(tm)
  m <- max(which(d == 1))
  n <- length(d)
  ifelse(type==""Q"",
         A <- tm[(m+1):n,(m+1):n],
         A <- tm[(m+1):n,1:m])
  return(A)
}
Q <- getRQ(P)
Q #Transient matrix 

#Step4: Calculate Mean Transition Time Matrix
# Find Fundamental Matrix
#Next, we find the Fundamental Matrix, N, by inverting (I – Q)
I <- diag(dim(Q)[2])
I

N <- solve((I - Q))
N

Comparing results from matrix N to table 4 the results looks different. Am I doing something wrong? 

Please note: Some of the code I have used comes from http://blog.revolutionanalytics.com/2016/01/getting-started-with-markov-chains.html


",Markov Chain Delinquency Problem in R,8q3kyt,new,7,7,7,0
"I have a box (mean = 200g and standard deviation = 6g). I have a water melon (mean = 450g and standard deviation = 15g). Calculate the standard deviation of a box with 3 water melons in it.

I calculated it like this: sqrt(1*(6^2 )+3*(15^2 )) = 26.66

My classmates however say I also need to sqrt the n, so it has to be sqrt((1^2 )*(6^2 )+(3^2 ) *(15^2 )) = 45.3

Who is right? Thanks in advance",Standard deviation of 2 different things,8q37ms,new,27,21,21,0
"Anyone know how to fix this?

Here is my code:

# load the libraries
library(caret)
library(klaR)

# Set the working directory
setwd(""C:\\Documents\\R\\Data"")

# Read CSV into R
Data <- read.csv(file=""Data.csv"", header=TRUE, sep="","")

# Attach data
attach(Data)

# Set Variables
x <- as.matrix(Data[,2:85],header=TRUE)
y <- as.matrix(Data[,1],header=TRUE)

Data<-NULL 
Data<- train(x,y, ""lasso"", metric=""RMSE"",tuneLength = 10, trControl = con) 

con<-trainControl(method=""cv"",number=10) 

coefs<-predict(data$finalModel,s=data$bestTune$.fraction, type 
               =""coefficients"", mode =""fraction"")$coef 
coefs 


I am getting the error on this line:
Data<- train(x,y, ""lasso"", metric=""RMSE"",tuneLength = 10, trControl = con) 
",LASSO Regression - Caret: Error: wrong model type for classification,8q35w7,new,2,2,2,0
"Hi everyone,

I am really trying to understand probability after months of studying it! It is a love hate relationship and I'm hoping someone can help me love it more than I hate it lol.

If I roll a pair of dice 6 times, what is the probability of getting at most one sum of 7?

The way I approach this is in P( 0 sum of 7 or 1 sum of 7) so in one flip there is a 15/36 chance of getting a sum less than 7 and 6/36 chance of getting a sum of 7. Because there are six flips, I have to multiply each number by 6 and I get 0.583.

I have a Stats final tomorrow covering everything. Any tips on probability when it comes to combinations/permutations? Thank you all. God bless and have a nice weekend!:)",Need help with probability?,8q2ahu,new,8,5,5,0
"The greek letter μ is not 'u' but 'm'. That is the very reason it has been chosen for **m**ean. ^(If you look at it the right way, you can even see how it is similar to the letter m (when hand written. (same thing for N and  𝜈, which is not a damn v\~\~\~\~)))

And yet someone decided to call the test u\-test? Wtf?

See reply of u/normee

But also see reply of u/efrique. Apparently in Hungarian they call the z\-test u\-test. Again, I have a tingling that the reason has to do with μ. :(",Why is the u-test not called m-test like it should be?,8q2a03,new,18,0,0,0
"Say, i have a boss in a videogame that has the following chances to drop desired items:

* Item A: 1:500
* Item B: 1:760
* Item C: 1:2300
* Item D: 1:2700

What are the odds of finding one of these items killing him once?
How many times would I have to kill him until I can expect to find one of these items (100%), and how many kills for a 50% chance to get one of these items?
Side question: How would the equations/calculations be changed if the boss could drop more than one item per kill (for instance 4 items per kill)?",[Video game boss] What are the odds of finding items?,8q00dx,new,11,3,3,0
"Hello, I am slightly confused about an email. I asked my teacher what the grading scale in the class was and he replied that it was a Z score transform and the average grade is 3.0. I looked all over and could not understand what this means as I have no background in statistics. Could someone help me understand this?",Z score transform.,8pyvre,new,7,15,15,0
,"Suggestions of online free resources could be eBooks , Books , YouTube Channels , MooCs to learn stats.",8pwd3e,new,12,26,26,0
"In addition to other differences, is it fair to say that step-wise regression seeks to maximize the amount of variance explained in the training set of data, while penalized regression seeks to limit the error when predicting a novel set of data?",Is this fair to say step-wise vs. penalized regression?,8pvzht,new,1,1,1,0
"Hi, I don't have very much real world knowledge or experience usings stats so I'm hoping you all can help me understand the best statistical test to use given my data and what I want to know. If I'm posting in the wrong place I apologize.

I have a small data set of 14 individuals each performing the same task 10 times. The task is a pass fail so 1 or 0 for each trial. The 14 individuals are further divided into two more groups based on their previous experiences. I am wanting to test if there was improvement in completing the task over the course of the trials for the group as a whole, for each sub group and maybe for each individual. 

I have a couple Ideas, but because I know very little beyond the basics I was hoping someone could give me a test that would fit my requirements, and then I could read up on it and run it in R. I don't want someone to do it for me. I just need some direction.

Any help is appreciated thanks.",Testing for improvement over time.,8pvjwk,new,3,3,3,0
"Here's my simple linear mixed model:

m1 <- lme(Y ~ Group + minutes, data = mydata, random = ~1|ID, na.action=na.omit)

14 athletes took a supplement (A or B, designated by the Group variable) then ran some distance. A week later they took the other supplement then ran again. Which supplement they took first was random.

Every 2 minutes, ""Y"" is measured. These people ran about 40 minutes so there's 20 measurements * 2 groups * 14 subjects = ~550 measurements despite just having n=14 participants.

I ran the model (tried it with minutes*group interaction, also tried it with random = ~minutes:ID) and ""Group"" has a p-value of ~0, but the coefficient is tiny. Definitely not clinically significant. I think the reason it's still statistically significant is just because I technically have so many observations even though they are really just repeated measures. Is this correct and just a standard part of doing mixed modelling? I kind of thought the repeated measures aspect would've been taken into account when calculating the p-value for ""Group"".",Is it normal for lots of repeated measures to sort of artificially drive the predictor variable p-value downward?,9pnciw,new,0,1,1,0
"I'm attempting to do an analysis of how the selling of our products change every month. How can I go about weighting the percent change so that I identify which products were the most impactful? For example product A sold 15,000 units in August, and 20,000 in September, Product B sold 3000 units in August and 6500 in September, and Product C sold 3 units in August and 15 in September. Product C has the highest percent change but its volume is so low the impact is minimal. I would be most interested in studying product B more closely to determine the reason for the change. Imagine I have thousands of products to go through I need to order them accordingly.

Thanks in advance",How do I weight percent change based on volume?,9pn6uz,new,0,1,1,0
Is there a way to compute the mean of all the cases instead of the variables? Ive been trying to create a new column for the mean of all the cases under one variable but i dont know how. I know it can be analyzed through descriptive analysis but it would create an output instead of a new column. I need the new column with the mean values since i'm going to correlate it with a different set of mean values. Is there any way to find the mean of all the cases under one variable which would produce a new column and not an ouput?? Help :(,Need help!!,9pmvnx,new,2,0,0,0
"Hey all,

I'm in the middle of running a whole heap of Poisson and negative binomial regressions via SPSS. Everything was going smoothly until about my 8th regression I encounters this issue:

""The hessian matrix is not singular. Some convergence criteria are not satisfied""

&#x200B;

What I'm confused about is each regression is using the same IV but each with a different DV. If the hessian matrix is calculated using the IV's how come some models work and others are getting this error?

&#x200B;

Additionally, what steps would I need to take to fix the error?

&#x200B;

Thanks!",Hessian Matrix not sigular in Poisson/negative binomial regression,9pmfeg,new,3,7,7,0
"The authors of the popular book *Introduction to Statistical Machine Learning* claim that the variance of the estimate of the MSE of a model increases when k increases, when using k-fold cross-validation.
To the extreme, leave-one-out cross-validation, should exhibit maximum variance.

I know that this claim is somehow controversial, and I have read a few papers about it (Bengio and Grandvalet, ""No Unbiased Estimator of the Variance of K-Fold Cross-Validation""; Kohavi, ""A study of Cross-Validation and Bootstrap for accuracy estimation and model selection"") and followed the [recent discussion on CrossValidated](https://stats.stackexchange.com/questions/61783/bias-and-variance-in-leave-one-out-vs-k-fold-cross-validation).

I would like to set up a computational experiment, to independently verify this claim.
My idea is to work on synthetic data (analogously to what the author of the CrossValidated answer has done).
To this end, I will generate a roughly linear dataset: y = 1.5 * x + e, where e ~ N(0,1) is the error.
Let's say my dataset consists of n points.
I can then perform k-fold cross-validation, for all values of k from 2 to n.
For a fixed k, I will train the k models and I will obtain k MSE's, say MSE_1, ..., MSE_k.
The MSE associated with k, then will be the average of these MSE's.

By repeating the above experiment a large number of times, say m, generating each time new data, I could then get a pretty accurate value for the ""true"" estimates of the MSE's given by each value of k.
The estimate associated with a fixed k, would be the average over the m simulation of the MSE associated with k in each of the simulations.

I would like, however, to decompose this into variance and bias (squared).
I have the feeling that knowing the underlying distribution of the error should allow me to calculate both the bias and the variance, but I am unsure on how to proceed.

Can someone shed light on this?

Also, what if instead of generating new data at each of the m iterations, I just work on the same dataset, but I simply shuffle it before applying k-fold? How would that impact the robustness of my results?",Computationally proving (or disproving) ISML's claim that variance increases with k in k-fold cross-validation,9pl7eb,new,0,1,1,0
"Hi all,

&#x200B;

I was wondering if anyone has any good resources that helped them to understand moment and probability generating functions? I believe it might be a weak point in my maths (have not studied power or generating functions ) rather than statistical understanding. I've tried going through the often recommended textbooks (Wasserman, Casella, Wackerly etc) and it just doesn't seem to click for me.

  
Can anyone help me out? Was anyone in a similar position and found a particularly useful resource?

&#x200B;

Thanks

Sef",Moment/Probability Generating Functions - Struggling to understand,9pl4qn,new,3,9,9,0
"so the title pretty much sums it up. How would I interpret ORs for an ordinal logistic given categorical predictors? Would i just treat the same as logistic when there is categorical? 

Cheers
",interpreting odds ratios from an ordinal logistic regression with categorical predictor variables,9pl1et,new,0,2,2,0
"What could the possible reasons be for a spatial autoregressive model (lagged on the dependent variable) to not significantly reduce autocorrelation in the residuals? I checked for spatial dependence between residuals, added the lagged term, fitted the SAR model and then checked for autocorrelation in the residuals- there are still the same number of certain regions rejecting the Moran's/Geary's test for no spatial autocorrelation..

&#x200B;",Reasons for why SAR model not significantly reducing spatial autocorrelation,9pkasa,new,0,2,2,0
"I've literally never understood the concept of sampling. So first off, does ""a sample"" mean literally one point sample, or is it a collective noun for a set of points all ""sampled"" independently?  

Also, how do you do this ""sampling""? Say I have the standard Normal distribution, and maybe a Bernoulli distribution. How would I sample from these? I think I might understand it better if I actually code these (which I want to do on my own), so I'd like an intuitive explanation.",How do you sample from a distribution?,9pj6a9,new,12,10,10,0
"Statistics Question

 

I have been trying to workout how SPSS works and it is pretty ok in terms of what to use and how to get to those setting. But I am stuck on how to recode a different variable into another.

For example, I have to recode inch into foot. I know that you have to use a table which you can convert the values (which is from your own data) into the new value(conversion table of cm-inch/2.54). As I was doing that, I realized some of the values that I collected are the same and spss would not let me use the same value as an input. So I just continue with all the other values that are not the same. When I ran the syntax, and went to the dataset tab, it gave me nothing but only a label of ""foot"" as the variable. Does anyone what I am doing wrong??

This is the syntax:

COMPUTE highinch=Height/2.54.

VARIABLE LABELS highinch 'COMPUTE highinch=Height/2.54'.

EXECUTE.

RECODE highinch (37=23) (39=24.5) (36=23) (40=25) (43=27.5) INTO foot.

VARIABLE LABELS foot 'american shoe size'.

EXECUTE.

Thanks

&#x200B;

&#x200B;

&#x200B;

Edit: I have tried using a different value for each data piont and it doesn't work",How to recode into different variable?,9phq89,new,0,1,1,0
"http://www.maths.usyd.edu.au/u/gartht/GarthTarrICORS.pdf

The scale estimator they suggest is interesting:

* Given a sample X₁, ..., Xₙ take all the pairs (Xᵢ, Xⱼ)
* For each pair (Xᵢ, Xⱼ) compute the pair averages: (Xᵢ + Xⱼ)/2
* Finally, the scale is estimated by taking the Interquartile Range (IQR) of all the computed pair averages.

If this seems weird, the sample variance actually can actually be derived in a similar way. 

It can be shown that the variance of a distribution is equal to E[(X - Y)²/2], where X and Y and are two i.i.d random variables drawn from that distribution. This is the case because :

E[(X - Y)²] 

= E[X² - XY  + Y² - YX]  

= E[X²] - E[X]E[Y] + E[Y²] -  E[X]E[Y] 

= (E[X²] - E[X]²) + (E[Y²] - E[Y]²) 

= 2Var[X]


(Remember that E[X] = E[Y] and E[X²] =E[Y²] because the X and Y are from the same distribution.)

So given an i.i.d sample X₁, ..., Xₙ from some probability distribution we can estimate the variance by taking some pair  (Xᵢ, Xⱼ) and computing (Xᵢ -  Xⱼ)² /2. 

While this estimator will give us an unbiased estimate of σ², it's not a very good one. What we need to do is combine all these crappy little estimators together into one good one. Here is a better estimator of σ²:

* Given a sample X₁, ..., Xₙ take all the pairs (Xᵢ, Xⱼ)
* For each pair (Xᵢ, Xⱼ) compute a crappy estimate of σ² given by sᵢⱼ² = (Xᵢ  - Xⱼ)²/2
* To combine all the crappy estimators into one good estimator we take the average of all the sᵢⱼ² . Call the average s²

It isn't too hard to show that s² is equal to the usual unbiased sample variance (the one with ""n-1"")

",Efficient and Robust Scale Estimation [PDF] - A set of slides describing a highly efficient method of robustly estimating the scale parameter from a set of data,9ph8ft,new,1,4,4,0
"An archaeological excavation at Burnt Mesa Pueblo showed that about 10% of the flaked stone objects were finished arrow points. How many flaked stone objects need to be found to be 90%  sure that at least one is a finished arrow point?

&#x200B;

EDIT: so I figure out the answer is (.9\^n) </= .1 and n=22 but like I forget how this is explained.",Quota Question,9pgxec,new,1,0,0,0
"I'm a sophmore stats major, and I've kinda fucked up. The midterm for my probability class is on Monday and I've missed far too many classes to get a grip on whats going on now. It's an early morning class and I've given myself too many excuses not to go, now I'm feeling straight hopeless on this midterm. I know the concept of a PDF, PMF and CDF, but after that the math is just escaping me. I've always done good in my math classes but fuck this one you can't fuck around in, which I'm learning the brutally hard way. I need urgent help in this class, if anyone knows an online service to learn probability I will be eternally grateful. The textbook I'm using isn't even the right edition and is cryptic as fuck so it's taking me hours just to comprehend a single chapter. As a stats major I feel like I fucked myself. SOS!!!! ",Feeling so lost in my Probability class,9pg2sq,new,7,1,1,0
"12 of the top 20 finishers in the 2009 PGA Championship used a Titleist brand gold ball. Suppose these results are representative of the probability that a randomly selected PGA Tour player uses a Titleist brand gold ball. For a sample of 15 PGA Tour players.

a). Compute the probability that exactly 10 of the 15 PGA Tour players use a Titleist golf ball

&#x200B;

Am I supposed to be using binomial distribution, i'm confused",I have an exam tomorrow and i'm stuck on a practice question,9pfkme,new,1,0,0,0
"Hi, 

So I am working with my secondary advisor on learning more about permutation tests as they relate to n of 1 clinical trials, and I am having trouble translating it into code. 

I understand the theory behind the test and the sampling process, but could anyone present a simple applied example with similar outcomes?


*Take for example*: intervention= drug X on pain in pediatric patients

outcome= analog scale of pain from 1-10, for a group of 6 patients. 

Also say you know the baseline means for each patient and need to simulate/don’t know the post intervention. ",Simulated data and permutation tests:,9pewep,new,0,2,2,0
"I realize that I've never actually asked why this is an issue. 

I know that one problem is that you might be missing explanations for the effects you observe. E.g., if you test ten kids from New York and ten kids from Houston, and find that the New York kids have a higher typing speed, BUT all of the New York kids went to the same school, then you have a hard time making the case that New York vs. Houston is explaining the effect. Instead, the effect could be arising due to specific schools. 

Is there another reason other than the one I alluded to above?",What exactly is the problem with nonindependence of observations in regression?,9pelpm,new,3,2,2,0
,"I do well in my stats classes, but I don't find statistics intuitive. What can I do to develop intuition for statistics and probability?",9pd8oo,new,25,57,57,0
"I had a statistics exam and there was a question that even now, I'm trying to conceptualize, and was wondering if I can get some feedback.

The question has to do with finding out the value of x, from the z transformation formula z = x - mean/SD.

The mean in the question is 800, and i forget the SD. This uses a standard normal distribution.

To find the answer, I looked for the area within the z table that was between the mean and supossed x value of .025. Basically I looked for the area that was 0.025 within the mean. My logic comes from this; I have to calculate the area that is 2.5% short of 800 (the mean/demand in this question). Since the distribution is symetrical, I can use a positive value in the calculation, and in using the z transformation formula I found the x value ; which ended up being like 805. something.

&#x200B;

Is my logic/conceptualization of the question correct? Or should I have looked at the area 2.5% and beyond (into the tail ends) of the curve. My friend did that, and got a value of 900 something. The question seems to ask to find the x that would ensure theres only 2.5% chance of you being short of the 800 demand though, so that doesn't make sense.

Can some reassure me haha.","How many boxes to buy, to ensure you are only 2.5% chance short of missing the demand.",9pd1d0,new,4,2,2,0
"Hi,
If one were to perform linear regression with a bunch of variables including ones that are (highly) correlated, lets say correlation coefficients of 0.8, I believe the estimated coefficients will not be reliable.
However, I have read that this does not affect the quality of the (out of sample) fit, and such a model might still be a better (predictictive) model than if we would have correctly removed some of the correlated variables.
Is this true? If I'm only considered with the quality of my model, and not interested in the coefficients, could I just use a lot of (correlated) variables?",Linear regression: correlated variables,9pbpqf,new,3,2,2,0
"Often, I find myself a bit confused on power. I understand it's related to beta and the probability to make a type II error (false negative). In other words, power allows you to be confident that there is no difference between samples.

With that being said, if we do find a difference between samples (at a given alpha), then does power even matter?",Statistical testing: does power matter if you find a difference between samples?,9paenq,new,38,7,7,0
"I am looking to determine the optimal cutoff point for a 2x2 table based on an ROC curve. Im a bit out of my depth as far as the algebra goes, but we are making this ROC curve based off a logistic regression model, and determining the optimal cutoff for the group comparisons I have chosen is a bit tricky, so we are trying to solve it algebraically. 


Here is what I came up with - the cutoff corresponding to a table with probability P (e.g .5) should be [ln(P/1-P) - intercept] / slope - is this correct? As far as I understand, the ln(P/1-P) represents the ln(odds ratio) - or logit, but will -intercept/slope give the cutoff for maximum accuracy when P=.5?

If this is unclear or requires more detail, please feel free to ask any questions!


edit for clarity --


SO - 

I have 3 ROC charts which compare 3 groups in 3 different ways
Group 1 - considered no disease (dx)
Group 2 - considered moderate dx
Group 3 - considered potentially fatal dx

Group comparisons based on a parameter measured as a percent drop in a certain cell value (eg group 1 median % drop = -22%, group 2, -50%, group 3- 90% - not real numbers).

Group comparisons I use - Group 1 v Group 2+3 - Group 1 v Group 2 - Group 2 v Group 3. 

With the ROC curves I have generated, I cant use a single cutoff because in the Group 2 v Group 3 comparison, the sensitivity at cut off of .7 is 100%, with 0% specificity, whereas, with the Group 1 v Group 2 comparison, a cut off of .7 yields ~85% sens and 70% spec. It doesnt sit well with me to just use 1 standardized cutoff, because the 3 ROC curves are testing this predictive parameter at different % drop levels, so it would be inappropriate to use the same cutoff for each. Furthermore, this being a test which may predict between fatal dx and non-fatal dx staging, I want my tests to demonstrate different levels of sensitivity, to favor either a higher fpr/tpr for higher dx stages, or to exclude more fp in lower stages.

I hope that made sense, it is a bit difficult to explain without looking at the data, but this is the jist of it. If you have any qeustions feel free to ask!!",Question about optimal cutoff points for ROC curve - algebra included,9p9ns5,new,19,15,15,0
,[HELP] is there any software like minitab or better for the iphone/ipad? plz..,9p9h78,new,1,1,1,0
"I have a theoretical question: 

I have four independent sets observables and am testing the null hypothesis that \mu_1 = \mu_2 = ... using one-way ANOVA. I have performed T-K multiple comparison tests for each pair of variables and will be reporting p-values as normal. 

My question is: when I'm making a visual figure of the data, and I'd like to construct confidence intervals on each set of data, what confidence interval do I use? 

My two choices are: 
- ""basic"" confidence intervals with t-distribution
- using 'contrasts' i.e. if my initial model is y_{nx1} = X_{nx4}*b_{4x1} + errors... then my CIs can be defined using contrasts such as c = (1, -1, 0, 0) such that: 
CI = \mu_i +/- t^{-1}*s*sqrt(c^T*(X^T*X)^{-1}*c)

In general, I'm also confused as to why confidence intervals for your parameters in ANOVA are not the same as confidence intervals for each individual observable.

I'm sorry if the above notation is cryptic and/or outdated.. I learned linear models from Scheffe's text and it's great but also feels like I'm reading hieroglyphics sometimes. 

Thanks!",CIs for a single array vs. CIs for separate arrays in one-way ANOVA,9p94be,new,0,0,0,0
"Hey,

&#x200B;

so i have to correlate random effects between two longitudinal models, to see whether he to dependent variables have simmilar growth curves over time.

So  one option would be to fit two univaraite mixed models, one for each dependent variable. Or I could fit a bivariate model. I understand that it onlymakes sense to use a bivariate model when I assume the the two dependent variables are not independent from each other. 

&#x200B;

However I do not relly understand what a bivariate model ""does"" given there is a correlation between my dependent variables.

So does it correct for the bias that would occur when i fitted two univariate apporaches?

&#x200B;

Could I say using a bivariate moel corrects the results for the correlation that exists between the dependent variables?

&#x200B;

I hope I explained my issue in an understanable manner, if not I am happy to rephrase or elaborate.

Thanks in advance",Univariate vs Bivariate Mixed Model,9p8jnu,new,0,1,1,0
"**Hypothesis:** calcium content in population A is higher than population B.

**Experiment:** atomic emission spectroscopy to measure metal content.

**Result:**

Measurements of 3 samples each from A and B. Means are compared with unpaired t-test. P-values are:

    Calcium p=0.03
    Sodium p=0.85
    Potassium p=0.61
    Magnesium p=0.04

What's happened here is I got more results than I asked for because the AES machine measures lots of elements at once. My question is where do I apply multiple-comparison-correction?

My gut feeling is I should correct Na, K, Mg but I don't need to correct Ca, because Ca was my original hypothesis and the P value should stand on its own. Is that right?",Multiple comparison correction when one test was planned?,9p7rvy,new,18,8,8,0
or are they all equal? Im just slightly confused. Can someone explain to me the intuition behind the answer. Thank you for your time! ,Is P(x > -u) = -P(x < -u)? or is P(x > -u) = 1 - P(x<u),9p6olk,new,6,0,0,0
New student in stats and couldn't find this answer anywhere. Thanks in advance.,What is the formula for figuring out a sample's Z-score when hypothesis testing with a sample of two or more?,9p57qj,new,2,2,2,0
"I need to test for heteroskedasticity using Egarch garch en arch. I'm aware how to do this using the formulae and in eviews for 1 single stock or index, but im at a loss when i need to do it for multiple stocks/indexes.

&#x200B;

Can I just aggregate the returns and residuals? can I just add more dependant variables in eviews/stata and run the same test as for one dependant variable? Currently I'm learning about R and the RMGARCH package but tbh i just started using it a couple hours ago and I have no clue how it works :).","Multivariate(?) Garch, EGarch and Arch models",9p420n,new,0,1,1,0
Anyone know where I can find a good free applet for demonstrating sampling distributions with histograms?,Sampling distribution applet,9p30m2,new,1,2,2,0
"I am trying to analyze some repeated measures data, the measures are physiological measurements such as heart rate, taken on a group of people at one  minute intervals over 30 minutes. The group is subdivided on two conditions, stress-control and low income- high income, that  I would like to consider as predictors in a regression. I would like to see whether there is a statistically significant interaction between stress and income categories. So far, I have fit smoothing splines to the data and created some graphs to compare the results visually. The curves are quite complex, rising and falling throughout the duration of the stress test. This is why I want to use nonparameteric regression methods.

I initially was going to use a generalized additive model (GAM) that included as predictors: a smooth function of time, and linear estimates for the categorical variables. But then I realized this might create spurious results because it is repeated measures data, and I need to somehow account for the within-subject correlation of the repeated measures (there are 30 observations for each person, so I think the parameter estimates for the linear terms will be incorrect.

Wondering if anybody has ideas of how to deal with something like this?

&#x200B;

Thanks very much.",Nonparametric multivariate regression for repeated measures... Can it be done?,9p2cql,new,13,21,21,0
"I'm in the early stages of a research project that would involve giving respondents the same survey more than once. What would be the best statistical test to capture how reliable the survey is through retesting? If anyone has any literature on the subject, I would be happy to read it. Thanks in advance!",Statistical test for survey test-retest reliability,9p27rn,new,0,1,1,0
"Hello - 
I am working on a study which is looking at a clinical assessment tool for predicting disease vs no disease (dx). It is a percent drop in a specific value that predicts which stage dx the individual will end up in.  I have generated a ROC curve, with an area under curve = ~.8.
How do I interpret this data, and what do the cutoff values indicate? I.e - When generating the 2x2 table on R, the package I use asks for a cutoff value, and the sensitivity/specificity values (as determined by the 2x2 table that is generated), change based on the cutoff I choose. How do I determine which cutoff to use, and can anyone kindly give me their best understanding of ROC interpretation?","Need help understanding ROC curves, 2x2's, and cutoff points for sensitivity/specificity.",9p1m2t,new,12,2,2,0
"I'm doing Electrical Engineering but I need to use **MCMC** & **Bayesian Additive Regression Trees (BART)** at a Data Science internship that I'm doing. I have absolutely no idea where I should start and any material I find online seems to be too advanced for me to follow. Don't get me wrong, I would like to study these topics in full detail, it's just that I'd prefer a source which starts from scratch (say, the Bayes' rule) and then proceeds to more advanced concepts.

&#x200B;

Note, I'm not looking for programming tutorials at the moment. Just cold, hard theory to get a solid understanding. Just enough so that when I use relevant python/ R packages, I would actually know what I am doing.  


Thanks!","Any detailed, step-by-step tutorial to MCMC theory for students with an engineering background?",9p1d3i,new,14,13,13,0
"Hello,

I am applying to quantitative psychology PhD programs and my linear algebra professor offered to write me a letter of recommendation. He wants me to write a little blurb on the things I learned in his class that applies to my field.  I learned matrix decompositions, eigenvalues/vectors, Gaussian elimination, and pretty much all general things linear algebra.  I’m having trouble with this task as I understand both fields (linear algebra and statistics), and I know many statistical techniques use matrices in notation, but I need to combine them in a way that would be helpful for my letter. 

Any help is appreciated thanks! ",My linear algebra professor is writing me a LoR and needs help.,9p13h7,new,7,1,1,0
"Hello! So, to make things short, I'm a grad student in a behavioral neuroscience program and I study the effects of neuropeptides on play behaviors in rats. I am not very well-versed in stats that extend any further than simple ANOVA and t-test, so I'm definitely struggling. 

My most recent experiment tested the effects of 4 separate treatments over 4 play sessions on 4 separate days, within subjects (so each rat received each treatment by the end). Because of the nature of play in rodents, this guarantees an effect of time. The order of the treatments was randomized (not counterbalanced \*sobbing\* because this was our first time doing a within-subjects repeated measures paradigm and... well... we can now call this to a learning experience). 

So... what I need help with is analyzing the data in such a way that will give me a play measure based on 1) the treatment received, 2) the day that treatment was received, and 3) the sex of the animal. If you need any more info, I'll add it at the end.

Thank you to anyone in advance. This level of stats knowledge just isn't a thing in my field, since repeated measures aren't used that often. I've gone to other faculty members and the best advice I have gotten is to try to find a way to 'nest' variables, but she wasn't totally sure if that would be the answer. 

\---

Here's a more detailed explanation of what my data looks like:

* I have 4 treatments (control, peptide 1, peptide 2, and peptide 3)
* Animals were randomly assigned to receive each treatment over 4 days, so one animal's experience could be, for example: 
   * Day 1: peptide 3 
   * Day 2: control 
   * Day 3: peptide 2 
   * Day 4: peptide 1

What is making things more difficult, is the fact that play levels in rats decline as the novelty wears off, so the play test on the first day is not going to be the same as the 4th, even without pharmacological manipulation. So I'm looking for some way to control for the variability of time while also controlling for the treatment. This ALSO means, that the control treatment could potentially act as a baseline if we could control for the variability of time. Sigh. Maybe I'm fucked. Desperation is encroaching. Please let me know if you have any advice at all or if you would like further clarification. I could send you a screen shot of my data if you, for some crazy reason, are really excited about helping me. ",Looking for help on how to analyze repeated measures with time as a co-variant in SPSS,9p0upt,new,7,9,9,0
"I'm a junior at a smallish state university in Minnesota. I started as an engineering major at a Big 10 college, but transferred for personal reasons. I then decided I wanted to study medicine as a doctor, PA, etc, but I don't think that's for me either. 

I have lots of math courses from my pre-engineering background - an A in Calc 1 and 2, and a B in Calc 3 and Linear Algebra. I'm taking an intro statistics course and am loving it. I'm thinking about pursuing a career in it. 

My college also has a Data Science undergraduate program, one of the best ones in the area if my professors are to be believed. I have little computing/programming background except for an intro Java course I got a B in, and a small amount of R, Python, and JMP I have from my math/stat courses. I have about 90 credits after this semester, and am about 15 credits or so from graduating with a Biology degree. 

&#x200B;

My question is, what career options would you recommend for me? 

\-Should I finish the Biology degree and take as many math/stat/data science courses I can fit and and consider graduate school, like my professor advises? How difficult is it to get into a good graduate school if I get a pretty good GRE score (75th percentile) and have a 3.5 GPA? 

\-Should I stop taking the Biology classes I've come to hate and focus on getting a degree in Stats/Data science? 

\-What jobs could I get with either of those two options? In the case of graduate school, is it worth getting a PhD or stopping at a masters? 

&#x200B;

Thanks in advance to anyone who can get me advice!",Career advice for a College Junior,9p0ujx,new,1,1,1,0
"A professor has noticed that even though attendance is not a component of the grade for his class, students who attend regularly obtain better grades. 

&#x200B;

In fact, 30% of those who attend regularly receive A's in the class, while only 10% of those who do not attend regularly receive A's. 

&#x200B;

About 75% of students attend class regularly.  


Given that a randomly chosen student receives an A grade, what is the probability that he or she attended class regularly? (Round the answer to four decimal places.)

&#x200B;

The answer in the key is .9000 but I am looking for an explanation to getting that answer.

&#x200B;

Thank you!

&#x200B;

\-Matt

&#x200B;

&#x200B;","Stumped on a simple problem in STATS course, help?",9p0tqk,new,8,0,0,0
"I’m brand new to stats (two classes under my belt intro and intermediate  stats). I’m having a great time learning, I’m hungry to learn more and find that I actually need practice! 

Where can I get practice problems, guided data sets, or more hands on experience? Those are the assignments that are the most helpful to me! I don’t want to take college courses (too expensive). 

Thanks! 
",Total noob! Need some guidance!,9p0suh,new,0,2,2,0
"Currently trying to do some statistical analyses on this data. All numbers are a possible score out of 6. Someone recommended that I use some sort of permutation tests but I'm not familiar with those and I'm not sure how to arrange my data to be used in a permutation test by R. Trying to look at differences between exposed and pool samples (e.g. *V. lanosa* pool vs. *V. lanosa* exposed), differences between the species in general (i.e. *P. umbilicalis, V. lanosa,* and *M. stellatus*), and differences between the independent variables in terms of where they were found (e.g. *A. esculenta, S. latissima,* and *S. nigripes* on *P. umbilicalis* pool vs. *A. esculenta, S. latissima,* and *S. nigripes* on *P. umbilicalis* exposed).

Any help would be greatly appreciated!

Data is number of samples out of 6 that were found to contain DNA from any of *A. esculenta, S. latissima, and S. nigripes.* 

||*A. esculenta*|*S. latissima*|*S. nigripes*|
|:-|:-|:-|:-|
|*P. umbilicalis* pool|4|0|2|
|*P. umbilicalis* exposed|2|2|5|
|*V. lanosa* pool|1|3|3|
|*V. lanosa* exposed|0|0|1|
|*M. stellatus* pool|4|6|0|
|*M. stellatus* exposed|4|4|0|

&#x200B;",What tests should I do? How to I arrange this for R?,9p0nrx,new,0,1,1,0
"I’m meeting with quantitative support managers, statistican I’s and II’s, a master statistician and a BI manager. 

Ive been out of college for about 5 months. What should I be prepared for and do I need to review everything I’ve ever learned?

",What to expect from an all day interview for a statistician I position?,9p0hz9,new,3,3,3,0
"I have an MS in stats and I'm working as a biostatistician but never took any biostats class aside from survival analysis. I want to get better at my job so I can feel more confident about applying to the next one.

Are there any must-read books for this field?

",What are some important books to read for being a good biostatistician?,9ozpad,new,14,9,9,0
"***Question In textbook Below with the steps***

**d.** Between what two values of Z​ (symmetrically distributed around the​ mean) will 98.76  
of all possible Z values be​ contained?

First determine the percentage of all possible Z values that are outside of the two values of Z.

The percentage of all possible Z values outside the two values of Z is **1.24**

​(Type an integer or a​ decimal.)

Let the lower Z value be Upper Z Subscript Upper LZL   
and the upper Z value be Upper Z Subscript Upper UZU.   
Use the fact that the distribution is symmetrical to determine the area in each tail of the distribution.

**The area in each tail is 0.0062 <-----**

&#x200B;

***My Question about this***

I know  where the 1.24 came from and how to get it, you simply subtract 100% and 98.76%.. **But how do you get the area in each tail via excel?** (this professor wants you to focus on using excel rather the z chart graphs for next semester using stata then python..

my apologies if this is against the rules to ask for help like this.",question on finding tail or area,9ozeme,new,2,0,0,0
"According to the technical notes of the survey I'm using, their weights already account for non-response.

Would I be incurring in any gross error by simply using `na.rm=T` with my estimation commands?",Is it necessary to modify survey weights when analyzing a variable with missing observations?,9oysvs,new,2,0,0,0
"Hi,   
I am trying to build a machine learning model for predicting house prices based on a number of features related to the house (size, number of rooms, number of bedrooms, garden (1/0), location, etc..).  
In order to increase my understanding of the problem I have decided to start off with a simple linear regression. When doing so, I observe a number of results:  
\- I am getting pretty good R-squared and R-squared adj. for my fits (around 0.78).   
\- Not all my variables are statistically significant (p value < 0.05). Which indicates that I should drop them.  


Now, I have decided to one-hot-encode my location feature, which contains cities in which the houses are. So for each city I have an added feature which can take values 0 or 1. Interestingly, in the fit report, some of the city variables seem to be statistically significant, whereas some others are not. How should I go about this? Does this mean that I should drop those cities that are not significant entirely?  


Also: in general, if I drop the non significant variables my adjusted R-squared still seems to drop. Should it not be the case that this would not affect my adjusted R-squared?  


I'm just a bit lost. My overall model does pretty well if I judge it based on adjusted R-squared, but within the model there are a bunch of variables which do not do anything for the model. Leaving them out lowers R-squared so what am I supposed to do?",Understanding simple linear regression,9oyeee,new,6,0,0,0
"I have river flow rates for two separate rivers

&#x200B;

The data produced this scatter graph for both rivers

&#x200B;

[https://i.imgur.com/jVnWF3t.png](https://i.imgur.com/jVnWF3t.png)

&#x200B;

What is the best type of correlation to use here?

&#x200B;

Spearman, Pearson or Kendall?

&#x200B;

Thank you.",Correlation,9oxzsw,new,1,0,0,0
"I'm working on my thesis but unfortunately due to a small sample size I've ended up with almost entirely insignificant results - is it ok for me to comment on say the coefficient and r-sq value even though it's not significant? For example: X and Y were not significantly correlated however if this were significant then X would have a negative relationship with Y and 80% of the variation in Y would be caused by X

Also as a side note, does anyone have a way to prove the sample size is too small? I'm unable to do a power analysis because it's a pilot study in a relatively uncharted field",Working around insignificant data - is interpreting other aspects of analysis still ok?,9owkq4,new,9,1,1,0
I hear a lot about how statistics BSc doesn't allow for relevant jobs - Master's degree at least. But what about Finance vs Statistics for employ-ability at the undergrad level? ,Which undergrad degree is more employable: Finance or Statistics?,9ovqkm,new,4,0,0,0
"I'm interested in calculating the relative risk of certain events across regions, and I would like to use population counts as controls. I would like to see if these incidents occur at specific locations because the intensity is higher there or because the population is clustered there.

The incidents in my dataset have corresponding lat/long coordinates- however I have population counts by census tracts. I'm not sure about the concept of intensities entirely- but should it matter if I just have population counts by census tracts without their own lat/long coordinates? After all intensities are calculated using areas. Can I calculate the intensities by census tracts and find the relative risk by census tracts as well?

I am confused because I was thinking that I would need the coordinates for the controls as well to really understand if events are occurring where the population is clustered. However, I don't have coordinates for each population point- would this be a problem when I need to calculate relative risk and evaluate the constant risk hypothesis?

&#x200B;

Edit: Can population density be a control since it includes the cases of incidents as well?",Question about Relative Risk (Spatial Statistics),9ovmvw,new,0,1,1,0
"Hi /r/statistics! I'm beginning a project and finding I'm a bit out of my element, so I figure this is the place to be. Before I get into my project, here is short explanation of why I'm doing it. Feel free to skip to the next section if you just want to jump to the project.

# -What led me to this-

In December, Super Smash Bros. Ultimate, the newest game in the series, will be releasing. Among many other features, the game will have over twenty stages that could be viable for tournament play. The Smash series has always struggled with finding stages for tournaments, since so many of them are designed based around fun free-for-all games rather than serious one-on-one fights. For comparison, the two most popular Smash entries each only have six legal stages. Compared to that, seeing over twenty potentially tournament quality stages is kind of insane.

In a current tournament, games are played in a best of three set, or best of five for the final rounds. The first stage is selected by having each player strike two stages from a starter list of five, and playing on the remaining fifth stage. This is intended to make the starting battle as neutral as possible. The winner of that match may ban any stage then, and the loser gets to choose from any of the rest. This gives the loser a bit of a comeback option, but without completely screwing over the winner. Repeat the preceding step until an overall winner is determined.

This system was developed because in Super Smash Bros. Melee (an older but still very popular entry in the series, and the game which kicked off competitive Smash) stages have a huge role in various character matchups, so it's important to make the first battle as neutral as possible. Second, with so few stages it was a simple answer. The later games in the series followed suit, since they also had small numbers of legal stages. It was also a sort of ""If it ain't broke, don't fix it"" attitude. I am of the opinion that the system is not perfect, however. It is sometimes confusing for players and viewers, limits the amount of variety seen in the game, and seeks to solve a problem that I feel is no longer a problem (I don't think stages have nearly as much impact on matchups as they did back in Melee). 

However, with Smash Ultimate on the horizon with an unprecedented number of stages, I am of the opinion that we should take a hard look at how we're handling stage selection rules. Much of the community is already looking at ways to cut the huge number of stages down to a mere five, thus allowing them to keep using the striking system we've always used and removing a ton of potential content from the tournament scene. Some are considering a seasonal stage rotation, but that comes with many organizational problems. Others are looking at ways of grouping stages of similar layouts - something even more confusing than what we already have.

Which brings me to:

# -My project to see the impact of Stage selection in Smash 4-

As a viewer and competitor, I think the impact of stages in Smash 4 is grossly overrated. Stages certainly affect how the game is played in the moment, but do not actively affect game outcomes as much as players think. Individual character matchups, and of course player skill, are what truly affect the game, in my opinion. This can be seen in the fact that high tier characters are strong on any stage, and low tier characters struggle the same on any stage. Players can be seen losing on their own counterpicked stages frequently, or choosing to counterpick to a stage their opponent already won on. A character could be playing on their so-called best stage and still lose all the same. 

My intent is to see exactly what kind of effect stage selection actually has on competition. Do stages affect results? Does counterpicking actually help? Do a character's supposed best stages actually reflect that with results? Ideally, these results will show if we should continue using our existing stage striking system or find something new, which will hopefully reconcile the following needs:

* fair competition
* making things interesting for viewers/players (ideally by using as many of the tournament quality stages as possible)
* keeping things simple enough that players and tournament organizers can understand and logistically implement it

# -What I have so far-

I've made a very rudimentary file with the data of two major tournaments included. There are multiple sheets in the workbook, with each stage's data on a separate sheet. It uses only the matches from the top 16, to be certain I'm only including skilled players. I've tracked who each player used, what stages were used in every match, which player chose that stage or if it was the starter stage, and who won the game. I included character dittos (where each player chose the same character) but I only included that for posterity, and don't think it should affect any data (that information could be interesting independently though). My goal is to include the data of every major tournament from the last year, or more if time permits, but I don't want to enter more data until I've figured out the issues I'm having.

Here is the file: [https://docs.google.com/spreadsheets/d/1gp9gqgq5hnEEUX1QMBnbF2nSRjqXTd6HKdV89S\_6KC8/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1gp9gqgq5hnEEUX1QMBnbF2nSRjqXTd6HKdV89S_6KC8/edit?usp=sharing)

# -What I need help with-

Turns out my skills with Excel/Google Sheets have been forgotten since I last needed them fifteen years ago. I'm entirely uncertain if I'm doing things in a way that will be easy to translate into readable data. Is it a good idea to have each stage's data in separate sheets like this, or is it better to organize another way? Should I even be using Excel/Sheets at all, or would a database program like Microsoft Access be better? I do want this to be shareable with the public later. 

Also, I'm terrible with the functions in Excel. I think I can relearn the basics with a crash course online, but if anyone has some obvious and simple tips I could use to turn this particular data into something readable, I'd appreciate it. I specifically want to be able to pull the following information when I'm done:

\-How often each character wins on each stage, regardless of if a starter or counterpick

\-How often each character wins/loses on the stages they themselves chose in counterpicking

\-How often each stage is chosen as a counterpick by each character, and against each character

\-How often each stage is struck to as the starter, both overall and per character

\-Win/loss ratios of every character overall and for each matchup, ignoring stages (for comparison).

\-Possibly other information if I feel I need it later? I think I covered it all here though.

And of course if there's any other insight to give, I'll gladly listen. Thank you very much to anyone reading my giant essay. :)",Analyzing Smash Bros. character and stage data - Looking for advice on organizing data,9ovkrg,new,18,23,23,0
,What tool can I use to make a regression on two time series that have a noisy time shift between them? (details in text),9ovi2x,new,6,7,7,0
"Hey All,

I am wondering what math and statistics subjects I should read and learn on my own to prep for a course in Statistical Theory. I typically use sites like edx, brilliant.org or khan if the material is in depth enough. There are three different levels available for this course and I will be in the section (level 2) that does not require Mathematical Analysis. This is our textbook: [Probability and Statistics](http://professor.ufabc.edu.br/~nelson.faustino/Ensino/IPE2016/Livros/Morris%20H%20DeGroot_%20Mark%20J%20Schervish-Probability%20and%20statistics-Pearson%20Education%20%20(2012)%20(1).pdf)

I have the prerequisites which are through multivariable calculus and linear algebra. No experience with any proof classes. This course does not require having a proof based course but from viewing previous semester’s homework and exam prep, everything we will be graded on will involve doing proofs and applied problems after solving the proofs. 

Thanks for the help!!",Statistical Theory (Graduate) Prep,9ovgqw,new,5,1,1,0
"Hello Everyone!

I’m a zoologist in my final year at university and I’ve long known the extreme importance of statistics. However from whatever little I was taught in the statistics course that was part of my syllabus, I can say that I haven’t truly grasped the beauty of the subject. 

Is there any book I can read to widen my understanding of statistics? Something along the lines of the Feynman Lectures for physics?",Introduction to Statistics,9ovgea,new,0,3,3,0
"So CMU 700-level (intro phd intermediate theoretical) STAT has 2 alternatives, the Larry Wasserman course (different course #), and this one: [http://www.stat.cmu.edu/\~siva/700/main.html](http://www.stat.cmu.edu/~siva/700/main.html).

&#x200B;

Wasserman's lecture notes (and text) quickly become incomprehensible for me.  I've had a decent amount of stat.  The Siva STAT is excellent - it's maybe \~5% too easy, but reallllly good.

&#x200B;

However, I'm pretty sure the later lectures are missing.  Calendar arithmetic, though not explicit, seems to imply we're missing the last 6/16 or so?  You can see from the course page I link that we only get the first 10.  He says on the traditional syllabus that the course page may not be updated past a certain point - guessing that's what happened: [http://www.stat.cmu.edu/\~siva/700/syllabus.pdf](http://www.stat.cmu.edu/~siva/700/syllabus.pdf).

&#x200B;

I'm hoping someone knows where I can find them, or has them.  I looked a lot.   I'd kill (or gladly pay) for these.

&#x200B;

Thanks!","Excellent ""easier"" CMU 700 STAT: Missing Some (<1/2) of the Lectures",9ov2q3,new,3,4,4,0
"I am looking for help analyzing a questionnaire. The questions that I want to analyze are yes/no, across a target group and control group. I would like to analyze the data and I am finding conflicting information on the best way to do this. 
Ideally I am looking for a way to do this in excel, as I am not very familiar with other programs.","Questionnaire analysis, 2 groups, yes/no answers",9ouhiv,new,0,1,1,0
"Like the title. I know there are many of them, but I only know a few, such as neuroscience, healthcare, web, AI.",What is the first application of ML on the top of your head?,9oswm7,new,21,9,9,0
My thought was a chi-square test using 50 correct and 50 incorrect as the expectation?,"If a person answers 100 true/false questions, what test could be done to see whether they performed greater than would be expected by chance?",9osru8,new,12,6,6,0
[https://www.indexmundi.com/facts/united-states/quick-facts/cities/rank/percent-of-people-of-all-ages-in-poverty](https://www.indexmundi.com/facts/united-states/quick-facts/cities/rank/percent-of-people-of-all-ages-in-poverty),is there a similar website with poverty rate for cities that is recent?,9oskrp,new,3,3,3,0
"The coefficient and intercept produce a negative number, say -2.  I can exponentiate by e to get .135. Does this make sense for predicting frequency? The intercept is about -2 and the coefficients have little effect on the estimate (also, what is the name of this value pre-exponentiated with e?). Also, if it's getting exponentiated, why would -2 be used instead of positive 2?     ",GLM log link question for frequency Poisson dependent.,9oraq2,new,3,2,2,0
"Okay so my research question is “is the severity of seasonal affective disorder (SAD)  related to pre-existing stigmas about depression, revealing a vulnerability factor?” Is this one or two tailed? Thanks!",Can you help me figure out if this is a one or two tailed test?,9oqvac,new,6,0,0,0
"Is there a limited number of outliers you can in any given data set?

For example, would it be impossible to have 90 outliers in population of 100. At what point is the line drawn? How do you go about calculating that?",Number of possible outliers in any given data set.,9oqiu9,new,5,0,0,0
"Hi all,

For a final project in my forecasting class ran a linear regression model of the impact of the National Parks' budget on visitor rates by park.

Looking at the final summaries, RMSE doesn't seem to be correlated with r squared and the F value at all. I have a lot of parks where the RMSE is bad but the r squared and F value are good and vice versa.

What does this imply exactly? I'm trying to figure out what to say in my write-up and I'm a little confused.  I *do* have some parks with a good RMSE and a good r-squared so at the very least, I can talk about how the model seems to have worked for those but I'd like to understand what the rest means.

Thanks so much!",Implications of a bad RMSE and a good r squared/F value (and vice versa),9opvde,new,2,1,1,0
">One intuitive way to gauge the margin of error of a poll is to see how likely such a poll is to accurately detect a swing in the electorate.  Suppose for instance that over the course of a given time period (e.g. a week), 7% of the voters switch their vote from not-A to A, while another 2% of the voters switch their vote from A to not-A, leading to a net increase of 5% in the proportion p of voters voting for A. How does would this swing in the vote affect the proportion \overline{p} of the voters being polled, if one imagines the same voters being polled at both the start of the week and at the end of the week?

>If the poll was conducted by simple random sampling, then each of the 1,000 voters polled would have a 7% probability of switching from not-A to A, and and a 2% probability of switching from A to not-A.  Thus, one would expect about **70 of the 1,000 voters polled to switch to A, and about 20 to switch to not-A,** leading to a net swing of 50 voters, that would increase \overline{p} by 5%, thus matching the increase in p.  **Now, in practice, there will be some variability here; due to the luck of the draw, the poll may pick up more or less than 70 of the voters switching to A, and more or less than 20 of the voters switching to not-A.** 

For the 70 of 1000 and 20 of 1000. Isn’t this wrong? Even though each person has 2% and 7% chance of switch, they have dependent probabilities. 

The prior assumption says 7% and 2% of population. So we have our prior the actual population parameter (lucky). 

Random sampling means everybody has an equal chance so 1/100 = .01 chance of being selected. 

So wouldn’t the probability for the sample be the probability of being selected and being a part of the 7% or the 2%. (After some further research wouldn’t we also be getting into inclusion probabilities?). Additionally wouldn’t we be assuming without replacement? 

While they acknowledge this weakness more or less here (sample of 1000):

>Now, in practice, there will be some variability here; due to the luck of the draw, the poll may pick up more or less than 70 of the voters switching to A, and more or less than 20 of the voters switching to not-A.

They state this before it:

>Thus, one would expect about **70 of the 1,000 voters polled to switch to A, and about 20 to switch to not-A,** 

Theoretically can we expect this? (Practically we obviously)


Edit:

Out of curiosity does this borderline the territory of [Confusion of the inverse ](https://en.wikipedia.org/wiki/Confusion_of_the_inverse) .

P(switch|being sampled) vs P(being sampled|switch). ",Is the voter poll population for this thought experiment right or wrong?,9opmp2,new,0,1,1,0
"Can someone please help me understand how to get to the answer below in the picture. I'm studying for my Statistics final and probabilities have been a bit stressful. I don't get what my teacher is doing to get the answer listed. Is there something I'm doing wrong? I've tried just multiplying the two numbers with their probabilities in the exponents. I've tried doing that while multiplying by 10 and even dividing by 5. 

&#x200B;

The main question was: There is a firm that reports approximately 30% of their accounts recievable are overdue. There is a sample of 10 accounts. What is the probability that exactly 5 are overdue?

&#x200B;

[https://imgur.com/a/HDdPV1T](https://imgur.com/a/HDdPV1T)

&#x200B;",Probability Help,9oov2u,new,4,0,0,0
"So, I played a drinking game in which you play based on rolling a dice. Lowest number lost and had to take a shot, drink, or something like that based on the rules. I started wondering about the odds. What made me perplexed was the fact that equal numbers meant loss. So in a 1v1 a losing hand would be lower and equal to your opponent. Surely the odds must be equal because you both are in the same situation. But what happens when more players join, more specifically more than six players. Do the odds stay at 1/6 losing chance for the rest of the game, no matter how many joins, or is there some statistical shenanigans at play here? In my limited university statistic (I am only a filthy economist) I couldn't remember a formula that covered that situation. Also, in my head I felt like after six players join, the odds for losing would be the same regardless, because of equal distribution. But boy, I feel so weird about it. Like somehow you should be marginally better of, hiding in the safety of the numbers.

Does anyone want to put my poor, uneasy mind to rest on the matter? :D  


EDIT: Haha, you guys are absolute animals! Thank you so much. ",Question about drinking game!,9onpri,new,12,7,7,0
"hi guys and gals,

I am working on a statistics assignment and im dealing with a data set about an experiment about an attention test. now i want to find out how three independent variables affect the accuraacy of the answer. i have the independent variables:
condition = {control, dot, inverse}
block = {1, 2, 3, 4}
lag = {1, 2, 3, 6, 8}.

The assignment says 'we treat the proportions as “values of measurements” '. i am just not sure what they mean by this. are they saying to treat 'block' and 'lag' as a continuous? 

thank you","what does 'treat these proportions as ""value of measurements"" mean?",9on8e3,new,4,0,0,0
"[The first study of why people struggle to solve statistical problems reveals a preference for complicated rather than simpler, more intuitive solutions](https://www.eurekalert.org/pub_releases/2018-10/f-wdw100518.php)",Why don't we understand statistics? Fixed mindsets may be to blame,9ol2uc,new,18,50,50,0
"[A link to the question and my current working out](https://imgur.com/a/iHraeot)

I am trying to answer this question and am stuck on Question 1 B which asks for a hypothesis test. 

So far I have attempted the question by answering the null/alternative hypotheses and the observed test statistic part of the question (*although I am unsure if what I have done is correct*), but am stuck on how to determine the null distribution and how to calculate the p value. 

Thank you",Help with hypothesis testing - figuring out the null distribution and calculating the p value,9okreb,new,0,0,0,0
"Hello, I don't know if this is the appropriate place to ask this but I couldn't find a sub specifically for basic intro questions.

So given a data, how do you decide which measure is best? Is it simply the presence or absence of and degree of outliers?  I know median is more robust statistic but why would anyone ever use median over mean in a data without outliers?","Given a sample data, when is it preferable to use the median over the mean as a measure of center?",9ojoza,new,15,9,9,0
"I don't understand how multiplying two groups by an opposite integers and all others by zero allows us to more accurately determine if those are the groups that differ significantly and thus contribute to the F-score.

&nbsp;

More specifically, I don't understand how by different groups having opposite contrasts that the FWER is decreased. Can someone refer me to an algebraic explanation?

&nbsp;

edit: idk if my tag is the best one. let me know if I should change it",Why do planned contrasts allow us to identify how groups within an ANOVA contribute to the F-score?,9ojmzs,new,0,2,2,0
"Hello, I have a simple question which I can't find any information about.  I may be asking the wrong question.

I have data with two conditions, but where each data point in a condition is a curve (vector 30 points long, they look basically like a normal distributions).  I want to test for differences between the two conditions, and currently, I'm taking the mean of each condition and running a Kolmogorov-Smirnov test.  But is there a test I can run which preserves all samples and accounts for n-value.  For example a small difference between the two means should still count as statistically significant if there is a high n-value.  

Just to add one condition essentially has a higher kurtosis, on average, than the other.  It's noisier than that but that is the basic behavior.

The only idea I have so far, is to put all the vectors of a condition in a 2D matrix and run a 2D KS-test, but I'm not sure if that's a valid approach.

I truly appreciate any help.",Two-sample KS-test with repeated measures,9ogyq3,new,1,3,3,0
"I tried transforming this with first differences and also with logs (adding a constant since I have negative values) but I can't get a stationary series.  What approach should I follow in order to find stationarity? I am attaching the plot.

&#x200B;

&#x200B;

[https://imgur.com/q65s8LN](https://imgur.com/q65s8LN)",Transformation of non-stationary​ time series data,9oggz4,new,2,3,3,0
"If they are dependent, does it matter if they are linearly or non-linearly dependent on each other? Or is the effect on the regression model the same regardless?","In regression, the predictor/explanatory variables are supposed to be independent but...",9og1da,new,12,12,12,0
" 

I have 15 time series representing the sleep and waking activity for each as measured on an wrist activity monitor. I also have biometric measurements for each. 

I want to test the hypothesis that their activity levels influence their biometric measurements. 

I am unsure what test I would run here. On the surface, I would simply have some regressoin model, with (for example) BMI as an outcome, and compare average activity levels between each. If I simply only had averages, this might make sense, but I have measurement taken every 30 seconds for a 24 hour period and I want to maximize the use of the data available.   


What would I use here?",Compare biometric outcome with time series for 15 individuals,9ofz5g,new,0,3,3,0
"Here is an article and interactive visualization (which I helped create) describing how uncertain our measurement of mean is as a function of the number of experiments/trials run.

https://treynorconsulting.com/measure-mean/

Would appreciate any comments or questions.",How well did we measure our mean?,9ofefr,new,3,3,3,0
"I guessed on this question and got it right. Can someone explain how to do it. 
    Question: The sample standard deviation of a distribution is 2.5. Assuming that the sample set is 9, find x and the variance if the mean is 15. ",Explanation,9ofc1d,new,3,0,0,0
"Beginner trying to learn r and ggplot2. 


Question regarding the ggplot2 book by Hadley Wickham.


Anyone read/have it can chime in whether they think it is helpful?


There are 2 editions:


1st edition from 2010 & 2nd edition from 2016



I've been tracking the price and the 2nd edition is really expensive imo. Whereas the 1st edition can be had for $10 or $15. So I'm thinking...is there a big content difference between both the editions? Or, to throw it out there, anyone have a copy they are looking to sell?


I know both can be had online for free but looking to pick up a physical copy to sit down and study with. Appreciate any thoughts.  Thanks",Anyone read: ggplot2 by Hadley Wickham?,9oex34,new,7,7,7,0
"Hello,

I have two variables.

Temperature values at two locations, A and B.

&#x200B;

Is it possible to find correlation by putting values for the two locations A and B on the x and y axis, respectively (On the same graph)?","Data Analysis, Correlation",9odonz,new,3,3,3,0
"Which  is  the difference   between a   LOSS  function  and  a   smooth function regression or  a   spline regression? 

&#x200B;

 I cannot  understand the difference.  Could  you please,  let me understand?

Thank you.",I have a tecnincal problem about understanding the regressions,9obyxw,new,1,0,0,0
"Hello  could  you help me  with  this?

I am  analizing  some data  of  a  dataset,   less than 60  data. they  have  a NOT an  homogeneus  variance.

Obviously I want to find mean  and  standard  deviation, and the median  and the quantil.    Do you  think that   applying the bootstrap  to   the  value I  have  found  can  help  me to get  better results?  Maybe  I am  confused,  but is  it  the  bootstrapping, beside  from  the  regression bootstrapping, also used  for   things like mean , SD, and  median  useful?",Bootstrapping,9obox1,new,13,1,1,0
"I’m working on statistical analysis and representation for the changes in leg angles in the babinski reflex. Positive responses (upward flexion of the foot for example) is represented as a negative angle change while a normal response (downward tilt of the foot) is positive. I’m performing a paired ttest to assess whether a difference in response exists when a hard force is used compared to a moderate to elicit these responses. The results show a negative angle change in moderate forces in all 20 participants while a mixture of positive and egative angle changes are seen in the hard force. What I want to know is when representing the data in graph, using a mean angle change, do I just use the values as it or do I use absolute values? The graph looks kind of funny with the negative and positive values. And if the values are to remain as is, when adding standard deviation error bars would I add a plus or minus? In normal graphs where the data is positive we are instructed to use a plus error bar. What’s the difference?  Also same question for a scatter plot for a Pearson correlation, a mixture of negative and positive or absolute values?  I hope this was clear enough to get some kind of answer. Thank you ",Would it be incorrect to calculate mean using negative values and plotting an inverted bar graph?,9obfsa,new,4,1,1,0
"So if I 400 patients and for my first independent variable, which is dichotomous, 350 scored and 50 scored 2. Would that give me enough power to do an analysis if say I have 3 more or less equally distributed independent variables?
Is there a rule of thumb?","To have adequate power, how much inputs/scores should you have for say a dichotomous independent variable in a logistic regression.",9ob4wn,new,1,3,3,0
"  

I am a 32 year old with a M.S. in Geology. I've worked 7.5 years in the environmental consulting field, but I've never cared that much for this field (more of there isn't really much other jobs in Florida for geologists). Getting into project management over the last year, but finding that I prefer technical and field work, but I need to pay the bills somehow.  I've had an interest in Statistics for years. I enjoy analyzing data and predicting the future and I feel like I really should have majored in Statistics. So far this year I've taught myself statistics through linear regression and spatial statistics, as well as python and R. 

My question is, should I go back for an M.S. in Stats? Or more specifically, would it be a good investment at my current age (32). It would probably take 3-4 years of part time schooling before I finish (considering I'd have to take prerequisite calculus I-II and linear algebra). Will I have difficulty finding a job? Would I be obligated to become a project manager or can I stay focused on the technical work? ",Changing from environmental to statistics career,9o8wzm,new,7,6,6,0
,What are the latest/ hottest problems or topics arising in the field of statistics right now?,9o835q,new,66,63,63,0
"I am a huge american football fan, namely college football.  There are various computer models and polls and committees that rank the teams from best to worst.  I have been looking at college football statistics for a few years.  and now I'm trying to develop my own ranking system.  But I'm stuck on a few things.

In college football there are 130 teams in the FBS, and each team plays 11-13 games a year.  Most teams will never meet.   
 So how can I semi-accurately rank which teams are better than others?

I've come to the idea that when team A and team B meet, team A's offense plays team B's defense, and vice versa.  So ideally I want to rank all the offenses and all the defenses and create a combined ranking.  But I need to understand how i can scale or normalize the data based on the relative strength of each team's opposition.  

For example, if A has a good offense, and B has a great defense, and A doesn't perform as well as they have in previous games against weaker opponents, I want to account for that.  Or perhaps B appears to have a great defense, but they've only played teams with sub-par offense.  But I need to rank the offense and defense if I want to scale the data.  And if I rank the defenses, scale the performance of all the offenses based on this, then this might change the output of the defensive ranks. 

I know this seems convoluted or confusing, I apologize, I'm confused myself, as to where to start.  If Team A plays a weak schedule full of weak opponents, on paper, statistically speaking, they will look great.   But as soon as they play a very good opponent, they may be exposed as a sub-par team overall.

The only thing I've found related to this is trying to normalize online reviews (1-5 star) based on the fact that some people consistently give products 3-5 stars while others typically rate products 1-3 stars.  So a 4 star rating from the second group is more valuable than a 4 star rating from the first group.  ",Methods for rating sports teams that never meet? Data normalization? IDK.,9o7r19,new,3,0,0,0
"Hi, can someone please explain to me this  thing, I didn't  understand?

T t, the trend component at time t, which reflects the long-term progression of the series (age variation). A tendency exists when there is an increasing or decreasing direction in the data. The trend component must not be linear. \[1\]     C t , the cyclic component at time t, which reflects the repeated but not periodic fluctuations. The duration of these fluctuations is usually at least two years.    

&#x200B;

 S t , the seasonal component at time t, which reflects seasonality (seasonal variation). A seasonal model exists when a time series is affected by seasonal factors. Seasonality occurs in a fixed and known period (for example, the quarter of the year, the month or the day of the week). 

&#x200B;

  I t, the irregular component (or ""disturbance"") at time t, which describes random and irregular influences. Represents the residuals or the rest of the time series after the other components have been removed.  So a time series that uses an additive model can be thought of as  
 So a time series that uses an additive model can be thought of as  


yt = T t + C t + S t + t t

while a multiplicative model would be  


y t = T t × C t × S t × i t.  


An additive model would be used when changes around the trend do not vary with the level of time series, while a multiplicative model would be appropriate if the trend were proportional to the level of time series

&#x200B;

&#x200B;

&#x200B;

My question is what is  the meaning of this sentence?

&#x200B;

''An additive model would be used when changes around the trend do not vary with the level of time series, while a multiplicative model would be appropriate if the trend were proportional to the level of time serie''

&#x200B;

When the  changes do not  vary  with  the level  of the series....   a multiplicative model  when  the trend was  proportional   to the  level  of the time series

&#x200B;

What  does it mean?  Ans  why  additive  model for   the first and  moltiplicative model  for the second?

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;",Decomposition of time temporal series,9o7k6e,new,1,1,1,0
"I think i need some help with my analysis of my data for a project and presentation.  I did some digging into my data and created this list of things i noticed:

""MONTH"" - Month the inflow took place in.  4 consecutive months.

""CONTEXTID"" - ID for the subgroup of the companies.  112 different context IDs.

""PARENTID"" - ID for the parent company. 12 different parent companies. 

""STATE"" - State where the provider is from.  Going to be divided into 4 different regions.

""SPECIALTY"" - Specialty of the provider.  22 different specialties.

""ERRORGROUP"" - Grouping the error was put into.  23 Different error groups.

""ERRORTYPE"" - Type of error it was. 7 different error types.""ERROR"" - The error that it was entered as. 3381 different errors.

""INFLOWCNT"" - Inflow count. Only continuous variable, it is also the dependent variable.

The 5 questions i needed help with were:

What are the top drivers of Inflow and how do those change month to month?

What clients are top drivers of the inflow metric?

How has those clients performance impacted the overall Inflow Rate?

Why did Inflow get worse in January?

What recommendations would you make to improve inflow?

What other data might you need for further or better analyses?

So far i have tried some linear regression analyses as well as some graphs such as boxplots.  I can provide the data I am using as well as anything needed along the way through R or any other means.

&#x200B;

Thanks for the help!",Help with a data analysis project,9o6j8k,new,3,0,0,0
"I'm doing a study to compare the rates of algae growth in water that contains fertilizer and water that doesn't contain fertilizer, and I'm not sure whether I should use a t-test or a regression. I'm measuring the rate by measuring the amount of dissolved oxygen in the water each day for 2 weeks. At first I was thinking a t-test since I'm comparing two categorical groups, but I don't know how you would calculate the mean of a rate, or if it would even make sense to do that. But I'm not sure how the regression would work since I need to compare the rates of the two groups to each other. Would I do a regression analysis and then an ANOVA test to compare the two groups to each other?",What statistical test should be used on R studio to compare the rates of two groups?,9o6bk5,new,5,1,1,0
,"I have a cohort of patients that receive 4 different kinds of treatment A,B,C and D. What statistical test should I be using to compare the duration of two of these treatments, say A and B ?",9o60ry,new,10,0,0,0
"I was wondering how reasonable it would be to use a continuous variable and a categorical version of that continuous variable (based on some cutoff value) in a logistic regression model. What issues  can I anticipate from doing this? I'm using this for a propensity score matching question.

Thank you!

&#x200B;

Edit: The dataset has 1320 observations with around 30 variables.",Variable selection question,9o4cni,new,11,7,7,0
,What is the best online resource (video or website) to learn about Time Series analysis?,9o4c5n,new,3,5,5,0
"Not sure if this an appropriate way to ask, but I'm not seeing an easier way available in this subreddit.

&#x200B;

I would like to propose having reddit user flair for this subreddit.

&#x200B;

I think it is important to have distinctions between different levels of expertise within statistics.

For example, if I had a user flair of ""beginner"" it may affect incoming comments differently. Other users may choose to explain and help differently too, depending on flair.

&#x200B;

It could also affect credibility of comments for those that identify as ""expert""

&#x200B;",Questions for Mods - User Flair,9o3imw,new,3,12,12,0
"Hello i am new to this. My supervisor jas asked me to run a polychoric correlation on my questionnaire data and have done this in spss. He has said i can enter the output into spss to perform a principal components analysis on the data. Is this easily done in spss?
Thanks
Kam",using polychoric correlation,9o36fp,new,0,2,2,0
"When  do you  consider  more  ''robust''  and    reliable  the median  instead of  the  simple mean? 

Are  there  any situation in which  the  Median  could be considered  as  a  more robust  ''parameter''  to  consider, instead of the  aritmetical Mean?","When to consider more reliable the median, rather the mean?",9o2u0t,new,40,15,15,0
"These are the MCPs I have to choose from.

1. *t*\-test
2. Tukey
3. Scheffé
4. Newman-Keuls
5. Ryan (Ryan-Einot-Gawiel-Welch-q statistic = REGWQ)
6. LSD or Fisher’s LSD
7. Dunnett
8. Duncan
9. Games-Howell
10. Bonferroni Method

&#x200B;

I'm thinking that Ryan's will give me the most power and either Tukey or Scheffé will give me the greatest confidence in not making a type I error, but I'm not sure which of the two would be best.

&#x200B;

Any help will be much appreciated!",Which MCPs have have the most power and which give the greatest confidence in not making a type I error?,9nxmpg,new,1,0,0,0
,Exists plot like mosaic plot of ChiSqr residuals but for continuous variables?,9nx9gm,new,2,1,1,0
"hi /r/statistics

&#x200B;

i need some clarification here. i have the following in my notes:

&#x200B;

1) Confidence interval is a type of interval estimate that gives a range of values in which a population statistics might lie 

2) 95% confidence interval does NOT mean that the probability of our population mean lying in the interval is 95%

3) 95% confidence interval means that if we calculated the 95% confidence interval for 100 samples, about 95 of these would contain the true population mean

&#x200B;

I'm having a hard time distinguishing #2 and #3. For #3, doesn't that mean that if we took 100 samples and calculated a confidence interval for each, if I randomly chose one of those confidence intervals, then there is a 95% chance that my population mean lies in this interval?

&#x200B;

Or in other words, for #3, another way to say it is if I took an increasing number of samples, let's say 100,000 samples and calculated each confidence interval, then roughly 95,000 of those CI will contain the population mean. Which sounds like if I have a random sample and calculate its confidence interval, then there is a 95% chance that it contains the population mean

&#x200B;

Are any of my definitions/notes off? I'm sure my logic is faulty here. Thanks!

&#x200B;

&#x200B;

&#x200B;",confidence interval definitions,9nx7nu,new,8,2,2,0
"I've been recently reading Judea's Pearl book ""Causality Models Reasoning and Inference"" and at a point he mentions:

> Finally, there is an additional advantage to basing prediction models on causal mechanisms that stems from considerations of stability (Section 1.3.2). When some conditions in the environment undergo change, it is usually only a few causal mechanisms that are affected by the change; the rest remain unaltered. It is simpler then to reassess (judgmentally) or reestimate (statistically) the model parameters knowing that the corresponding symbolic change is also local, involving just a few parameters, than to
reestimate the entire model from scratch.

With the Footnote:

> To the best of my knowledge, this aspect of causal models has not been studied formally; it is suggested here as a research topic for students of adaptive systems.

This looks like a really interesting and exciting research area. However, the book is not that recent (2nd edition is from 2009). So, this is a bit of a longshot, has any development happened in that? Does anyone know any name/article/book which relates to the intersection between these two areas?",Causal Models and Adaptative Systems,9nwvbj,new,5,19,19,0
"So for the test we will be given 3 datasets, with these data sets we have to find the confidence intervals for slope, intercept and an individual/group/population of any of the values in the datasets. 

&#x200B;

We are allowed to use excel to perform a simple regression with the data analysis toolpack during the test. However I dont know what the difference between the confidence intervals for slope and intercept are. And how to find the conf. intervals for an individual, a group and the population of the data set. 

&#x200B;

Any help is greatly appreciated!:)",Studying for test in my stats class & have some questions,9nwg2d,new,3,0,0,0
"Let X, Y be random variables, a, b non-zero real variables. Express Var(aX + bY) as a function of Var(X), Var(Y), and Cov(X, Y).",Can somebody help me solve this problem?,9nvpc9,new,4,0,0,0
"Let us consider a toy example. We measure heights of 40 randomly chosen men, and a mean height of 175 cm. We also know the std dev of men's heights is 20 cm. 

When I calculate 95% confidence interval for above example using usual formula, I get following range -

    (168.8, 181.2)

To get same results, I used following Scipy function -

    from scipy import stats
    stats.norm.interval(.95, loc = 175, scale = 20/np.sqrt(40))

What I am not getting here is why `scale` is set to `20/np.sqrt(40)` rather than 20.

This relevant [SO link](https://stackoverflow.com/q/28242593/5362515) isn't helping me either. 

What am I missing here?",Confused about calculating confidence internal via scipy,9nvaq1,new,6,2,2,0
"I cannot understand  what  are latent  variables  in the Partial Least Squares Regression, can some one help  me to understand what are those latent variables  with an example, please?",I cannnot understand what are latent variable,9nv2if,new,11,4,4,0
"This always confuses me.

If I have two variables, how do I know to do a one or two tailed test?

Is there a nice rule to apply?

This is for correlation and t-tests.

&#x200B;

Thank you!",One or Two tailed test?,9ntuwn,new,9,1,1,0
"Hello,

Is there a general rule to decide which test is most appropriate for a data set?

Pearson, Spearman, Kendall?

&#x200B;

Thank you.

&#x200B;","Pearson, Spearman, Kendall?",9ntcsa,new,8,16,16,0
I want to perform a paired sample t-test to compare two scores for the same individuals (but it is not a pre-post design; just two different scores from the same individuals). The issue is that I want to control for two other variables (covariates in a sense). Is there any way I can do that in SPSS? I am not familiar with other statistical softwares. ,t-Test with covariates?,9ntc5f,new,9,2,2,0
"I just don’t get when to use the same standard deviation or different ones and how to compare ? 

I am not a statistics major ",Can anyone explain to me unpaired t test,9ntamx,new,2,3,3,0
"Hey there!

I hope this doesn´t count as homework, but we spoke about a paper in uni and I really had difficulties understanding one of the tables. 
We really just took a short look at it, but I didn´t understand it and am trying to figure this out, so I won´t go crazy. 

So I basically have actually two questions regarding [This table](https://gyazo.com/14f0538e92b295cd4bfa219617930e25). 

If you look at ""Energy from Carbohydrate %""-row then the numbers are the percentage that carbohydrates make up of the daily energy-intake.

Now when it comes to the numbers in the brackets behind these percentages, I ´m not sure what to think.
Generally I´m pretty sure that these bracket-numbers are the standard deviation, except for the the on in the ""overall""-column.
 
Why is the bracket-number (11,6) in that column higher than every other bracket-number in this row? 
I´d expect it to be the average of all the standard deviations in that row. That obviously can´t be the case, since the average can´t have the highest value. 
Looking at the next row the ""overall-bracket-number"" suddenly hasn´t the highest value anymore. 

I have absolutely no idea what the ""overall-bracket-number"" could be and how they calculated it. 
I also thought of the standard error, but I guess it can´t be the SE, since (as much as I know) it always has to be smaller than the standard deviation. but here it would be higher than all of them.


And since I tried to calculate the standard error to see if it maybe would match the ""overall-bracket-number"", I realized that I am not sure how to calculate the SE in this case?
 
First you´d calculate the average mean of the carbohydrate %, then calculate the variance and pull the root (do you say that like this?). 
This way you end up with the standard deviation. 
Now to get to the standard error you´d divide the SD by the root of ""n"". 

But what is ""n"" in this case? Obviously the total ""n"" = 135335, but since the standard error makes a statement about the deviation of the averages to the ""average of the averages"", shouldn´t ""n"" = amount of averages? 

So in this case shouldn´t ""n"" = 7, since there are 7 averages from 7 countries? 
Or am I thinking way to much and ""n"" is simply 135335?





",Questions regarding the standard deviation and standard error,9nt4vw,new,5,5,5,0
"Hello! 
I’m really stuck as my question is pretty specific. I’ve searched high and low and have asked my advisor to no avail (because the question is specific). I am running an experiment testing the anxiolytic effects of exercise and will be using within subjects design to compare participants using exercise in  interventions vs an alternative. We will also have a control group to compare the exercise group to who would have no intervention. 

I was to use g power to calculate the sample size needed for this experiment. I was to get effect sizes from a similar study that uses the same indexes for anxiety. However, the research that I have found that has been similar uses 40 items to test trait and state anxiety. We will only be using the first 20 items as we are only attempting to capture state anxiety. No other study we have found does this and I had even struggled to find published research that displayed effect sizes, power etc. This particular study uses an effect size of f 0.14, alpha level was 0.05 and power (1-beta) was 0.80. I was under the assumption for no reason at all really that since we are using half the items we should double the effect size to 0.28. This also lead us to a manageable sample size of 28, whereas using the same figures we got to like 102 (this isn’t including the control mind you 🙃). The whole project needs to be completed by March or so and that’s including the dissertation write up so 102 ppts will not be realistic on top of exams etc. Am I right in believing with less items we would need a greater effect size? Am I also right to assume that of this 28, 14 should be in each intervention and then another 14 in the control to make it all balanced?  

My other question is that I’m very confused by the terminology on g power. I was using the anova: repeated measures within factors to calculate my sample size. Is this the same as between subjects design or does it mean something else? Am I also right to believe I will be using two ANOVAs in my analysis; one within  subjects for the experimental group (comparing interventions) and one between subjects comparing the exercise intervention with the control. 

Any help at all would be great. I wouldn’t come to Reddit unless I had really really tried because people tend to complain about helping with projects. I’ve done everything I could and have worked so hard. I just really struggle with statistics and would like to get better. At this point I don’t know what else to do or where to turn to. I have also asked friends but g power is completely new to us all.  


Much much much appreciated thank you! ",Doubling effect size for less items?,9nsy71,new,5,0,0,0
"Can  I use  the mean  squared error as  a  method  to  see  which  model    fits  the best?  My dataset is composed  of less than 80 predictors, I am  trying  to  predict  a  dependent  variable  and I want to  use MSE  as a method to  evaluate   which  regression  fits  the best.   Is it  true  that  the less  the MSE, the  better the model  fit?

&#x200B;",Can I use on a small dataset the MSE ?,9nson3,new,3,1,1,0
What’s the probability of a car I’m passing playing the same song As me? State your assumptions ,Probability of driving past someone with similar music,9ns3d0,new,4,0,0,0
"Hello fellas!  


I work for a small company in the sales team and I got my hand on their production plan of this year. I want to analyse the heck out of it (nobody in the company is really responsible for this, so they always to it in a rush, which is a pitty!)

&#x200B;

We only one kind of product in different shapes and colours. Nothing more. Therefore, the production plan is rather overseeable and it contains:

&#x200B;

Date of order

Product shape

Product colour

Customer (we have always the same customers, we are a B2B company)

With EXTRA 1 (yes/no)

WITH EXTRA 2 (yes/no)

WITH EXTRA 3 (yes/no)

Special wishes

&#x200B;

Of course, the first things were the TOP list of shapes and colours, the average order of EXTRA 1,2, 3 and a simple date order line chart. But... I have the feeling that there is still many more things I could do with the data!

&#x200B;

Any ideas? Thanks in advance! ",How to analyse the company's sales data?,9ns0jz,new,2,1,1,0
"I have ordinal data with support over 1 through 10. I want to test whether there is high dispersion with the null hypothesis is that 90% of the probability mass lies either at the median, one level above the median, or one level below the median. 

Can I use the sample median as part of my null hypothesis? Or is there another way to frame this type of hypothesis?

Example: let's say I have 100 samples of random variable X, whose median is 7. This means that the null distribution is:

P[ X = 6, 7, or 8] = 0.9

From my frequency table I see:

Observed at 6,7,8 = 850
Observed at 1,2,3,4,5,9,10 = 150
Expected at 6,7,8 = 900
Expected at 1,2,3,4,5,9,10 = 100

So my 2x2 contingency table shows Chi-squared stat = 2.77 + 25 = 27.777. P < 0.00001. Effect size=0.166.

My testing works out alright but is it okay to define null hypothesis based on data via the median? Null hypothesis should not rely on data value.

Or does it suffice that the null hypothesis is around the median? 

Lastly, how can I encode a null hypothesis such as 
P[ X = 6, 7, or 8] >= 0.9
This hypothesis contains the equality so it would improve power for alternatives of P[6,7,8] < 0.9. Can I still use a chi-square test? Or is another test more appropriate? Say, one-sided test of proportions?",Chi-squared test of central tendency around median with ordinal data.,9nq4wp,new,9,8,8,0
"Hi, 

I would like to calculate the magnitude of difference between two groups. The study paper only provides the F-statistic for the two groups along with the group size (Ex. F (1, 56) = 10, p<.05). 

Is there an equation or resource to calculate Hedge's g and the 95% CI?

Thanks. ",How to calculate Hedge's g with F Statistic?,9nn7mv,new,0,1,1,0
"I need to use Rstudio in a biology class I'm currently taking to analyze some data that was collected some years ago. I think I have to use a t-test, ANOVA, or linear regression and I'm really bad at understanding which one to use. One of my data sets includes sample site, species, and %cover of that species. I shouldn't use linear regression right? Isn't that for only 2 variables? Like how x affects y? ",Help with deciding which statistical test to use for field data,9nm3lm,new,2,0,0,0
"I've taken advanced graduate statistics courses but seem to have forgotten basic rules of probability. 

We just found out we’re having our third girl and only have girls. Can someone walk me through the basics of the probability of having girl, girl, girl? 

Just watched a video on YouTube and found it unnecessarily confusing. Do you merely divide by total possible outcomes for each event? 50% for girl one, 25% girl two, 12.5% girl three? What's an easy formula for this? ",Probability of gender- halp!,9nlyh9,new,5,0,0,0
"Also, which one would be better for finding work quickly/going to grad school for Statistics or other STEM degrees?",What is the difference between a BBA and BA in Statistics?,9nl18p,new,3,0,0,0
"False discovery rates are completely different from false positive rates, but people often think they are the same (or think the latter means the former). I'm looking to describe a similar effect, usually I do it with the example ""All lottery winners bought tickets but most lottery tickets aren't winners "".

Are there appropriate terms which laypeople would understand? I guess it's like P(B|A) vs P(A|B), which people might call posterior probability vs likelihood but I don't even think a statistician would use that language.",Looking for specific terminology,9nk7tf,new,2,1,1,0
"Hi, Latest edition of prof. Makridakis time series forecasting competitions (http://en.wikipedia.org/wiki/Makridakis_Competitions) has ended. The data set was rather large: 100K of series. If you are interested in time series forecasting, using standard or ML methods, including NNs, consider attending a two day M4 Conference in NYC on 10-11 December 2018. For the conference program and registration visit www.mcompetitions.unic.ac.cy. The speakers will include Nassim Taleb, Spyros Makridakis, Scott Armstrong, and yours truly :-)

Slawek Smyl","[N] M4 Forecasting Competition Conference, December, NYC",9njwdu,new,17,18,18,0
"i.e. GET FILE = [location], but then the same for opening a new syntax window?",Is there a GET syntax line for opening a new syntax window in SPSS? As there is for opening a dataset?,9njh40,new,1,1,1,0
"Pearson R is normally paired with T-test to test the significance or r, right?

But we have to use Z-Test because our sample size is>30,

Basically our null hypothesis is: There is no correlation

so I guess that makes our test a two-tailed test? Because we're hoping to get either both a negative and a positive outcome. Right?

How could I use the r to the z-test formula?

I know you can convert stuff but I need some help.



If there is a better hypothesis test that will determine significance of r,
from a sample size over 30 and that is easy. Please mention it, that would be greaat.
","Can you pair a Pearson Correlation Coefficient R to the hypothesis test ""Z-Test""",9nimah,new,4,1,1,0
"I uploaded a copy of the exercise that we need to do and I've been running on fumes because I can't figure out how to solve this.

Here's the link:

[https://imgur.com/90HHu47](https://imgur.com/90HHu47)",Does anyone know how to solve the MLEs and LRTs and confidence interval?,9nik59,new,5,0,0,0
So I'm comparing a quantified trait in male and females under two different conditions. I performed two t-tests (condition 1 and 2) for males vs. females. I've noticed that the males have a much greater increase (4.13 to 18.36) than the females (2.07 to 2.84). What method should I utilise to show that the males have a greater increase than females? ,"I performed two t-tests for the same population (male vs female) but each test was for a different condition. Can I compare my two t-stats or p-values? The males experienced a greater increase from one condition to the next, how can I properly present this?",9nhjbb,new,14,13,13,0
"Hi all!

I am new to using the IBM SPSS app. I am using the app to find the beta coefficients of three predictor variables in a binary logistic regression. I have followed online tutorials and videos but the ""variables in the equation"" section for block 1/step 1 does not seem to appear for me. It does appear in step 0 though. 

",IBM SPSS HELP,9nhdfn,new,1,1,1,0
"I'm working on a somewhat linear model project for a class and the data is very weakly clustered and the best R squared I could get (by applying a log transformation to the response variable and a square root to the explanatory variable) was .13. 

All the other linear model assumptions are met but the R squared is very weak and I don't know what other transformations I could use. I only have one explanatory variable.

Any suggestions?",How can I improve R-squared?,9ngywj,new,10,1,1,0
"Related to another topic I posted; [https://www.reddit.com/r/statistics/comments/9nckgc/how\_to\_analyze\_this\_data\_from\_a\_gameworld\_of/](https://www.reddit.com/r/statistics/comments/9nckgc/how_to_analyze_this_data_from_a_gameworld_of/)

&#x200B;

I ended up making a multiple regression model as suggested for each class. Problem is it has been years since I studied this in college and I'm pretty much illiterate at reading the outputs. [https://docs.google.com/spreadsheets/d/133akEBmut8vzilsSDepwa88ciHE\_oB0QswAmbOxmE38/edit?usp=sharing](https://docs.google.com/spreadsheets/d/133akEBmut8vzilsSDepwa88ciHE_oB0QswAmbOxmE38/edit?usp=sharing) \*edit: note winrate is the dependent variable\*

The outputs are to the right of the data and organized vertically. Assuming it isn't a harder than just scanning the figures for someone who understands the outputs does anything stand out as significant to you? Also a source where I can learn to read these outputs myself would also be grandly appreciated.",Reading Multiple Regression Outputs,9nfb3p,new,3,1,1,0
"I'm going through an basic book for stats, covering topics like t-tests, ANOVA, regression etc. I noticed that many texts, even ISL, don't go into the proofs for most of the basic equations they present.


Is there any text that does cover these topics? I'd love to be able to reason with these concepts from an algebraic perspective.",Good book for math theory/proofs behind basic stats?,9nf6o1,new,4,0,0,0
"I manage the data aspects of a hotline and I want to improve our data documentation going forward. I've done what I can to document the historic data, but I am looking for the most efficient and user-friendly way to document changes to our forms. Things like added/removed variables or response options, changes in branching logic ect. Do you have any suggestions or literature recommendations?",Documenting changes to a live form,9ne1fx,new,0,0,0,0
"In a large shipment of latex gloves, 1% of gloves are defective. Three gloves are randomly selected from this shipment. Calculate the probability that all three gloves are defective. (Note: gloves are defective or not defective independently of each other.) 

&#x200B;

Not sure where to begin with this because total amount of gloves is not given. Would appreciate an explanation.",Help with BioStats Question,9ndmq4,new,2,0,0,0
So I’m using SPSS and the category is “attendance.” 1 means they did attend and 2 means they did not. Would it be best for me to use mode to describe this data? I feel like mean and median wouldn’t be the best. Thanks!! ,For a nominal set of data which is the best measure of central tendency?,9ncvf3,new,9,0,0,0
"So I'm bringing my math and excel nerdiness to my video gaming hobby and was hoping there was someone who could help me try and find more ways to gleam info from the data. As a basic background info for the game there are 4 classes in world of warships, Carriers (CV), Battleships (BB), Cruisers (CA) and destroyers (DD). These classes are played across 10 tiers. A match is generally 12 vs 12 players.  I'm trying to specifically use the Win rate of each class to try and determine which classes have the greatest influence on winning the match. Using damage dealt is too simplistic and doesn't account for key functions in winning like spotting and base capping.

In the data I have found the weighted win rate by class overall and by tier. I also found the weighted Standard deviation to try and find that affect winrate more. \*I guess the real question here is does anyone know what other functions I can use like Std Deviation to find more meaningful metrics from the Data to determine how the classes are balanced against each other?\* I'm using data from an older period because now they have mirrored match making where each side has the same # of each class where as before they didn't except for CVs, each side could have +1/-1 of each class compared to the other so in theory all the data from current period classes should have net 50% winrates I think. 

Link to google doc of Data:

[https://docs.google.com/spreadsheets/d/1-IIOyB-aWcZN7R09-jTDBPTaGf1dILrU7CJ07-X5h\_M/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1-IIOyB-aWcZN7R09-jTDBPTaGf1dILrU7CJ07-X5h_M/edit?usp=sharing)

&#x200B;",How to Analyze this Data from a Game(World of Warships),9nckgc,new,19,21,21,0
,Linear Regression Produces Non-linear Predictions?,9ncbmq,new,3,3,3,0
Nice writeup that shows how to generate & interpret uncertainty estimates. [Hacker's Guide to Uncertainty Estimates](https://erikbern.com/2018/10/08/the-hackers-guide-to-uncertainty-estimates.html),Hacker's Guide to Uncertainty Estimates,9nbqfv,new,7,13,13,0
"I know it's easy because I'm taking an intro to stats course but I've fallen behind on this chapter and I don't really know where to go with this. I don't really want an answer, just some guidance would suffice! <3 

&#x200B;

An average of 10.89 crimes were committed in 2008. A standard deviation of 9.73. There are 514 observations. Assuming a normal distribution, how accurate of an estimate is the sample mean of the average number of crimes? Use the 95% confidence level. 

&#x200B;

Really appreciate it, thank you!",Can someone help me with this easy question?,9nawb1,new,3,0,0,0
"I know the correlation function in excel where it gives outcome between 1 and -1 to show how correlated variables are.   


Is it same as Pearson correlation? I just read about Pearson correlation, are they the same function?

&#x200B;

thnx",Excel correlation vs pearson correlation?,9n9772,new,4,3,3,0
"Hey!

&#x200B;

I have a question about the unbiasedness of the OLS estimator. I'm aware that to be BLUE we have to expect that the E(b)=beta, meaning the expected value of our estimator equals the estimated value of our regressor - correct? 

I do get that, but what I dont get is when we DONT expect this to be true? Under which circumstances do expect it to be biased, and how to we know/test if it is biased and how can it be unbiased even though we have autocorrelation and/or heteroskedasticity? 

&#x200B;

Thank you so much! ",OLS and unbiasedness,9n8g9k,new,7,6,6,0
"Hi all,

I wanted to do a mixed ANOVA with my data. However I have type count data and therefore I wanted to use the Generalized Linear Mixed Models. I am struggling to undestand how to translate my mixed Anova design into GLMMs. I understand the fixed and random factors and since my fixed factors would be type count data, the type of distribution I need would be poisson/negative binomial. But where is the between-subject factor going?   
To set an example, my subjects gambled on a fortune-wheel for 9 trials (within sbj factor) and I want to see whether the performance on these trials different across participant with high and low clinical symptoms (between sbj factor).   
Hope this is clear, I am not even sure it is possible to do.  
Thanks.

Silvia  
",Mixed design ANOVA using Generalized Linear Mixed Models,9n8ddg,new,1,2,2,0
r/https://www.datasciencecentral.com/profiles/blogs/changes-in-job-counts-by-education-grouping-1992-2017,"Changes in Job Counts by Education Grouping, 1992-2017",9n82rq,new,0,2,2,0
"I'm looking for PhD programs in statistics (or biostatistics) that have several faculty who are interested in applications to ecology. 

I really want to obtain a PhD in statistics/math, but involving research that can be applied to ecology (ie, creating new models of ecological phenomena). 

However, I am having trouble locating programs, so I figured I would reach out to a larger crowd. Do you know any schools that would fit this description?

Thanks all! <3
",PhD program in statistics: ecology research focus,9n59fa,new,31,18,18,0
"The NHTSA released some data about vehicle safety statistics which potentially shows that the Tesla Model 3 has the lowest probability of injury of the tested vehicles.

The data is available here: https://www.regulations.gov/document?D=NHTSA-2017-0037-0037

There has been a lot of debate on the validity of this claim. Lot's of people are saying that you cannot conclude that due to 'confidence intervals' and 'margin of error.'

Here is an article explaining the premise: https://jalopnik.com/how-tesla-made-the-model-3-so-safe-1829610576

My thought is that whatever the confidence intervals and margins of errors are in the risk curves, those are the same for all cars and therefore there is no reason to conclude that the relative ranking of the VSS in terms of probability of injury is not statistically valid.

Is it reasonable to make this conclusion or are there issues with the analysis?",NHTSA Vehicle Safety Statistics Question,9n4rgh,new,0,5,5,0
"I think this is probably the wrong place let me know if it is but...


This is for my IB Math IA project

Info: the two variables in my project are age and favorite musical genre. My question for the project is basically: ""Is favorite musical genre independent of age?"" And the data I've got is ages (from 11-20 to 61-70) and ~11 musical genres (I can show the survey if needed). 

For the project, we have to find a way to display the data and results, and use two lower and one upper math processes.

Ex. display/lower- frequency chart, histogram, cumulative frequency curve; upper- pearson's regression and chi squared test

I think the Chi Squared Test works for my data, so that's my upper math. But I can't find 'lower' ones that work

I suppose I could do like a frequency chart or graph or something for each separate genre but that's not really what is supposed to be done from what I can tell

Help?

",I need to find 'lower maths' that my data works for and I could use some help,9n3tqh,new,0,3,3,0
"I'm interested in finding out how the factors that contribute to a person's decision change when they are making a continuous vs. a dichotomous choice.

However, I'm concerned that these two types of decisions produce different types of data. Thus, would it be possible to make any conclusions about different decision processes taking place, without the concern that observed differences are just a product of linear vs. logistic regression?

To provide a bit more info, people would be making decisions about stimuli. I would be interested in how properties of the stimuli affect ratings differently than binary choices.","Is it possible to compare continuous vs. dichotomous decisions in a meaningful way, that doesn't come down to different kinds of analyses?",9n2erj,new,1,0,0,0
"Hello,

I'm doing project where I need to analyze a set of data and I'm not sure exactly how to do it (my stats class was not the best). Without going too in depth, basically a group of people completed a pre survey to test base knowledge on 5 subjects, completed a set of learning activities, and then took a post test to see if they learned anything. There were 10 questions on the pre and post test and 5 activities, so each activity has 2 questions associated with it. There was also an 11th question which was basically a Likert scale going from 1 to 3 (opinion based question, so I'm not really looking at that one with the other data which were all knowledge based) 

&#x200B;

Basically all the surveys were graded and we collected the number of correct answers on the pre test (for each activity and overall) and then did the same for he post survey. We turned those into percentages to get the % correct for each section/overall and the % of improvement from Pre to post but we need to figure out how to get a P-value to see if the results were significant. The only program we have to run the test is excel. I tried to figure it out online and it seems like some kind of T-test would be what we want but I don't really understand exactly how to set it up so excel can run the calculation. I also still have the raw data. So basically I have a pre and post test set up and need to see if the level of improvement was statistically significant. 

&#x200B;

I know it's a basic question but my statistics class was terrible and I honestly don't know how to do it. Any help would be greatly appreciated. ",How to determine if the % of improvement in scores in a Pre- and Post-test set up is statistically significant?,9n28ve,new,4,2,2,0
"My background is in PDE/stochastic PDE, but I’ve recently switched to incorporating statistical inference in my analysis of such equations, so apologies if I’m somewhat vague. In analysis, one typically seeks well-posed ness of these equations and some sort of regularity (i.e. how many “derivatives” can we expect the solution to have), so now I’m trying to understand what types of questions I can be asking to further explore. So far, I’ve been limited to “My SPDE has a parameter in it, how can I approximate it?” Indeed, it seems as though one does some tinkering and finds a formal expression for the MLE and tries to prove convergence in probability to the true parameter as well as trying to find a rate. I’ve thought of perhaps doing some other form of estimation, such as Bayesian estimation, but that’s about the ceiling of my knowledge of the subject. I’m trying to get my foot in the Statistics door for industry jobs and potentially academia in a statistics department vs Mathematics one. Thank you for suggestions!",What sorts of questions does one try to answer in statistical inference?,9n21g2,new,4,1,1,0
"Yi = 19.57+0.121Xi + -1.675Zi - 0.1ZiXi

I understand the relation between Xi and the constant but for some reason it says that the progression with the Zi is of 0.02 per added event. How is that so? I seem to have missed a basic concept here.",Beginner statistic formula,9mzv3o,new,2,0,0,0
"I just had a small insight about regression when there's missing values in the target value. I'm surprised I missed this in the regression courses I've attended and I'm curious to see what this technique is called and its pros/cons.

If we have a data matrix *x* with *n* observations and and *f* features, *y* with *p* target values then regression is to find *W* s.t 

    y ≈  xW 

Since x^T x is symmetric/square so often invertible we can estimate the least square solution as 

               x^Ty = x^TxW 
    (x^Tx)^{-1}x^Ty = (x^Tx)^{-1}x^TxW 
    (x^Tx)^{-1}x^Ty = W 

Where *(x^T x) ^-1* is *(f x f)* and *x^T y* is *(p x f)* 


Note that we can calculate (x^T x) ^-1 without y! So even if some y_i=NaN we can utilize the (non-missing) feature values for this. Experiment shows its a good way, here with n=50, f=19, p=1. (torch is just like numpy)

    import torch
    torch.manual_seed(1)
    x = torch.randn(50,20)
    x[:,0] = 1 # Bias term.
    y = torch.randn(50,1)

    # m = torch.randn(50)>0.5 # Target (m)issing at random
    m = y.sum(-1)>0.5         # Target (m)issing in a biased way

    # Use all rows (impossible in real life due to missingness!)
    W_opt = x.t().mm(x).inverse().mm(x.t()).mm(y).t()

    # Use all rows to calculate some stuff (proposed)
    W_use = x.t().mm(x).inverse().mm(x[~m].t()).mm(y[~m]).t()

    # Use only rows with no missing target (regular technique)
    W_drop = x[~m].t().mm(x[~m]).inverse().mm(x[~m].t()).mm(y[~m]).t()

    # Impute target value Y (also commonplace)
    y_hat = y*1 
    y_hat[m] = y[~m].mean()
    W_impute = x.t().mm(x).inverse().mm(x.t()).mm(y_hat).t()

    # Calculate sum squared error
    print('W_opt \t',(y-W_opt.mm(x.t()).t()).pow(2).sum())
    print('W_use \t',(y-W_use.mm(x.t()).t()).pow(2).sum())
    print('W_drop \t',(y-W_drop.mm(x.t()).t()).pow(2).sum())
    print('W_impute',(y-W_impute.mm(x.t()).t()).pow(2).sum())

With target missing whenever its above a treshold (above):

    >> W_opt     tensor(25.7269)
    >> W_use     tensor(33.9462) #<-proposed
    >> W_drop    tensor(49.8913)
    >> W_impute  tensor(41.5520)

With target missing at random:

    >> W_opt     tensor(25.7269)
    >> W_use     tensor(28.2667) #<-proposed
    >> W_drop    tensor(58.3970)
    >> W_impute  tensor(28.4812)

Can anyone put this into an academic context? What is this called and why haven't I heard about it before?

**TL:DR** Using all feature data for some part of the regression-calculation works for NaN in target value 

**Edit:** Not surprised anymore lol, probably not very good idea see below",Thoughts on NaN in y for regression,9mztk8,new,8,8,8,0
"I'm getting my first paper published! I have a forthcoming paper I helped coauthor, to appear in the *Journal of Business and Economic Statistics*, about change point analysis.

I organized the project as an R package and made my code available at my [university home page](http://www.math.utah.edu/~cmiller/software/CPAT/) (see the [archive](http://www.math.utah.edu/~cmiller/software/CPAT/archive.html) page for a tar ball containing all data and code to completely reproduce the project). On that page you will see not only a link to the R package but also a brief summary of change point testing, a statistical procedure intended to determine whether a series of data is stationary or not.

I'll likely submit the package, **CPAT**, to CRAN at some point in the near future, and will write some blog posts talking more about change point analysis, but in the meantime, here is [this Wikipedia article](https://en.wikipedia.org/wiki/Structural_break), and [this one too](https://en.wikipedia.org/wiki/Change_detection).","Change point analysis, with accompanying R package",9mxahl,new,0,7,7,0
Can the results of a study of 200 randomly sampled men and women be generalized to the population at large?,Generalization,9mw9j9,new,4,0,0,0
"I have a data set with two variables derived from the same population: x and y. I have the means for each of these variables, separately.

&#x200B;

In the literature, normally these variables are expressed as a ratio (x/y) - that is, they are normally calculated for each patient/observation and then expressed as a mean of ratios. Is there any way to derive a similar ratio using the separate sample means?

&#x200B;

As an example, consider BMI. In my scenario, using this analogy, I have mean weight and height (and SDs), and I want to express 'BMI', as that's normally used in the literature. Is an estimate possible here?

&#x200B;

I can see, algebraically, how these are fundamentally different expressions, but I'm unsure whether it is possible to make a reasonable estimate at the ratio of these variables with only sample-level figures? Running a dummy data set suggests that a basic x/y ratio of the means is close but not quite right. Can anyone help a luddite like me here?",Estimating ratios from sample means?,9mw3xu,new,2,2,2,0
"I constantly hear those two terms and I know there are definitely some similar aspects in them. But what are the real differences between SL and ML. It seems to me that CS people do ML and Statisticians do SL. Also, do those two fields use different programming languages? For example, Python for ML and R for SL. ",Statistical learning vs machine learning,9mvzwm,new,60,28,28,0
" Github Repo : [https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm) 

&#x200B;

Papers :

1- Wei, J.-M., Yuan, X.-Y., Hu, Q.-H., Wang, S.-Q.: A novel measure for evaluating classifiers. Expert Systems with Applications, Vol 37, 3799–3809 (2010).

2- Delgado R., Núñez-González J.D. (2019) Enhancing Confusion Entropy as Measure for Evaluating Classifiers. In: Graña M. et al. (eds) International Joint Conference SOCO’18-CISIS’18-ICEUTE’18. SOCO’18-CISIS’18-ICEUTE’18 2018. Advances in Intelligent Systems and Computing, vol 771. Springer, Cham","PyCM 1.3, An implementation of Confusion Entropy(CEN)/Modified Confusion Entropy(MCEN)",9mu69u,new,0,11,11,0
"As an example, if player A shoots 50% on 100 shots, how could I compare that to player B who shoots 45% on 2000 shots?

What are some ways of weighting proportions by sample sizes to allow comparison?

Edit: Instead of an actual statistical test, I was curious if there is a rule of thumb, like “multiply the proportion by the sample size”. ",What are some ways of comparing two proportions while adjusting for sample size?,9mtrml,new,6,2,2,0
"I recently came across DOE and understand its purpose in helping find experimental inputs that are beneficial for experimental outputs.

I've seen examples of DOE experiments were the user has 1 constant input and 2 changing inputs. The changing conditions are split into normal levels, -10% of normal level and +10% of normal level. The calculation to find out the optimal level and which input is providing the greater benefit is relatively simple due to the small number of combinations. 

I would like to combine DOE into the work that I currently carry out. My work involves making up formulations that contain up to 10 different reagents. The object is to find a formulation that will provide the longest stability within a temperature range. 3/10 of the reagents would remain at the same concentration, the other 7 reagents can each be potentially tested at 10 different concentrations. 

As you can see this is going to throw up a lot of combinations. Is this the type of experiment were DOE is extremely beneficial? I normally use OFAT but in hind sight it looks like DOE would be the better option, what do you think?","DOE newbie, would like some advice",9msv96,new,2,0,0,0
"Say you wish to compare two diagnostic tests (in my case they are two surveys, whose accuracies are measured by a ""gold standard"" as in diagnosis by a physician). How can you formally compare the two tests' AUCs? (I believe sensitivities/specificities would be compared with McNemar's test)? I will eventually need to calculate how many samples are needed in order to do this too so any tips there are great.",What tests can be used to compare diagnostic tests?,9mslz1,new,4,0,0,0
"Hello data experts. I’m trying to smooth some data and I’m struggling with a certain issue, hoping you all can help. 

I’m using a pressure transducer to measure the pressure in a tank being carried by a vehicle. Think of a water truck. The liquid sloshing around while driving and taking corners creates a lot of bouncing data, but a moving average calms it down fairly well and results in a reliable data set for a gauge display. However, when the vehicle is traveling in a straight line on a highway or somewhere with constant speed, the moving average doesn’t work. Since the pressure is near constant (and higher due to sensor placement and inertia) while driving at that constant rate, the moving average is inflated incorrectly. The tank will have around half pressure while driving around town, then I’ll get on the highway and it will climb up to reading full pressure, then I’ll get off the highway and it will go down to reading less than half pressure. So the full pressure reading is obviously incorrect. 

I’m open to hardware suggestions as well, but I’m hoping I can just make a coding change with a different formula to counteract this artificial inflation. Any thoughts would be fantastic. Thanks in advance. 

Note: I’ve tried moving average, weighted moving average, basic/double/triple exponential smoothing. It’s been a while since I’ve taken stats courses, but I do understand a bit. ",Data Smoothing Advice Wanted,9msfnx,new,22,5,5,0
"Let’s say a research paper says r = .22, what does that mean exactly

&#x200B;

Okay I believe the correlation between income and IQ is something like .4 (I’m not trying to make a political post regarding the validity of IQ as a measure either... just using it as an example regardless of data)

&#x200B;

So doe that mean you take .4 and square it? so the r-squared is .16... so would that mean IQ is responsible for 16% of income? and the variance is 16%?

&#x200B;","I don’t fully understand variance and coefficients, ELI5?",9mr9ci,new,19,0,0,0
"People often produce histograms with error bars on each bin, which I assume come from treating the bin frequency as a Poisson random variable and assigning sqrt(bin count) as the error in each direction. How valid is this as an approach? I haven't been able to justify it personally.",Should you put error bars on histogram bins?,9moly9,new,20,13,13,0
"In Annals of Internal Medicine it's been recently published an article titled ""Researcher Requests for Inappropriate Analysis and Reporting: A U.S. Survey of Consulting Biostatisticians"".  Basically, the authors surveyed 390 biostatisticians about ""the frequency and perceived severity of specific requests for inappropriate"" statistical practices. The most frequent request rated as ""most severe"" by at least 20% of the biostatisticians was ""removing or altering some data records to better support the research hypothesis"". 
Any thoughts or experiences? 
Article: http://annals.org/aim/article-abstract/2706170/researcher-requests-inappropriate-analysis-reporting-u-s-survey-consulting-biostatisticians",Researcher Requests for Inappropriate Analysis and Reporting.,9mlpts,new,4,6,6,0
"Hello,

&#x200B;

I am writing a research study on the reliability of a foot scanning app. I want to evaluate the ICC (intraclass correlation coefficient, absolute agreement) for a couple of datasets I have. In one set, I only measured each person once (about 45 people). In another, I measured 3 people 15 times. I have extracted 10 measurements from each person (5 different foot measurements of each foot). I also have some [ICC calculation code](https://www.mathworks.com/matlabcentral/fileexchange/22099-intraclass-correlation-coefficient-icc) to help me calculate these things.   
The code requires that I know what the ""objects of measurement"" are (rows) and the ""judge or measurement"" (columns). How should I arrange my data to calculate an ICC value for each foot measurement type?",Evaluating ICC for Reliability Study,9mjs01,new,0,1,1,0
"I read that something like 100 years ago lifetime cancer rates in the USA were roughly 3% and now today they are at somewhere just over 50%. Now obviously this isn't an apples to apples comparison because of different life expectancies, different ability to diagnose cancer, etc. But what kind of statistical modeling can be done to try to state that after correcting for various factors cancer rates have indeed increased dramatically over the last 100 years. Meaning a model that would say supposing hypothetically those people from 100 years ago were around today what their cancer rates would be. I don't know exactly how you would define an apples to apples comparison given different life expectancies, different diets, different medical technology, etc. But what statistical subfield would this be under?",Comparing cancer rates today with 100 years ago,9mjq72,new,5,2,2,0
"I'm working through the textbook **The Elements of Statistical Learning** and I ran into a part that I'm confused about and was hoping I could get someone who knows more than me to help clarify what it means. I've included a snippet below, the part that confuses me is in **bold.**

Particularly, I'd like clarification on

1. What a hyperplane would consist of in terms of X and Y
2. what is meant by ""the hyperplane includes the origin and is a subspace"" (especially what a subspace means in this context)
3. and what an ""affine set"" is

>Given a vector of inputs XT = (X1, X2, . . . , Xp), we predict the output Y via the model  
>  
>Y = β^(0) \+ sum(j=1 approaching p of: ( X^(j \*) β^(j) ) )  
>  
>The term β^(0) is the intercept, also known as the bias in machine learning. Often it is convenient to include the constant variable 1 in X, include β^(0) in the vector of coefficients β, and then write the linear model in vector form as an inner product  
>  
>Y = X^(T \*) β  
>  
>where X^(T) denotes vector or matrix transpose (X being a column vector). Here we are modeling a single output, so Y is a scalar; in general Y can be a K–vector, in which case β would be a p \* K matrix of coefficients. **In the (p + 1)-dimensional input–output space, (X, Y) represents a hyperplane. If the constant is included in X, then the hyperplane includes the origin and is a subspace; if not, it is an affine set cutting the Y - axis at the point (0, β****^(0)****).**

&#x200B;",Question/Clarification on a section of Chapter 2 of Elements of Statistical Learning.,9miwrv,new,2,0,0,0
"First post on here. Our stats class was getting a little dry when our prof goes to the slide on conditional probability. He shows us this hilarious [meme](https://ibb.co/bSZ0Dp) of an explosion that says ""Michael Bay was here."" Michael Bay's movies are famous for having explosions in them (Transformers, etc), so the joke/point about conditional probability is as follows: 

""Imagine I ask you about some movie and I ask you to tell me what is the probability that it has explosions in it:

A= Movie has an explosion in it. You would probably take a rough guess at how many movies have explosions in them and how many do not, and then divide the number of movies with explosions by the total number of all movies.

Now, if I were to tell you that this movie was directed by Michael Bay, B = Movie directed by Michael Bay, then you would almost surely update your answer to a probability of 1.""

It was both a good laugh and a nice way to teach the class about conditional probability.",Our professor used Michael Bay to teach us conditional probability.,9miq02,new,12,72,72,0
"I’m having trouble with a problem about expectation and variance. The problem is:

The probability distribution for x, a random variable, is: f(x)= .08x,0 <(or = to) x < (or = to) 5. 
Find E[x]. 
It also asks for variance but I figured I need to tackle expectation first. If anyone can help it would be GREATLY appreciated. ",Question about expectation,9mhzyc,new,6,1,1,0
"Hi, r/statistics - I'm needing some values to approximate population density by zip code in the US. The census website has population within in each zip code- generally would this be an okay approximation for population density? Or is it frequent that is wouldn't work? Like are there are some zip codes which have high population but are big in geographical area, so using high population as approximation of population density would be misleading? ",Is population an okay approximate for population density in the US?,9mhyml,new,7,1,1,0
"I'm working on this problem set dealing with hypothesis testing. I was given a sample set of 100 numbers with no other context and asked which population distribution the samples come from. I thought I might run a one-sample test of hypothesis about a population mean, but I don't have a hypothesized population mean, so I don't think that will work. Then, I thought I might do a chi-squared goodness-of-fit test to determine if it follows a normal distribution, but again I don't have estimated values. I've built a histogram with the values and it appears to be normally distributed, but the second part of the question asks how sure I am about my answer. I am assuming a p-value would be the way to answer the ""how sure"" part, but maybe not. Does anyone have advice about how to approach this problem?

Edit:
Thanks for everyone's input. For those of you who are interested in what I did:
I developed a frequency distribution for the sample and it appeared normally distributed. Then using the normal distribution, I calculated the expected distribution based on the sample mean and standard deviation. I performed a chi squared goodness of fit test to see if the sample fit the expected. This told me that the sample is likely from a normally distributed population.",Simple(?) Hypothesis Testing Question,9mgt46,new,14,5,5,0
I found that Undergraduate level textbook and graduated level textbook follow similar pattern when it comes to statistics and probability theory. Why is that? Is there any underlying logic of this?,What is the pattern of probability and statistics theory textbook?,9mgm84,new,1,3,3,0
"I have a feeling I would need to do a Monte Carlo simulation to generate multiple predictions. While I only have one measured data point, it is the 20-year average of the product of two variables, so I think it would be possible to do with both the predicted and measured data. ",Is it possible to test difference (H0: difference = 0) between one predicted and one measured data point (with error estimate)?,9mgbt6,new,0,1,1,0
"[https://www.datacamp.com/community/blog/election-forecasting-polling](https://www.datacamp.com/community/blog/election-forecasting-polling)

&#x200B;

No idea why I can't just post a link, but I found the interview very interesting.",Andrew Gelman discusses election forecasting and polling.,9mgb62,new,6,7,7,0
"I have some survey data that tracks answers to a yes or no question, and one of the predictors I'm interested in is age.

I'm interested in seeing how the probability of a 'yes' response changes as people get older. The problem is that I don't think the usual logistic regression will work, because I expect the data will look  sort of parabolic; ie, I'm expecting that the probability of 'yes' will be high for young people, low for teens and young adults, then pick back up as people get older.

I've considered using LOESS smoothing to capture the unusual shape, but I worry about whether or not that's a valid approach. LOESS is effectively a series of linear regressions, and what I'm modeling doesn't seem to adhere to the usual regression assumptions (normally distributed errors with mean = 0 and constant variance, etc).

How much do we need those assumptions when using LOESS smoothing? Is it a valid approach to my problem, or do I need to try something completely different?

Thanks in advance for any help!

Edit: Spelling",Using LOESS when the outcome is a probability?,9mfwaf,new,7,5,5,0
"I'm pleased to publicly announce MCHT, an R package for bootstrap and Monte Carlo hypothesis testing. [Here is the blog post announcing the package](https://ntguardian.wordpress.com/2018/10/08/announcing-mcht-package-bootstrap-monte-carlo-hypothesis-testing/), and [here is a link to the GitHub page](https://github.com/ntguardian/MCHT).

In the blog post (and in the read-me of the actual package) I explain how Monte Carlo and bootstrap hypothesis testing work, and I give demonstrations of basic usage of the package.

This is the first in a weekly series of blog posts explaining the package. Future topics include making the primary objects of the package (`MCHTest`-class objects) self-contained and immune from side effects; maximized Monte Carlo testing; bootstrap testing; testing with multi-sample or multivariate data; time series inference; and a final example that motivated my initial creation of the package.

I'm excited about the package and hope others will find it useful. I'd also like feedback if anyone thinks that the package could be worth a paper in [*J. Stat. Soft.*](https://www.jstatsoft.org/index) or the [*R Journal*](https://journal.r-project.org/) or if you think it deserves any other documentation. And please report any issues you encounter on GitHub; I won't know until someone tells me.",Announcing MCHT: An R Package for Bootstrap and Monte Carlo Hypothesis Testing,9mfu59,new,5,33,33,0
"I'm trying to run Logisitic regression in Python and need the pvalues for feature selection. Here's what i've been able to come across from some posts in stack overflow.

I have two options:

\- sklearn

\> lots of parameters to tune. (C, penalty, etc.)

\> no p value out of the box

\> alternatives to fetch p values

\->using sklearn.feature\_selection.f\_regression

\-> using sklearn.feature\_selection.chi2

\- statsmodels

\> pvalues out of the box

\> not a lot of parameters to tune

&#x200B;

I'd prefer using sklearn just for the slightly better results. My question is which approach to fetch pvalues is used by statsmodels and which would you recommend?",pvalues in Logistic regression - python,9mf8ti,new,0,1,1,0
"If a population that is entirely X changes from X to Y at a rate of 20% every year, when would the population be entirely Y?",Very simple question for anyone who knows about statistics...,9mezzq,new,13,0,0,0
"Any papers that are a good example of using it?

&#x200B;

I would like an example so I can follow it and hopefully have a better understanding of how it is applied.",Concrete examples of applications of do-calculus in research (medical or other),9mdtd3,new,4,21,21,0
"
What are the 5 or 6 most common analysis done on a dataset besides checking that the data is normal? 

I understand that there is a whole body of statistical analysis tools for a dataset but I'm most interested in the basic ones. Thank you.

[cross post with r/askengineers]",Must know statistical analysis at a Reliability/Quality lab.,9mcrl8,new,2,0,0,0
"Ellen scored 19 for the Uptown League whose mean score was 16.5 with a standard deviation of 1.2. Barbara scored 32 for the Downtown League whose mean score was 30 with a standard deviation of 2. Which player had the most outstanding score with respect to the league they represented?

&#x200B;

Just a homework question I'm stuck on, it's 2am and I'm tired. Some help would be awesome. I suck at math.",Basic Mean and Standard Deviation Comparison Problem,9mcbx8,new,5,0,0,0
"Anyone have hilarious statistics or stories where the use of averages ( or means) were used?  Here are few I have below:

&#x200B;

1. A***verage*** age of people who wear diapers in U.S -- 23 years old
2. The ***average*** human-being has one testicle and one breast
3. The US county with highest ***average*** income is probably the one with Warren Buffet

any others?",Hilarious misleading statistics by use of Averages,9mbr6h,new,4,0,0,0
"I had my friends and I rank 25 NBA players. Let’s say one guy didn’t list player X, but he’s relatively high in the others’ lists. How would I average that (is there a formula)? And would it be different if player X is near the bottom 25 for others’ lists? Thanks!",Question about averaging ranks,9mbklf,new,4,9,9,0
"You have an 8% chance of being late to work each day. Assume each day is independent from the next. 

1a What is the probability you are late twice two days in a row. 

1b what is the probability you are late once a week(5 work days in a week) 

If anyone can please help me out that would be much appreciated, I've been stuck on it for a little while. Thanks ",Help stuck on question for homework,9mb3ei,new,4,0,0,0
"Hello Reddit,

&#x200B;

I have 4 datasets in which, each has different sample size; 103, 100, 70, 71. Is it allowed to compare these sets with one another? like comparing their mean, std dev, etc?

&#x200B;

Thanks!",needed sample size to compare two sets,9mayk1,new,7,1,1,0
"So, I found out that two of my factors have a VIF value of 6.08. This means that they're highly correlated, correct? And I should only use one of them in my analyses? How do I decide which one to drop? Is it just based off of my background knowledge of the system/study?

Also, VIF is independent of the response variable, right? So I should drop one of these factors from any analyses that I do where I was initially going to include both, correct?

Finally, one of the two factors that I've been talking about above is a categorical variable. That shouldn't matter, right? Just curious since when I typically think of correlation I think of two continuous variables. ",Multicollinearity and VIF,9ma82z,new,15,14,14,0
"So, I'm running a GLM where I'm using AICC to determine which factors to include. If anything is within 2 AICC of the ""best model"", I'm going to include it since they are technically equally as good. What does it mean if AICC is telling me to include a factor that's not significant? I can't really say anything about the factor since it's not significant so I'm curious how I would interpret this when discussing the model.",What does it mean if you include a factor in a model (due to AICC) but it's not a significant factor?,9m9npc,new,1,2,2,0
"I'm an actuary who is interested in machine learning and its application in insurance. Currently I'm willing to study a master's degree in Statistics to further my skills in this field and make me more competitible in the job market. Nowadays there a lot of Statistics departments offering ML degrees, courses and research. 

Although I like to be an actuary, I know it's a narrow field since it's only related to insurance and I'd like to have more opportunities in the financial/banking sector. I was thinking about a master's in Statistics and I'd like some recommendations of good programs in Europe. I found a lot of programs to be too theoretical and, given my actuarial science background, I'd rather prefer something more applied. Does anyone know good programs in this sense?

EDIT: To make things clearer, when I say ""too theoretical"" it's mostly related to pute math/stats. I'm looking for something that can be more applied. For example, I have found some people doing research in how to use machine learning to better predict auto insurance premiums, others are using to better assess healthcare costs, to better model financial variables and trends, to better predict the reintrance of a pattient at a hospital, etc. This is the kind of thing I'm looking for. 

Originally posted in r/MachineLearning but I think that posting it here and hear from you is also a good idea",Advice for a master's degree in Statistics in Europe,9m7z3t,new,1,5,5,0
"I’m working with one doctor now as a test run- no compensation, just wanted to get on her research as an author. Have also worked as an employee conducting the actual research on 3 other projects. Everything went very smoothly, I turn around work within 24 hours of assignment. Looking to do this part time for some pay while I apply to schools - how do I get started/connect with docs who need help with their research? Any tips advice or things I should look for or advertise for myself would be greatly helpful! Thx ",Im proficient in biostatistics - how can I connect with docs who need help with their clinical/bench research?,9m7spt,new,24,9,9,0
"Is statistics a branch of probability or is probability a branch of statistics? If the former, then it is possible for one to only like probability, right? ",Is it possible for one to find Statistics to be rather dull/boring but probability to be very fascinating? or are the two so interlinked that one cannot like one but be apathetic toward the other?,9m6aoe,new,10,1,1,0
,What jobs do people who hold B.S degrees in Statistics typically get?,9m64zy,new,57,16,16,0
"Hi all

&#x200B;

I'm well aware of post-hoc testing to look for differences between specific groups when there's more than two levels of the independent variable. What I'm wondering is if such a thing exists when you just have two groups of the IV but 5 levels of the DV (a likert agreement scale). 

&#x200B;

It's a repeated measures pre-post test, so I used the Wilcoxon signed rank test, and I have significant differences between the two groups. But I want to see if Strongly Agree differs significantly from Agree, etc. I.e. did the distribution of responses change because more Agree than are Neutral, or that more Disagree than Strongly Disagree, or all of the above, or whatever.

&#x200B;

I thought of just re-doing the test between each of the comparisons of levels and then manually correcting the p-values with Bonferroni, but is that valid? Or better yet, is there an actual method to do it automatically (preferably in SPSS)? Google has not been helpful.

&#x200B;

Thanks",Post-hoc test for differences between multiple levels of ordinal DV?,9m5kns,new,0,19,19,0
"A couple weeks ago, I put together a quick toy analysis to test if a set of dice were balanced.  I just realized that I should have posted it here.

Analysis: [Thoughts on u/VaraNiN’s dice: A small project to test dice for balance based on rolls and expected values](https://revgizmo.github.io/testing_dice_balance/)

Original post: [Thoughts on u/VaraNiN’s dice / testing dice for balance based on rolls and expected values](https://www.reddit.com/r/DnD/comments/9hlmwk/thoughts_on_uvaranins_dice_testing_dice_for/)

I'm interested in what I got wrong, and what I could have done better. Feedback welcome.",Feedback requested on A small project to test dice for balance based on rolls and expected values,9lzs5a,new,4,14,14,0
"Hi, I’m in a business class that tries to solve case studies. Part of this case is using regression to study the relationship between some factors (cost of a good, purchases of a good) to the week. We’re using Excel but I’m completely inexperienced and new to all of this. 

I was able to use Excel to find the regression data, and we were told to look at the t stat because we have less than 31 observations (25 weeks). But we were also told to calculate the critical value to understand the significance of it. 

My main question is if its possible to calculate critical value for this scenario and what values we would plug in or if looking at a table will get us the same answer? 

My second question is what we can infer based on the t stat and critical value comparison? If only the x variable or intercept check out but the other doesn’t, is there no relationship? 

Thanks for any help..I don’t want the assignment done for me so I can’t really place the work here, but a beginners guide to understanding critical value would be appreciated. 

Edit: for clarification, we were told to use regression to understand to look at the relationship between the cost of a good (dependent) and week (the independent). Focusing on r square, the coefficients, and t stat. Then again for the purchases of the good every week and the week. ",Regression Analysis and finding Critical Value,9lxmmp,new,3,1,1,0
" We've been running the attached sensory analysis test at work for the past few years and using the attached ""Chart 1"" to determine the p-value. I would like to walk through the math that created the chart, but I'm not sure where to start.   
For example, I tried to work out the case that 6 out of 10 panelists correctly identify both sets. According to ""Chart 1"", the p-value would be 0.0197, or highly significant. This is as far as I was able to get:  
Expected value = 0.25 (Probability of correctly guessing both sets)  
Observed value = 0.60 (actual value in this example)  
chi-squared value = 0.49  
Significance level = 5% (assumed)  


Plugging this into a chi square distribution chart, I'm getting a p-value between .1 and .9, which is completely different from ""Chart 1"". Where am I going wrong? 

[https://docdro.id/mXmMBYq](https://docdro.id/mXmMBYq)

[https://docdro.id/njHyDXu](https://docdro.id/njHyDXu)",p-value for panelists correctly identifying both sets,9lx2fs,new,8,2,2,0
"Let's say that we have a fully crossed factorial ANOVA with two treatment factors (TF). TF A has 3 levels, while TF B has 4 levels. Therefore, the treatment factor table will have 12 cells as all of TF B ""occur"" in each of TF A. Here, we have a crossed design. 

For a nested design of the same experiment, you would have 12 cells in your treatment factor table as well. *But* the levels of B do not all ""occur"" in *each and every* level of A, hence the ""nested"" classification. 

Here's the issue in my mind... for the nested example, it seems that since there are still 12 cells of treatment factor combinations, then TF B no longer has 4 levels occurring in each of TF A. It actually has 12 levels, each of which is occurring in combination within and only within one of the 3 TF A cells. 

So a question such as ""how many levels does B have"" could be answered with ""12"". Discuss for the benefit of r/statistics and yours truly!

Thanks!! ",Clarification on Two-way Crossed vs. Nested ANOVA designs.,9lwd9h,new,0,0,0,0
"I've been looking into LDA to try and get a handle on something I've been told about it, namely that the probabilities are compromised by not having multivariate Normal data but LDA scores themselves (such as would be obtained from the R function) are fine. To this end, I have been looking at ESLII's stuff on LDA (pp. 108 - 117).

As far as I can tell, ESLII introduces LDA very specifically in the context of classification and then go on to present three kinds of decision boundary... a probability, OLS and empirically defined ones (pg. 110). The last of these I don't think would require multivariate Normality, the second works only with 2 classes and the first is very much reliant on multivariate normality. Page 111 also makes me think that ESLII could be suggesting that LDA might deliberately be used to because it's low variance, that we can justify accepting its biasedness in the same way that we do in other cases where we talk about the bias/variance tradeoff (e.g. boosting and bagging).

I'm not sure I have completely got the point in the above stuff, but  I think what I was looking for comes later in section 4.3.3 where they talk about Fisher. Looking at this, I think it's saying that if we divorce LDA from its usual use (i.e. classification) it's really just descriptive... no more bound by multivariate normality than, say, plotting a dataset is... but just so happens to be the same as what we'd get by exploring the classification problem's best case whilst assuming multivariate normality and equal covariances? Page 117 I think I just don't understand: couldn't we just use the co-ordinates to derive entirely empirical (by which I mean to understand cross validation like) classification rules?

(If you remember my previous post here you might wonder why I don't ask my lecturer but it turned out the course is **extremely** applied so this isn't really relevant to the course, and also it's the weekend here and she's completely swamped right now like most of the lecturers.)",LDA and Normality,9lvtdg,new,6,7,7,0
"Hey guys, first post on this sub. 


My internet speed is absolutely awful, and I'm trying to organize data neatly to bring this up to my ISP/governments. If I have data for average download speeds in the US with a mean and variance, would it be possible to estimate the variance of upload speed given the mean? I can't find good data on upload speed besides a mean. Is this possible? And why does it work (for my personal enlightenment)?",Question: Estimating variance using another data set,9lvnfd,new,2,2,2,0
"Hi guys! I need help with interpreting my correlation coefficient and p value for a science report. 

So I did a correlation on fetal weight and litter size in rats. I produced an r value of -0.42 and a p value of 0.407. I concluded from the r value that there is a negative relationship between the factors and as fetal weight increases, litter size decreases but then with the p value I fail to reject the null hypothesis, so conclude that there is no relationship between fetal weight and litter size... 

So what would be my actual conclusion, I am so lost.

Thank you in advance!!!",Question on correlation coefficient and p value,9lu3l0,new,7,2,2,0
"To read books and articles dealing with statistical simulation, what areas of statistics or how advanced do I need to be  to understand it? I'm thinking a more intermediate (beyond open intro statistics) statistical inference book or course, but the below article looks like it might be graduate level material. 

As an example of what I have in mind: [https://floridadep.gov/waste/district-business-support/documents/cmf-presentation-simulation-study-extrapolation](https://floridadep.gov/waste/district-business-support/documents/cmf-presentation-simulation-study-extrapolation)

[https://floridadep.gov/waste/district-business-support/documents/extrapolation-uncertainty-exposure-assessment-draft](https://floridadep.gov/waste/district-business-support/documents/extrapolation-uncertainty-exposure-assessment-draft)

thanks, ",What areas of statistics should be well understood before learning statistical simulation?,9lstz0,new,1,4,4,0
"As a player of collectible and trading card games, I had a question that I didnt find an easy answer to with a search, but thought was interesting enough to post here:

Given a deck of size X that contains Y copies of card A, what are the chances of drawing at least one copy of card A in Z draws? 

Would appreciate some input on this! ",Problem from a card game player,9lsekq,new,2,1,1,0
"Much of frequentist optimality theory depends on the asymptotic efficiency of maximum likelihood estimators. But in many real world problems, the likelihood surface is nonconvex, and analytic global optima are not available.

Further, it is not evident that the likelihood becomes any better behaved as the sample size increases. So practically speaking, in many cases, what is referred to as a maximum likelihood estimator is really a local maximum obtained through a numerical convex optimization method, and often, appeals are made to asymptotic normality of the MLE when discussing it's properties.

However, it's not obvious to me that these properties of global MLEs apply to maximizers to potentially increasingly ill behaved likelihood functions.

So to be brief, the subject of discussion for this thread is ""to what extent do asymptotic properties of maximum likelihood methods extend to local maximizers?""

Or put even more briefly, ""hell ye brother, cheers from iraq"".","Hell ye brother, cheers from iraq",9lsd6q,new,3,4,4,0
"For example, Monty Hall problem doesn't come naturally to even a person who studies statistics until you read explanation.  But  I would say it is important in understanding the concept of probability.

&#x200B;

What are some similar problems that you feel is very important and/or seems illogical and unnatural?","What are some ""idea"", ""paradox"", ""problem"" that you wish more people become familiar with?",9lrcih,new,29,12,12,0
"I am teaching a course on simulation methods for economists next semester, and I am scratching my head trying to come up with a short, simple expectation which is clearly very hard to solve analytically but is very simple to solve using simulations.  


The problem is that many of the students, at the start of the course will only have had a few courses on statistics and mathematics and so wont know much beyond the standard distributions (Gaussian, t, chi2, F etc.). ","Looking for a problem: An easy to understand expectation, very hard/impossible to do analytically but simple to simulate",9lqlms,new,5,2,2,0
"https://imgur.com/a/T2sQ5pX

is it? and why?
what about this ?

https://imgur.com/a/R081vdX",Probability,9lq9b1,new,0,0,0,0
"I believe this term means that a given analysis doesn't assume the data follows a specific distribution. But I have trouble intuitively understanding what it means when it comes up. 

For instance, I've just read that the LOESS function is non-parametric. What does that mean in practice?","Trouble with really grasping what ""nonparametric"" means.",9lq6ge,new,45,39,39,0
"My friend just created a PCA and regression tutorial video. 
Just want to share it in reddit statistic

https://www.youtube.com/watch?v=WMd-t9IGSug
",PCA and regression,9lq22r,new,0,0,0,0
"I have some data about the performance of an advertising campaign, where performance is revenue.

The data is broken down by hour of day.

We have historic data for the last two weeks and want to try to look forward and predict for future performance will be using this data.

The revenue is influenced by:

hour of day (strongest)

day of week (medium)

day of month (strong)

I am not really sure where to start with this. I have the data in an accessible format.

I've been looking at whether hour of day should be a categorical variable or whether it is a circular variable?

As background, we find that the biggest hour-on-hour change in performance is between last hour of today vs. first hour of tomorrow: 23:00->00:00

Am I barking up the wrong tree here? Gratetful for direction.",Hour of day circular variable categorical variable,9lptt6,new,12,3,3,0
"(Refer to [this image](https://i.imgur.com/iFtIgcz.png) if my explanation isn't clear)

Scenario: I have a bunch of real-valued observations generated from different random processes A, B, C, ... etc. The observations are i.i.d. within each process. Each process is described by a different continuous probability distribution function. We don't make any parametric assumptions about the forms of the distributions. We keep track of which observations belong to which process. In all cases, the observations are constrained to lie in some known fixed interval.

Next, suppose I get a new group of observations, but I don't know which process in my collection was the one that generated it. **I want to figure out which process is the most likely to have generated it.** (We ignore the possibility that the new data was generated by a process that isn't in the pre-existing labelled collection).

The way to go seems like to calculate some kind of ""distance"" between the new sample and the known samples in turn, and pick the one with the lowest distance score. However there's dozens of ways to compute distances between distributions and between samples. For instance there's:

- The supnorm distance between the empirical CDFs, as used in the [two-sample Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)  
- Integrated squared-difference between the empirical CDFs, as used in the [two-sample Cramer-von Mises test](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion)  
- [Earth-movers distance](https://en.wikipedia.org/wiki/Earth_mover%27s_distance) between the empirical histograms  
- I could also estimate the pdfs via KDE and then use something like the Kullback-Leibler divergence, or the mutual information, etc.

There's probably a bunch of other ways of nonparametrically comparing samples that I haven't heard of.

Many of the approaches seem geared towards hypothesis testing, but that's not really my goal. I really just want to build a classifier, so I need to know which process is most likely to have generated the new data. It would also be nice to be able to compute confusion matrices.

Also, if this seems like completely the wrong way to go about my problem, I'd like to know. I wasn't sure what this kind of classification problem is called, since I'm trying to classify an entire *set* of observations rather than each observation separately. This meant I didn't know what search terms to use, so for all I know this is a solved problem already.",What is a good nonparametric two-sample distance measure for my classification problem?,9lp4bn,new,7,6,6,0
"Let's say I have a list of 100 phone numbers. I call them all. Nobody picks up for 70. I get someone on the line for 30. Of those, 10 are wrong numbers. What can I conclude about the distribution of wrong numbers in the full list of 100 numbers?",How many wrong phone numbers?,9lode8,new,8,2,2,0
"I am working with a standard chemical process which has 10 consecutive steps (operations). I start with a raw material that is first exposed to a process where I can control a few parameters such as pressure, temperature, time etc... It is then exposed to a second process where I can control similar parameters and so on. That is done for 10 steps and then I have a product which I can measure a few parameters on. See this sketch: [https://i.imgur.com/LjXIYKp.jpg](https://i.imgur.com/LjXIYKp.jpg)

&#x200B;

Each step is a little bit unstable so the temperature, pressure etc varies a bit each time the process step is run, but I can still measure it. I don't know how much the noise is and how much is actual variation though.

&#x200B;

This process only takes a few minutes and has been run quite much in the past so I have data from a few thousand experiments where I have all data for the different process steps as well as the end results. The problem is that end results has a quite large variance and drifts, and I want based of the old data understand how the different process steps contribute to this variation.

&#x200B;

Every step after the first will affect the end result differently depending on all of the previous steps. I.e if the one step once had a mean of 100°C, and the next time only a mean of 90°C. This will affect the end result even though all consecutive steps where identical both times.

&#x200B;

Which approaches would reasonable to use to try to determine how the parameters influence the end results? I've thought of linear regression? Multiple target regression? Principal component regression? Random forests? But to my limited knowledge I don't think any of them really suits.",Which approach for this problem with several process steps?,9lo5c8,new,7,3,3,0
"Am I doing something wrong, or PSPP (the free version of SPSS) can't do Two-Way ANOVA?",Can PSPP do Two-Way ANOVA?,9lnwdi,new,8,1,1,0
,"When building linear models, assuming noise comes from gaussian, as we have more x values, mse should go down, not up right?",9lnvrd,new,5,0,0,0
,JMP or SPSS?,9lnnmy,new,5,0,0,0
"Whenever I try to make boxplots on SPSS, it only shows the line at the end of the whisker, and not the actual box or whiskers themselves. Does anyone familiar with SPSS have any idea why this might be happening? ",SPSS Boxplots,9lnhqe,new,2,0,0,0
"I want to calculate the odds that one fantasy football team will defeat another team based on historical average and standard deviation. I have found that over the past 3 years all the scores in the league are well represented by a normal distribution. 

My question is how can I calculate the odds that one team will beat the other based on the normal distribution I can create for the 2 of them. I am several years out from my last stats class and I only took a basic engineering statistics course so I'm a bit rusty but with some research I was able to get it down to

P(X1>X2) = P(X1-X2>0) to 1- Φ(-μ/σ)  [source](https://math.stackexchange.com/questions/40224/probability-of-a-point-taken-from-a-certain-normal-distribution-will-be-greater)

From here I am not entirely sure what to do. I want it to automatically calculate in excel but I can't seem to find how to calculate the Φ function. 

Thanks for the help",Find the Odds that 1 Normally distributed variable is greater than another,9lmtzf,new,8,2,2,0
"I'm trying to determine the sample size I need for a study using G\*Power, and something doesn't seem right.

The study is a 2x3 mixed design, with a between-subjects factor and three within-subjects factors. Using G\*Power, I've selected ""ANOVA: Repeated measures, within-between interaction"" and ""a priori: compute required sample size."" I entered a medium effect size (f = 0.25, based on past research) at 80% power with 2 groups and 3 measurements. I also left the correlation among repeated measures at the default of .5, and the non-sphericity correction at the default of 1.

When I do this, the total sample size that G\*Power estimates is 80. This seems rather low...did I do something wrong? Is G\*Power doing something unexpected? I've searched all over the internet and have not found a solution.

Any help would be greatly appreciated!",Compute Required Sample Size for 2x3 mixed ANOVA in G*Power,9llx16,new,5,4,4,0
"Hi, could you please  tell me when  a k-neareast  regression  would  be better  to use, rather  than any other kind of  regression?

What makes  you choose for a model instead of  another one? For  example   MARS  regression instead  of  K-nearest?  Is it  the p-value  or the  BIC  and  AIC?

&#x200B;

Do you compare  AIC, p-value  and BIC  of  both models  and then  you choose  the  right one?   Could you compare the AIC of two  different  kind of models (one  with  Knearest  regression typo  and the other  one  with  MARS regression)?",K-nearest regression,9lkalk,new,13,2,2,0
"Does anyone have any thoughts on an appropriate way to construct a confidence interval for a daily count of the number of active users (DAU) of a product?

This seems like it should be something a lot of people have already thought about, but I've had a lot of trouble finding any literature or discussion of the topic on-line.

One can think of the count of active users as a census, in which case it can be argued that a confidence interval is not meaningful.  Realistically however, there is some statistical process that generates the DAU counts each day, which has some expectation on each day - it should be possible to calculate a confidence interval around the observed count as an estimate of the expectation of the process.  I think that an explicit time-series model is probably unnecessarily complex here though.

As a simple example, we could imagine that we have some population, n, of possible users and that the observed DAU is binomial(n, p) for some p.  Or we could consider the DAU count each day to be poisson distributed.  Neither of these however seems likely to be a very good approach.

I considered trying to find some resampling approach, exploiting the fact that users have unique IDs and can thus be segmented into buckets that are independent of anything else.  I'm not quite sure where to go with this though.

Any suggestions or pointers to literature would be very much appreciated.

&#x200B;",Confidence interval for daily active users (DAU) metric,9lk6mh,new,0,1,1,0
"For a project, I ran a t-test and found little significant difference. 

My professor recommended that I observe the correlation between the two samples as a diagnostic, but I don't understand how a correlation between two distributions explains the lack of a significant difference.",Can correlation among samples affect t-test?,9lix8b,new,18,2,2,0
"Are there any books on using machine/statistical learning on electronic health records?

Looking for something like ""Analysis of Neural Data"" - a domain specific book that's pretty stat heavy. Also, I'm looking for applications of statistics/ML in clinical healthcare, not medical research.",Any books on EMR statistics/bioinformatics?,9li9ct,new,2,1,1,0
"Hello everyone! 
Is there a handy primer that you know (maybe suitable for an advanced undergraduate class) that clearly delineates the assumptions for the t, F- and regression models that states the assumptions, the effects of violation, how to spot the violation and how to treat it? 

Pulling from half a dozen sources now, but if there's a single resource that's more accessible I'd be grateful to know. Thanks! ","Accessible guide to assumptions of t-, F-tests and regression?",9lgr10,new,6,42,42,0
"Trying to solve a problem here, would love some help! 

A data set has missing values and is positively skewed with skewness = 1. Further examination tells you that they are spread along 1.5 standard deviation from the median. How much data would remain unaffected (%).  

Unaffected, I'm guessing, means the data with no missing values.

Thank you for any help!",A novice seeking help to solve a statistical problem.,9lg3cq,new,0,1,1,0
"Hi r/statistics, 

I am learning about the Shapiro Wilk test and I see that the formula is https://imgur.com/a/gLovdZ4. I don't understand what the 'a(i)' part is from the numerator. I looked at the wikipedia page which had an explanation, however it was too confusing for me to follow!

Please could someone explain it in a straight forward way?

Thank you!",What is 'a(i)' on the Shapiro Wilk test?,9ldb1z,new,3,1,1,0
"Not sure of the level of readers in this subreddit, but I wanted to share a resource two basic statistical concepts: correlation and p-value. [This tutorial](https://dataschool.com/correlation-p-value/) explains what both are and goes through some example calculations. Happy learning!",Correlation and p-value,9ld800,new,5,6,6,0
"I have a big number of hotels, and each hotel has an number of price matches. There are two types of price matches : exact price match and price mismatch. 

The number of price matches per hotel could vary between 1 and 15000, and this distribution has a skewness coefficient of 2.02 .

For each hotel I want to compute the exact price match ratio . The formula is number of exact price matches divided by all price matches.

But I only want to do that for the hotels what have a decent number of price matches. I mean, if a hotel has only one price match and it's a exact price match, I can't be confident that the actual score (100% exact price match ration) is relevant. 

How can I find the minimum number of price matches for one hotel, that will give me, let's say 95% confidence in the result?",Finding the minimum number of entries that will give me a confidence in the result,9lc8me,new,10,12,12,0
"Hi!!  

We are two students of University of Milano-Bicocca and we need your help for our thesis!

Could you fill out this questionnaire? It takes a few minutes!

Link ""Survey “Distribution Channels and Perception of Made in Italy in South Africa”: [https://sondaggi.didattica.unimib.it/index.php/538562?lang=en](https://sondaggi.didattica.unimib.it/index.php/538562?lang=en)

&#x200B;

Thank you in advance",HELP FOR THESIS!! Survey for South Africans!!,9lc4gj,new,0,0,0,0
"I am analysing tumors, mainly vascular permeability, in treated tumors that are regressing in volume over time. I used digital contrast enhancement with MRI for it. I have put the semi-quantitative date into a Tofts-model and have gotten values between 0 and 1. I'm at an impasse on how to analyse parametres gotten from a 3D-spatial map (MRI). Each individual voxel is numbered and has a variable value attached to it. I want to compare these values over time but as you may have guessed, if the tumor shrinks, voxel 1 at day 1 is not voxel 1 at day 5 anymore. So I thought about getting a mean of some kind, but a mean value does not mean much if it is a mean of 2400+ indivual values.   


A simple graph wouldn't mean anything either. Would Area Under the Curve be a more sensible approach? I'd still have to correct it for regressing tumor size.

Basically row 1 is an individual voxel on a 3D-spatial map, attached to it is a single value.  
...untill you get to row/voxel 2460 and has a value to it attached as well.","Analysing and comparing voxel-based parametres of MRI, output to an excel file",9lc3iy,new,2,1,1,0
"I'm looking for a stats application that will help me see the work behind a problem and help me understand what I'm doing wrong instead of just spitting out an answer. Right now we are doing conditional probability, Bernoulli etc. Anything that could help? Preferably free",Applications to help me understand the work behind problems,9lbcy1,new,4,2,2,0
"Dear all,  
I have two regression models, one linear regression and one binary logistic one. Now I want to describe the models in mathematical notation but I would only like to include 1 equation in the paper. How can I model these equations to be both applicable to both models, even with a different link function? Like in a linear regression I would write  
Y = beta_0 + beta_1*x1 + beta_2*x2 + e  (linear).

",Regression equations for two models,9la6u6,new,4,1,1,0
,Did you fail any classes on the way to getting your degree?,9l9ddr,new,35,17,17,0
My department is hosting a [workshop](https://sites.google.com/site/utd2019workshop/) on data science from the perspective of mathematics and statistics in summer 2019. I just thought I would get the word out.,(Summer 2019) Workshop on Recent Developments on Mathematical/Statistical approaches in DAta Science (MSDAS),9l7yf0,new,1,16,16,0
"I'm still quite confused on how to use indicators and expectations. In hindsight, I can understand the explanation for problems but when it comes to doing problems using them, I feel like I'm just guessing how to use them. Please help me understand!",How to use Indicators?,9l67va,new,3,1,1,0
"Int just occurred to me that perhaps Pareto Distributions are caused by a snowball effect. For example, we know that wealth/income inequality (forgive the leftist term lol) exists on a Pareto Distribution. But maybe that’s caused by the “Mathew Principle” where ‘the rich get richer, the poor get poorer.’ So maybe Pareto Distributions exist due to different types of snowball effects? What do you guys think?",Do you think Pareto Distributions are caused by ‘snowball effects’?,9l5nvy,new,5,0,0,0
"I’m really struggling in my statistics class and I’m pretty sure I’m going to fail the first test. I’ve payed attention in class but the professor gives us zero context with the problems. He’s not explaining things in a way that I understand and I’m falling behind. I have a hard time remembering the Greek symbols and their meanings. I never realized how hard this class would be, but I need it. What are resources or methods that people used when they were first learning statistics? I want to improve. ",How can I get better?,9l4dyt,new,5,0,0,0
"Quantile regression is increasingly used, yet can be difficult to interpret.

In quantile regression, it is commonly found that X->Y association are stronger at higher outcome quantiles when the outcome is right skewed (e.g., education -> income;  socioeconomic status->body mass index). This is then variously  interpreted. However, is there a mathematical/statistical/numerical reason why this is the case? E.g. If X is associated with Y, and Y is right-skewed, then associations will be higher at higher outcome quantiles. When looking at BMI, almost everything seems associated more strongly at higher quantiles (e.g. Where X is SES, genes, behaviours etc). 

Thanks!",Quantile regression interpretation with a skewed outcome,9l3xxs,new,4,0,0,0
"I'm sitting in a class which is ""taught"" by an instructor who seems to just be publicizing a stream of consciousness.

I understand why this sort of modelling is important, but I am more interested if/where this is used in practice.

Does anyone do causal modelling before designing an experiment?",Do any of you use causal modelling?,9l2ckz,new,16,15,15,0
"Hi there!

I am working on a Market Research project and could use some help with terminology to narrow my search terms while doing research about a specific problem.

Problem: Each year we model out market share for a client using their sales and survey data. The client wants to compare the modeled data year-over-year, but we caution against that since their are deviations in the modeling process each year (i.e. different number of survey periods, changes to process, changes to the way accounts are bucketed for regression, etc...).

I’ve been researching to see if there is a way we can transform the data to make it a little more accurate for a yearly comparison. I’ve been searching for a good methodology or “best practices” when doing this, but have come up empty handed so far. Most of my searching results in ways to compare the validity of two different models but not how to compare the results of models over yearly accurately.

I thought I learned about this in school as a practice, but textbooks aren’t yielding anything concrete. Is there even a term for this, or is it so specific to the actual models used that there isn’t a one size fits all approach?",Help with model comparison terminology,9l1ehg,new,0,1,1,0
"Hi r/statistics \- I'm newly learning SAS and have been trying to figure out the correct procedure to use - I want to compare two variables, one of which is dichotomous (did someone have an illness or not) and one of which is categorical with more than two options (so lets say married people, single people, divorced people, widowed people, etc.) Is there a way to run something like a chi-squared where SAS will tell me which of the relationships is signifigant? 

&#x200B;

Thanks if anyone can help! ",SAS help - wanting an procedure Chi-square but where one of the variables has more than 2 options....,9l139t,new,2,2,2,0
"Sorry  for  the dumb question, but when  evaluating the  model  to use what is multicollinearity referred  to?  

&#x200B;

Is it referred to the linear relationship  between one  or more  predictors and  the dependent  variable (Y) 

OR

is it referred to the linear relationship  between  two  predictors  themselves (x1  and  x2  for example) ? 

&#x200B;

Which  model to use  in case  of  multicolinearity?",multicollinearity,9kz30y,new,11,0,0,0
"hi everyone,

To begin with, sorry if this post shouldnt be here as Im was not sure where I should post it to.

I 'm learning data analysis (with Python but I can handle other languages , excel macros vba...) and I""m trying to figure out how to analyse sales (performance) of products in a time series format using sample data file here https://community.tableau.com/docs/DOC-1236

There is a tutorial that I found https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b

but what I'm after is to study the performance of each product (like if the product is performing better over time or not, plus performance between each product etc..)

Basically, I am expecting to be able to answer a few questions such as - find out the products performed well last month /year but no this month/year and vice versa

find the ones that are selling well in country X but not country Y

explore the patterns (for the above, for example , is price increase a factor)?

I'm not sure if there is a keyword that I should google for to nail down this particular analysis that I""m after. I'm hoping I can get some suggestions from this group :)

thanks for your time",Questions re sales data & product performance analysis,9kyull,new,0,1,1,0
"While predicting even direction of change in financial time series is nearly impossible, it turns out we can successfully predicts at least probability distribution of succeeding values (much more accurately than as just Gaussian in ARIMA-like models): https://arxiv.org/pdf/1807.04119

We first normalize each variable to nearly uniform distribution on [0,1] using estimated idealized CDF (Laplace distribution turns out to give better agreement than Gauss here): 

**x_i (t) = CDF(y_i (t))** has nearly uniform distribution on [0,1]

Then looking at a few neighboring values, they would come from nearly uniform distribution on [0,1]^d if uncorrelated - we fit polynomial as corrections from this uniform density, describing statistical dependencies. Using orthonormal basis {f} (polynomials), MSE estimation is just:

**rho(x) = sum_f a_f f(x)** for **a_f = average of f(x) over the sample**

Having such polynomial for joint density of d+1 neighboring values, we can substitute d previous values (or some more sophisticated features describing the past) to get predicted density for the next one - in kind of order d Markov model on continuous values.

While economists don't like machine learning due to lack of interpretability and control of accuracy - this approach is closer to standard statistics: its coefficients are similar to cumulants (also multivariate), have concrete interpretation, we have some control of their inaccuracy. We can also model their time evolution for non-stationary time series, evolution of entire probability density. 

Slides with other materials about this general approach: https://www.dropbox.com/s/7u6f2zpreph6j8o/rapid.pdf

Example of modeling statistical dependencies between 29 stock prices (y_i (t) = lg(v_i (t+1)) - ln(v_i (t)), daily data for last 10 years): ""11"" coefficient turns out very similar to correlation coefficient, but we can also model different types of statistical dependencies (e.g. ""12"" - with growth of first variable, variance of the second increases/decreases) and their time trends: https://i.imgur.com/ilfMpP4.png",[Research] Practical Markov modelling for continuous value time series - by estimating joint distribution of a few neighboring values with high degree polynomial,9kyhng,new,4,50,50,0
"How can I prove that Adjusted R^2 is monotonically increasing? A hint that I've got is R^2=F/(F+c), where c is not dependent on x or y.",R^2,9kygj4,new,4,0,0,0
" [https://s.surveyplanet.com/0BDTt49P7](https://s.surveyplanet.com/0BDTt49P7)

Thanks guys, you're real lifesavers!",Could y'all please take this survey for my Stats class?,9ky85b,new,3,0,0,0
"I'm just finishing up a probability theory course (for an applied statistics program), and will be starting a math stats course in a couple of weeks. At some point halfway through the course, the difficulty spiked significantly, and then again when we started working with joint distributions. 

What should I expect when I move onto math stats? My grade is fine right now, but I felt like I was treading water with this these last couple of sections. I'm worried that it'll just get crazier after this. ",Where does the learning curve go from here?,9ky6id,new,13,4,4,0
,Is your statistics related job boring?,9ky2j5,new,22,4,4,0
"Hi,  in portfolio risk management  Var  and  Cvar  are  a  way to determine  risk magment  of an investiment.  I  am  at the beginning in  statistics,  and    maybe  you know  better  than me.

I was  curious   about HOW,   given a  request  of  a customer,  Example:

&#x200B;

''I want to  have a 4%  of   risk  of losing 1 million   on this portfolio  and  a 95%  of  chances   to  profit  of 4% per year''

&#x200B;

how  can  hedge  funds  teoretically satisfy a    request like  this?

&#x200B;

I  mean, they build their  model   (I assume  using Montecarlo  simulation),  and then  how  do  they  satisfy the  request of  the customer (5% of chances of  losing only one  million  and 95% of chances of profit of 4% per year) ?

How  does  it go the process  with  a Montecarlo or a  regression?  How  do they  find the confidence intervals?

&#x200B;

Thank you and sorry  for  the  question, I just would like  to know,  I am fulll of  curiosity and  at the beginning",confidence intervals for portfolio risk managment after a model / Montecarlo siulation,9kxkba,new,0,2,2,0
"Hi! I'd like to know if my reasoning is correct.

&#x200B;

I'd like to deduce the approximate number of pregnant women from the number of births in a given country in a given year. Let's say there are 1 million births in 2017 in a given country. Let's use the following simplifications:

* There are no twins, no miscarriage, nothing of the like. Every birth comes from exactly one pregnant woman, each pregnant woman gives birth to exactly one kid.
* 2018's births will be virtually identical to 2017's.

&#x200B;

**Is the following correct?**

&#x200B;

There is a 1/4 chance that a woman will give birth in the same year as conception ; there is a 3/4 chance that a pregnancy will be spread over 2 years. Therefore 2017's 1 million births can be accounted for this way:

* 1/4 of births will be from women that got pregnant in 2017 -- i.e. 250,000.
* 3/4 of births will be from women that got pregnant in 2016 -- i.e. 750,000.

The number of pregnant women in 2017 is the number of births in 2017 + the number of conceptions from 2017 not yet born. Assuming 2017 and 2018 have the same-ish number of births, the number of conceptions from 2017 not yet born is the same as 2017's figure for births resulting from a conception in 2016, i.e. 750,000. *Therefore the number of pregnant women in 2017 is 1,750,000 if there were 1 million births in 2017.*

&#x200B;

Is that right? Thank you.","Based on number of births, find out number of pregnant women in a given year",9kwwp8,new,3,2,2,0
So I know that STD dev. is the square root of variance. And variance is the average amount of distance from the mean for the data set. How do I describe std deviation without it sounding like variance and simply giving the equation in words? Thanks!,Trying to answer the question of variance vs standard deviation. Please help!,9kvrlt,new,7,1,1,0
"Title pretty much covers it, I don't have the money to buy a TI-83 or TI-84 and need a calculator that would for for stats. I have an old TI-30xs Multiview from highschool, will it work fine? Or is it missing features that I'd need?",Is a TI-30xs Multiview enough for a stats class?,9kutuv,new,7,2,2,0
"Trying to do a sample size calculation. I have an MS in stats but this is sorta new for me.

First question: Logistic regression to see if continuous variable A impacts the probability of being ""yes"" for event Y, which is something that happens a couple months after the other variables are measured:

Y ~ A + B + C

Where A is the continuous variable of interest and the rest are control factors like the person's weight. Based on recent research into relaxing the rule of 10 I said they'll need at least 5 cases * 3 variables = 15 ""yes"" cases to determine whether A has a significant impact on the outcome. It's a somewhat rare event so they'll need probably n=60 total to get 15 yes cases.

Question 2...they also want to investigate whether an intervention can influence A. So they'll measure A before and after the intervention, but before event Y.

I doubt they want to enroll a whole new set of patients for this second question. Thus I'm thinking I should re-write the original model as:

Y ~ A + B + C + intervention

Even though ""intervention"" isn't of interest in that model, it would still have to be controlled for if they use the same population, I would think..since it is a possible source of variability. 

Thus they actually need 5 * 4 = 20 cases to answer question 1.

As for question 2, this would require an [ANCOVA](https://www.theanalysisfactor.com/pre-post-data-repeated-measures/):

A(post) = A(pre) + intervention

And for its sample size calculation, I just used GPower for t.test of means with previous literature info for mean/variance of A. Does this sound legit?","Sample size calculation: Trying to kill two birds with one stone, is this logically/statistically valid?",9ku2pg,new,5,2,2,0
"I'm analysing a company's sales data. I'd like to calculate an up-to-date statistic to quantify the frequency of purchase for every costumer. At a first glance this looked an easy task to me but now I realise I can't move forward. I'd say that there are two equivalent ways to look to at the problem: one is to focus on the frequency of events (number of events / given period of time), the other is to quantify the average time between two purchases.

There are several reasons why I can't proceed:

* I don't know how to deal with new costumers: should I exclude them from the analysis? Wouldn't the estimate of the frequency of purchase be underestimated for this costumers?
* How can I choose the right interval of time for the calculation of the frequency? For example, should I focus on a semester, a year or a 3-years period?
* It is reasonable to think that the frequency of purchase might change over time. How can I handle this variation? In time series analysis moving averages are very common. Should I use a rolling statistic as well?
* It is reasonable to think that the frequency of purchase depends on the quantity of items bought during the last purchase. How I control for this fact?

I'd really appreciate if you could help me.

Thank you a lot in advance.",Frequency of purchase,9ktrtz,new,1,0,0,0
"Hi everyone,

I have a theoretical doubt about the different type of data and the analysis that I can perform.   
I wanted to do a correlation matrix to see if my variables were associated to eventually perform an exploratory factor analysis. Some of these variables were normally distributed and some weren't. I was told that if my variables are not normally distributed, I could use the Generalised Linear Models. However, by doing so, I realised that my variables are mostly of the COUNT type so, as far as I understood, I cannot treat them as continuous. This means that the correlation matrix and the factor analysis cannot be undertaken and I was wondering whether you know if there are cases in which count variables are treated as continuous. This is also because in previous papers using more or less the same variables as I am, they did not mention anything like that and they performed linear regressions and ANOVA as the data was continuous so I am a bit confused.  
I hope this is clear.

&#x200B;

an example of my variables:  
\- number of total error in a cognitive test

\- number of times the ""x"" object has been chosen over the ""y"" object in general and during each trial

\- percentages of ""x-type"" of offers accepted  


Thank you in advance.  


Best

Silvia",Count data treated as continuous data,9ktgp2,new,2,1,1,0
"Hi,

As the title says, what are in your opinion some good SAS courses to become an expert in SAS? ",Best online SAS courses not for complete amateurs,9kt1n6,new,1,0,0,0
"Title says it all. Book, article, web, lecture recommendations? 

Thanks friends.",Learning recommendations? Propensity Score Matching Methods,9ks9c7,new,2,1,1,0
"Hello guys, I’m new to stats and I need help with a question that I’m currently dealing with. My professor gave us a a data set and he asks “How many observations are there?”. I’ve tried to google but nothing substantial has come up. 

Eternally grateful!!! Thank u in advance",Help! What’s an “observation“?,9krvuy,new,8,1,1,0
Brian Wansink and p-hacking: https://www.npr.org/sections/thesalt/2018/09/26/651849441/cornell-food-researchers-downfall-raises-larger-questions-for-science,Cornell Food Researcher's Downfall Raises Larger Questions For Science,9krp6s,new,18,43,43,0
"I am running some analyses on a 38-item survey for parents, given to parents of kids with and without a certain disability. I've done some factor analysis and identified three reasonable factors. My boss would like me to run item-by-item group comparisons, so I'm using the MWU test (negatively skewed data). However, she wants to know if I should be applying a correction for the child's age. I know that I could include age in an ANCOVA, but I don't know how to do this for non-parametric data. I'm not even sure I should be correcting for age. I did create a scatterplot of age vs. score for each group. For one group (the group of parents who have kids with disability) the line has a negative slope and an r-squared of .148. For the other group (the group of parents who have kids without disability), the slope is positive with r-squared of 0.059... To me, it seems like age isn't making a significant contribution. I did also run an ANCOVA, and age is not significant. Should I just go ahead and tell her no? 

&#x200B;

Thanks in advance!",Correcting for Age?,9krn4r,new,0,2,2,0
"Hi people,

&#x200B;

I'm finishing my master's thesis and had already performed the tests with participants, expecting to use RM-ANOVA to do the analysis of variance. I thought it would be pretty straight forward, since I had no problem getting the feel for the one-way ANOVA from these Khan Academy videos: [https://www.khanacademy.org/math/statistics-probability/analysis-of-variance-anova-library](https://www.khanacademy.org/math/statistics-probability/analysis-of-variance-anova-library) , and RM-ANOVA is pretty similar. Nevertheless, when I tried following this tutorial on RM-ANOVA in R: [http://rcompanion.org/handbook/I\_09.html](http://rcompanion.org/handbook/I_09.html) , I quickly realized that it's not as straight-forward to grasp how to setup the model properly and how to interpret the results.

&#x200B;

Also I found a resource that advises not to use RM-ANOVA at all, and instead move to the mixed-effect models: [https://www.r-bloggers.com/how-to-do-repeated-measures-anovas-in-r/](https://www.r-bloggers.com/how-to-do-repeated-measures-anovas-in-r/) .

&#x200B;

So all in all, I'm a bit confused and would love to get an advice and what to read to get better understanding of what I'm doing.

I had basic statistics at the uni during my Bachelor's, but either I've forgotten, or we haven't really gone through ANOVA and stuff like that. 

&#x200B;

**Experiment design**: repeated-measurements design, 3 groups, 12 participants, 24 data points per participant in total (8 per group). 

Measurements collected: participants' reaction speed.

&#x200B;

**TLDR**; Deadline in one and a half month, how do I grasp RM-ANOVA in R?

&#x200B;

Thanks and Cheers!",Base knowledge to perform RM-ANOVA (in R),9kqlob,new,0,2,2,0
"And do you think it's a growing field? I'm asking this as a soon-to-graduate Masters student whose thesis was an applied Bayesian project. Is there anything I could supplement my thesis with to make me more employable?

I'm currently working on picking up stan since I've only used JAGS before this",How often are Bayesian methods used in industry?,9kqg9a,new,20,9,9,0
"Hey all. 
PhD in political economy here. 
MA was in Intl Relations, BA in Politics. Very little background in applied quant but have always been eager. 
I’m UK based and been to a number of ECPR/ICPSR workshops on Panel Data/MLM etc. So fairly competent. 

Now that the background is out of the way. I’m working with U.K. bank lending data and employment. It’s taken months to collect the data to get to this point and now I’m planning to start some univariate analysis before my MLM. 

I have 14 industries across 74 months long wise. I need to SA the data to avoid conflating seasonal effects on employment and salaries with the effects I’m looking for. 
To this end I ran a series of regressions for each month and industry. Stored the resid and used these residuals as my new Seasonally Adjusted values. 

The problem is. This has resulted in a significant number of negative values which is counter intuitive and will cause issues for my descriptives. 
 
So my question is:
Can you offer any readings that will provide a better understanding of seasonal adjustments and recommend a method to apply it. 

I am thinking about running the ARIMA package in stata/R. But not quite sure how this works and how complex the syntax is likely to be. 

Any and all advice appreciated!",Advice on Seasonal Adjustment,9kqdfa,new,5,8,8,0
I'm majoring as a pure math major right now but it feels like it might be too hard for me. I'm just looking for a decent job after getting my degree. I'm thinking a stats major and a math minor might be sufficient. Any thoughts? I'm in my third year right now.,BS in statistics or pure math?,9ko61r,new,11,4,4,0
"Hi guys, 

I have a quick question regarding follow up ANOVAs that I was hoping to get some insights on. I have analyzed a dataset with a 3-way Anova with O x F x S (S is within-subjects repeated measure). 

The 3-way interaction is not significant but I have two significant 2-way interactions (S x O and S x F). Am I justified in analyzing these data with specific 2-way ANOVAs (S x O and S x F - S is RM in both instances and the individual factor of O and F is analyzed by separate ANOVA) to better understand what is driving these interactions?

Thanks!",Question regarding 3-way ANOVA,9ko2wv,new,4,1,1,0
" Or at least in the grad school you attended or part of. What admission requirements do you remember seeing most of the time?
Assuming this is paired with a math minor too that includes linear algebra, analytic geometry, and integral calculus.

Program in question if you're curious: [https://zicklin.baruch.cuny.edu/academic-programs/undergraduate/majors/statistics-quantitative-modeling/](https://zicklin.baruch.cuny.edu/academic-programs/undergraduate/majors/statistics-quantitative-modeling/) ","Would a BBA in Statistics qualify me for an MS Statistics in most grad schools? Aside from Statistics, what other fields would I be able to pivot to?",9knmyx,new,4,1,1,0
The last time I took a stats class was my freshman year of college. I am trying to revise and was hoping if someone can recommend me a free online course. I started doing Harvard Stat110 class but that seems too hard for me.,Want to learn Probability and Applied Statistics,9kn14h,new,3,4,4,0
"Hello everyone, I’m relatively new at statistical analysis and was hoping someone could help me out. 

What is the best type of effect size for a repeated measures t-test? 

I do have a huge difference between my means (I double checked the math) but my cohens d is =2.00  this seems very large and I am concerned especially because my sample is n < 20 

I was using the SD of the pre group to calculate the d. 


This seems like an inflated eff size, are there better ones that could adjust for bias? 

Also, if I am running many different t-tests (pre/post) for all subscales in all my measures. 

Do I do a Bonferroni correction? Or is that just for ANOVAS

",Effect sizes,9kmdcx,new,4,1,1,0
"If I had Y= alpha + beta1X1 +beta2X2 and I already have the coefficients, will adding beta3X3 which will be:

Y= alpha + beta1X1 +beta2X2+ beta3X3, will that change the coefficients of X1 and X2? I feel like if they are hetergeneous then it shouldn't right?","If I were to run a regression with two independent variables, will adding adding a third independent variable change the coefficient results of the previous two independent variables with the new regression?",9klh20,new,5,1,1,0
"I mostly find survival analysis applied to medical data, e.g., time to death or comparing different treatments or to engineering data e.g. time to failure of some mechanical part.

However, I was wondering if survival analysis could also be used on economic problems, but I can't think of any meaningful applications. 

Do you have any ideas for proper application on economic problems or know of any examples where survival analysis was used on economic problems?",On which economic datasets should I perform a survival analysis?,9kldu4,new,5,1,1,0
"I am currently looking at grad school for a few different things. I have always had a love of statistics and have done well in the statistics classes I've taken, but I've never really followed up on it because I'm just not sure what one would do with a degree in statistics. Any insight on what kind of job a person could do with a statistics degree?",Statistics Jobs,9kl48b,new,3,1,1,0
"edit: Thanks guys, this is definitely enough to find something from! 

I am trying to find interesting conferences in the US or Canada to go to (will be my first conference). 

So far I'm leaning toward this one, which my boss said was pretty good last year

http://www.amstat.org/ASA/Meetings/Conference-on-Statistical-Practice.aspx

Anyone have any other suggestions? Symposium on Data Science and Statistics doesn't have much info...",Good conferences for statistics? (esp. biostatistics),9kkliy,new,13,19,19,0
I have a data set where I'm looking at different facility types (Think SNFs) and I want to find the variables that have the largest effect on these facilities. In other words I want to see if these variables influence the facility type. How would I go about approaching this problem? Thanks for any help ,Finding variables with the biggest impact,9kjvzk,new,1,1,1,0
"Hey all,
Im doing a logistic regression using binary variables and want to double check how I’m interpreting the coefficient. Initially, I generated a variable (let’s say A and B are my two variables so the third variable is A*B). I used an alternative method with the  # commands so I did i.A and i.B and then did “logistic outcome i.A#i.b” and got an output that reads 0  1, 1  0 on the left and 1  1 and coefficients. The first two coefficients make sense as the coefficients of A and B alone as confirmed by a regular logistic regression. However, I am wondering if 1 & 1 is the interaction coefficient and I can interpret this the same way as adding cons+coef A + coef B + interaction. Because when I do that manually, I don’t get the same number as 1 1 in the output ",Interpreting interaction term on stata,9kjmgf,new,5,1,1,0
"Hey, I'm sorry if this is a stupid question or not the right location to post this. 

&#x200B;

I've recently graduated in an MSc in Social Anthropology, and have secured a job as a policy researcher. However, I'm looking to improve my CV for future interviews and promotions down the road. I have a lot of experience in Qualitative research (fieldwork, ethnography etc) but not much at all in Quantitative research. Too cut this post short, where the hell do I start? I've heard that R Studio is a great place to begin with, and I luckily still have online access to a introduction module on the software at my university, but I have also heard of SPSS, Python, SQL, SAS. 

&#x200B;

If any of you could give me a run down of where a good starting place would be to learn statistics software/the differences between softwares that would be extremely helpful. I am a policy researcher at a political organisation, and most of my future research work will revolve around social science/social policy, health etc. 

&#x200B;

Thanks! ",I'd like to know which statistic software I should start learning,9khs8m,new,24,1,1,0
"A class of 600+ students are asked to report how many hours they study per week. John studies 12 hours per week with a standard score of 2. Ashley studies 3 hours per week with a standard score of -4. 

Determine the mean and variance for the class of 600+ students

Don’t give me the answer just explain how to solve it, I know you have to work backwards I’m just stuck, thanks!",Can some help me with this stats problem?,9kh4v3,new,3,0,0,0
Howdy stat folk! I'm encountering an issue where running split tests and reaching statistical significance is taking far too long and Im getting push back. I need to be able to run a test in 45 days or less. What method would any of you recommend for finding results after a defined period of time rather than surpassing a 95% thresh hold every time?  ,Running Split Test in Defined Amount of Time,9kgktz,new,0,2,2,0
"Next semester I will be taking that class again with the same professor that failed me last semester. (He failed a total of 5 people out of a class of 17, and these are smart people who had Math and Statistics degrees in undergrad). Me? I had an Environmental Science degree and a Math minor, so it’s even harder for me. The book we are using is Intro to Mathematical Statistics by Hogg (Chapters 4-8) which is insanely difficult to understand because I am dumb and obviously not smart enough to achieve a Master’s degree. 


I just need two more classes until I graduate with a M.S. in Applied Statistics, but this professor is acting like a gatekeeper and is preventing me as well as others from graduating. He’s the type of person that made the final exam 50% probably so he could watch people suffer and break down. It’s really pissing me off that I came so far, worked so hard and this grumpy old man is denying me access to a degree. Hopefully this second time will be better and I can pass, otherwise I wasted $40,000, 2.5 years of my time and energy to pursue something and in the end to be told to get lost because I’m too dumb. Sorry for venting and sounding like an entitled jerk, but I need advice on what to do.
",How can I pass Mathematical Statistics I: Statistical Inference?,9kgcrp,new,37,11,11,0
"Hello guys,

I am doing a project and was told to use a chi-square for my values.
I used percentage out of 100%, as you can see in the attached picture.
https://imgur.com/a/5rVTngl   
I really struggle with math. My percentages don't add up to 100 as I didn't include the values for underweight participants

1: Have I correctly completed the chi square?
2: how does a chi square work?
3: is my data significant?

I am just having a really hard time. Thanks for anyone who responds.","HELP, I really don't understand CHI squares",9kfkw3,new,4,1,1,0
"I'm tasked with writing a report explicitly excluding simple contrasts but including main effect contrasts.

It's a 4x2 design, and a lot of my upcoming data relies on the 2 contrasts I produce, so I want to make sure I get them right. How can I ensure I'm using main effect and not simple?",The difference between simple contrasts and main effect contrasts,9kfi8n,new,0,3,3,0
"I have a weird model that I am sure someone has examined before.

&#x200B;

The bias of my coin p(t) changes in time.

Call heads '1' and tails '0' and the result R \\in {0,1}. The Bernoulli distribution is

Pr( R|p(t) ) = \[ p(t) \] \^ R \[ 1 - p(t)\] \^(1-R)

Assume I am sampling the coin at a rate that is sufficient for my purposes.

&#x200B;

**Question: how do I estimate p(t) in a principled way?**

E.g. I could try to do MaxLik on a moving window consisting of a fixed number of coin flips. But how do I choose the window in a sensible way?

&#x200B;",Learning the time dependant bias of a coin,9kd272,new,4,2,2,0
"I have many questions :( Thanks in advance!
1. what are the differences between these two equations?
y= xβ+ɛ and
ŷ= xβ^ , r = y-ŷ

2. I know that ŷ= xβ^ is the estimate equation, but isn't y= xβ+ ɛ also the estimate equation?

3. Assuming ɛ~N(0, σ² I_n), why is y~N(Xβ,σ² I_n) ? Shouldn't it be y~N(Xβ+0, 1+σ² I_n) ?

4. r =y-ŷ , which is ~N(0, σ²(I-X(X^TX)^-1 X^T), but somehow it can become (I-X(X^TX)^-1X^T)Y? (from my lecture note)

5. why is the rank for r equal to (n-p-1) , but SS_tot is (n-1) and SS_exp is (p) ? How is the dimension being decide?

6. What is SS_res/σ² used to calculate, and why does it follow a chi-sqare distribution ?",Explanation of Statistic formula,9kccj0,new,2,0,0,0
"hello r/statistics,

I know this is more math than stat, so if it doesn't belong to this sub please let me know I will remove it. 

This is the textbook I am using: http://www.utstat.toronto.edu/~brunner/books/LinearModelsInStatistics.pdf

both of my questions are on page 109.

1. Is there a name to this formula https://i.imgur.com/3NH6TPU.png? The second half of the equation mainly. It looks similar to (a+b)^n except every ""b"" is a different constant. 

2. If we let [C equal to this](https://i.imgur.com/PwI15um.png). I believe the assumptions here are A is a constant matrix and Sigma is positive definite & symmetric. How do they end up getting [this derivative](https://i.imgur.com/Um0wqjs.png)? 

Thanks in advance!","2 separate questions on formula derivation, Linear Model in Statistics by Rencher and Schaalje txtbook",9kat3o,new,4,1,1,0
"Hi! I am a 3rd year Statistics Major, applying for Co-op placements for the coming summer. I'm looking into putting some projects onto my resume to both look good to employers and to actually become more familiar with R and SQL (I'm pretty comfortable with python, but I wouldn't be against a project that used a combination of python and another language). What kind of projects using R and/or SQL would look good on a resume for statistics/data science jobs? I have very little experience with R and SQL, but I am very good at googling, so any beginner/intermediate project ideas would be perfect! I am also currently learning R and SQL online to have working knowledge of them by the time I get to interviews. Any help/ideas would be greatly appreciated!","Ideas for projects using Python, R or SQL",9ka3lp,new,13,30,30,0
These two classes are not included in my stat curriculum at my university. Worth taking to boost my marketability? ,How often do statisticians/ people with stats degrees use Calc III and Linear Algebra?,9k8ymp,new,18,5,5,0
I am looking at some limb proportions in different animals. I would like to test how much an outlier in the data for a given animal deviates from the trend line for those data. I would also like to test whether this deviation is significantly different from that seen in the data for a different animal. I'm not sure if this question even makes sense.,What kind of analysis to use?,9k8txy,new,2,1,1,0
"Hi! I'm currently reading notes about neyman-pearson and came across a problem I'm having trouble understanding the math behind. I understand the process of finding the significance level, power, etc. 

[In this problem](https://imgur.com/d7xVRdu), we must first find the likelihood of the test statistic under the null and alternative hypotheses. While this is clear, I'm not sure how they went from that step to saying the sum of all observations x_i are equal to x_bar. I've tried analyzing and working through the problem but can't figure out how it simplifies to x_bar. Any help would greatly appreciated! ",Confused about the math in neyman-pearson problem,9k7wf4,new,3,1,1,0
"So I've been wondering about this for a while now, but do you think it requires a high IQ /brain power to perform the job well on a day to day basis?

So after I started reaching higher level math and statistics courses I realized that I don't really stand a chance on the homework problems (if I try to do them alone without help, (but I tend to understand them with solution)), so starting to be a bit scared now if that will also translate into low performance in any job I may get after University. Will I possibly get fired for low performance? is it as ""IQ"" intense as solving theoretical problems? In other words, do you just sit there on your job and feel completely useless /helpless all the time, and have to ""leech"" off the other employees, or do you mostly just do repetitive shit, basic procedure type things, that doesn't really require that much raw brain power? do you have to ""create"" a lot of new stuff /unseen stuff (that are intellectually advanced)?

I actually have no idea of what the other students are doing, but it seems like I'm getting by ""fine"",, and it actually seems like I will be able to finish University despite not really being able to do the problems in more advanced books as well as homework problems. I just learn the solution on practice exam and hope for the best, usually end up in the 50th percentile or so. So I have no clue what's happening with the other students, or how on earth I'm still middle of the pack, if they're just ""faking it"" as well or not.

What do you guys think? How is work life like? How intellectually demanding is it?  What about Actuary? What about Quantitative analyst, what about Data Science, ML Engineering etc? Obviously I'll mostly be interested in these types of jobs since they are the ones that are popping right now, just wondering if I'm cut out for it... If you have any other suggestions and know how demanding it is, feel free to say so as well.",How intellectually demanding are jobs that require a master's in statistics?,9k6ng8,new,17,36,36,0
I've been struggling with arithmetic means for about a month now(taking AP statistics) and still don't know why they work. I know i'm probably doing this wrong but if we took two people and calculated how much time each person spends on the computer a day and one spent around 12 hours and another spent 5 how could the average be 8.5? Neither of them spent 8.5 hours on the computer. Thank you.,Can someone please help explain arithmetic means?,9k5znv,new,15,1,1,0
"Hey there!

I hope this doesn´t count as homework, but I was sitting in uni and we were briefly takling about a paper.
I saw a graphic that I didn´t understand and after staring at it for almost an hour at home I still have no idea how to interpret the information.

[THIS](https://gyazo.com/5020840ae1af1d70102bdd08a0ee5dc1) is the graphic I´m talking about.
It´s from [THIS](https://www.thelancet.com/journals/lancet/article/PIIS0140-67361732252-3/fulltext) paper. (PURE-study)

I get the main info, but I have no idea what the blue dotted line is supposed to tell me. 
Since the only info given is that the data was adjusted for age, sex, etc., the only thing I could imagine is that the top-line are the highest riskfactors combined --> highest risk
And the lower line are the lowest riskfactors combined --> lowest risk.
And then the black line would be the average?


On a related note:
In the same paper you´ll find [THIS](https://gyazo.com/8d142829bb0628077f6e76f0ab5eaf2c) table.
While looking at this paper my prof. said that in this case ""Quintile"" doesn´t mean the same as it usually does  in statitics. 
For some reason he didn´t feel the need to explain what it actually does mean here.. Does anyone have an idea?
 
Would be really happy if you guys could help me. And before someone asks: I didn´t get the chance to ask my prof. ",Help interpreting a graphic,9k5i50,new,2,2,2,0
"Hey guys,

I'm a first-year statistics grad. student with no previous stats. experience. I'm taking a class on inferential statistics and although I'm doing well, I'm basically plugging and chugging my way through. I don't want to simply memorize Bayes' Theorem, or the CLT, or a formula for conditional expectation, I want to understand what this all means. If you guys could point me in the direction of some videos, books, or otherwise that provide visual representations or philosophical discussions for stats.,  I'd greatly appreciate it.

Thank you.",Proditional Conbability?,9k40ew,new,4,0,0,0
"Say I did linear regression and got the coefficients to show that a 1 dollar increase in R&D expenditure provides a $0.5 increase in revenue. All the residuals and normality etc are looking good, so I go to my boss with the great news, and they ask me ""So then **how much** should we invest in R&D?"".  


There might be a nice correlation, but it won't hold forever, and eventually a 1 dollar increase in R&D will lead to a **reduction** in profit.  


How would you go about determining how much to invest in R&D? Using calculus to find a maximum springs to mind, but what formula am I working with? The equation for a straight line? In which case after taking the first derivative I'd be left with just the coefficient, which is next to nothing, and the second derivative would be 0 so it's a saddle point, so quite sure that's not the way to go.  


Sorry if I'm not asking this in the right place, honestly not too sure where to ask this. Am quite open to suggestions to that as well. ",Question about what happens after building a model re: company profit,9k2wy4,new,19,24,24,0
"My dependent variable is a chemical concentration found in water.   Many observations take on values less than 1.   Some are greater than 1, as high as 20.   How can I take the log transform of this.    If I do that, then all the observations less than 1 become negative.   I cannot have negative values of a chemical concentration.   Thank you for your time. ",Y transformation?,9jzq5a,new,5,1,1,0
I have a device that measures the population of something which used to be measured by manual assessments. I have 12 of these devices at different geographic locations. I want to test the accuracy of these devices by comparing them to manual assessments with 95% confidence. Would anyone be able to tell me how I determine how many assessments I need to do?,Need help determining sample size,9jyqqg,new,7,10,10,0
"For a 2x4 factorial design, i am going to use Scheffe because the contrast is planned and post hoc. However, the SPSS result is lower than the critical F that I have calculated which means it should be non-significant, but the p-value is lower than 0.05 which is my alpha.  


Edit: I used this formula to calculate the critical F: [https://imgur.com/0196h5Q](https://imgur.com/0196h5Q)

  
Where have i made a mistake?  
",Help with Scheffe's Procedure,9jx8rp,new,3,1,1,0
"Can I use LDA  as  a  substitute of regression analisis to make  prediction about a  dependent variable, which  in LDA  is  called  ''class''  or  would  it be  useless?  I want to make a multivariate  non linear regression with many predictors (road conditions, atmosphere weather, speed) and  the dependend  variable  Y would be performance of  a car.  I would  also  make some interactions  between predictors.  Do you suggest  me  any machine  learning  tecninque, or  to  go  with  a non linear  regression (the  relationship  is  not linear between predictors and  Y, as  I have  seen  plotting  the data)

&#x200B;

I would  also  find out  WHICH  predictors  explain the  most  the variance of the  dependent  variable.

&#x200B;

&#x200B;",Can I use Linear discriminant analysis (LDA as a substitute of a regression model?,9jupbk,new,6,0,0,0
"Hi all,

I am new to logistic regression so I just wanted to get some input to ensure that I not missing anything essential and am interpreting things correctly.

I have a dataset where subjects were rewarded (1) or not (0) on each trial and for each trial there is associated activity during an expectation or choice phase.

My intention was to use logistic regression to see if there was an association between activity and presence or absence of reward.

Am I correct in thinking that the coefficient for x1 (0.29, p<0.000) would indicate that there is a positive association between reward and activity during expectation (i.e., increased activity associated with reward) and the coefficient for x2 (-0.25, p<0.000) would suggest a negative association between activity and reward for the choice (i.e., lower activity is associated with reward).

[Link to screenshot of logistic regression output](https://imgur.com/u3awqiB)

Thanks in advance for any insights :)

Edit: typo","New to logistic regression, am I interpreting this correctly?",9jtyug,new,15,12,12,0
"Ive never had a formal introduction to statistics and I try to learn concepts as and when I encounter them. I came across the chi-square test recenty. Am not able to figure out why it works? 

I can find the goodness of fit without any problem. What am trying to figure out is what is the intuition behind it. Almost all articles I found online just tells you simply how to perform the test. If you could either explain the reasoning behind it or point me to some ulrs explaining it I would appreciate it very much.

Thank you.",Why does chi-square test work?,9jtg2v,new,8,1,1,0
"I'm trying to answer three questions relating to machine learning:
1) Why does the normal distribution matter so much? (I think this is because of the properties such as symmetries)
2) Why do we want to know if two samples are from the same distribution?
3) Why is the central limit theorem important for machine learning? 

Sorry if this is the wrong place to post! ","ELI5: Understanding central limit theorem, Gaussian distribution, and their relevance",9jsttg,new,11,5,5,0
"if I had  **log(y)**=  **α0+ β0D0+ β 1logX1+ β 2logX2+**  **ε**  where the **β0D0** includes the dummy variable, how do I interpret the results? If my coefficient is, say -.016, what does that mean? Is it if the dummy variable is **1**, then the **log(y)** decreases by **1.6%** ",How do you interpret a dummy variable in a log regression?,9js513,new,16,4,4,0
"Could  you please  help me  to understand  when to perform an Instrumental variables estimation?

&#x200B;

In [statistics](https://en.wikipedia.org/wiki/Statistics), [econometrics](https://en.wikipedia.org/wiki/Econometrics), [epidemiology](https://en.wikipedia.org/wiki/Epidemiology) and related disciplines, the method of **instrumental variables** (**IV**) is used to estimate [causal relationships](https://en.wikipedia.org/wiki/Causal_inference) when [controlled experiments](https://en.wikipedia.org/wiki/Controlled_experiment) are not feasible or when a treatment is not successfully delivered to every unit in a randomized experiment.[\[1\]](https://en.wikipedia.org/wiki/Instrumental_variables_estimation#cite_note-Imbens:00-1) Intuitively, IV is used when an explanatory variable of interest is correlated with the error term, in which case [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) and [ANOVA](https://en.wikipedia.org/wiki/ANOVA) gives [biased](https://en.wikipedia.org/wiki/Bias_(statistics)) results.

&#x200B;

It  says ''when IV (independent variable)  is correlated   with  error  terms,  ok... but  I  used to do  Fitted   versus residuals  tecnique  to see  if there  was  any correlation  or  any *Heteroschedasticity*

&#x200B;

So which  is  the difference  with instrumental  variables?

&#x200B;",Instrumental variables estimation,9jruay,new,1,0,0,0
Anyone interested in making a bit of money by helping me out with basic statistics work from now untill mid december ? I can discuss pay in DM's.,College stats help,9jrbdq,new,0,0,0,0
"The rates of on-time flights for commercial jets are continuously tracked by the U.S. Department of Transportation.  Recently, Southwest Air had the best rate with 80 % of its flights arriving on time. A test is conducted by randomly selecting 20 Southwest flights and observing whether they arrive on time.  Find the probability that exactly 11 flights arrive late.

&#x200B;

n = 20

p = 0.80

q = 11

0.0073870 but it's incorrect. What gives? What in the world am I doing wrong when I am literally following the formula step by step. By the way I am using Minitab.

&#x200B;

When I enter the same exact thing on the TI-84 Calculator I receive .0073869589 and that's still wrong.",Binomial Distribution Question,9jr4ye,new,7,1,1,0
"I made some sick graphs in my AP stats class today.

[AP Stat P1 Bar](https://docs.google.com/spreadsheets/d/e/2PACX-1vQPnV3i9EMGP55FGgl4Ojvr_3HTqywm_IrfIp03fK9I4w7v9pHFiT0aQLOtY9UnV4YqBNAqUdGAJi2H/pubchart?oid=1687073590&format=image)

[AP Stat P1 Histo](https://docs.google.com/spreadsheets/d/e/2PACX-1vQPnV3i9EMGP55FGgl4Ojvr_3HTqywm_IrfIp03fK9I4w7v9pHFiT0aQLOtY9UnV4YqBNAqUdGAJi2H/pubchart?oid=1789142130&format=image)

[AP Stat P1 Boxplot](https://docs.google.com/spreadsheets/d/e/2PACX-1vQPnV3i9EMGP55FGgl4Ojvr_3HTqywm_IrfIp03fK9I4w7v9pHFiT0aQLOtY9UnV4YqBNAqUdGAJi2H/pubchart?oid=97233106&format=image)",Graphs,9jqp56,new,0,0,0,0
  Suppose that you flip a coin 13 times.  What is the probability that you achieve at least 7 tails? ,Coin Toss Probability,9jqas6,new,4,0,0,0
"Since I follow Nate Silver's on twitter I came across this article https://fivethirtyeight.com/features/election-update-why-our-model-thinks-beto-orourke-really-has-a-chance-in-texas/

The first paragraph alone is great for any modeler:

> When building a statistical model, you ideally want to find yourself surprised by the data some of the time — just not too often. If you never come up with a result that surprises you, it generally means that you didn’t spend a lot of time actually looking at the data; instead, you just imparted your assumptions onto your analysis and engaged in a fancy form of confirmation bias. If you’re constantly surprised, on the other hand, more often than not that means your model is buggy or you don’t know the field well enough; a lot of the “surprises” are really just mistakes.

Went into the rabbit hole on how his model works and ended up on this page (https://fivethirtyeight.com/methodology/how-fivethirtyeights-house-and-senate-models-work/#fundamentals).

It doesn't go into it that deep but it did compare lite, classic and deluxe with use of CANTOR. 

I was curious if anybody know what type of model Nate Silver uses or a good guess? Is it just linear regression? I also can't find any research paper on CANTOR if anybody have those resources I'd like it too. 

Thanks.
",Read an Article on fivethirtyeight.com and thought how Nate Silver's model for polling is interesting,9jq1zv,new,4,10,10,0
What is the difference in how they are calculated as well?,What is the difference between a regular standard error and a robust/white standard error?,9jppa0,new,1,7,7,0
"JAMA has a series called [Guide to Statistics and Medicine](https://jamanetwork.com/collections/44042/guide-to-statistics-and-medicine). I just found an article written by surgeon Lisa E. Ishii titled [Thoughtful methods to increase evidence levels and analyze nonparametric data](https://jamanetwork.com/journals/jamafacialplasticsurgery/article-abstract/2296036). In the introduction, she writes 

> It is also a good example of using a nonparametric statistical test, the Wilcoxon rank sum test, to evaluate nonparametric data.

Can you spot the error?

*Data* is never parametric or nonparametric! Only models are.

I wish the editor and reviewers were a little more thoughtful (*wink, wink*) during the publication process. It's a shame that even a ""high impact"" journal such as ~~JAMA~~ (edit: It's a sister journal of JAMA called ""JAMA Facial Plastic Surgery"") can't manage to detect such errors and propagates misinformation in the process.

I just wanted to share this because it annoyed me a little bit. Thanks for reading.",Can you spot the error in this guide to statistics in JAMA?,9jp98w,new,20,34,34,0
"What does it mean if r square change is highest for one predictor (0.45) with a beta weight of .25 while there is another predictor with a lower r square change (.13) with a beta weight of .56? These are results from the last block of a hierarchical regression.  r squared = 61.2. 

&#x200B;

I thought that the higher r square change means the first predictor contributes the most variance (0.45 out of 61.2) but I also thought beta weights showed which predictors contributed to the most variance. And for the first predictor, the beta weight is less than the other one? Can someone please clarify and explain. I'm a student. ","Question about Hierarchical Regression - beta weights, significance, r square change",9jo6hb,new,7,1,1,0
"I've been reading about [""tidy"" datasets](https://vita.had.co.nz/papers/tidy-data.pdf) but the concept doesn't seem to be ""clicking"" for me yet. I'm attempting to apply the concept to refactor a dataset I'm working with. It's a record of a measles outbreak for 188 people. You can download it from [this page](https://r-forge.r-project.org/scm/viewvc.php/develop/twinSIR/hagelloch.txt?view=markup&revision=1063&root=surveillance&pathrev=1065). 

The following is an extract from the data as a `pandas` dataframe, which I've modified slightly.

		 Patient_ID  Family_ID  Household_ID Classroom   Age  Sex  House_X  House_Y Prodrome_Date  Rash_Date Death_Date   Died
	183         184         51             6         2  13.0  NaN    182.5    200.0    1861-10-30 1861-11-06        NaT  False
	172         173          9            35         0   6.0    M    212.5    107.5    1861-11-01 1861-11-03        NaT  False
	177         178         17             6         0   4.0    M    182.5    200.0    1861-11-07 1861-11-11        NaT  False
	176         177         17             6         1   8.0    F    182.5    200.0    1861-11-07 1861-11-11        NaT  False
	173         174          9            35         0   3.0    F    212.5    107.5    1861-11-08 1861-11-08        NaT  False
	182         183          4             6         0   4.0    M    182.5    200.0    1861-11-11 1861-11-15        NaT  False
	44           45         48            51         1   7.0    M     75.0     55.0    1861-11-11 1861-11-13        NaT  False
	174         175          9            35         0   2.0    M    212.5    107.5    1861-11-12 1861-11-15        NaT  False
	180         181          4             6         2  10.0    F    182.5    200.0    1861-11-13 1861-11-17        NaT  False
	181         182          4             6         2  13.0    M    182.5    200.0    1861-11-15 1861-11-18 1861-11-18   True


I've changed the column names to be more readable, omitted some that aren't relevant for my analysis, converted mm.dd to proper datetime variables, added a boolean column for whether the patient died, and a few other miscellaneous ease-of-use changes.

Here are what the columns mean:

- `Patient_ID` is a unique identifier for each patient  
- `Family_ID` is an identifier for different families. More than one patient can belong to a single family.  
- `Household_ID` is an identifier for different households. More than one patient can belong to a single household.  
- `Classroom`: which classroom the patient went to.  
- `Age`: age of patient  
- `Sex`: sex of patient, if known  
- `House_X`, `House_Y`: location coordinates of the household, in metres  
- `Prodrome_Date`, `Rash_Date`: dates of onset of the two phases of measles symptoms  
- `Death_Date`: date the patient died, if applicable
- `Died`: boolean for whether the patient died (computed for convenience)

----------------------------------

So, the ""tidy philosophy"" is:

>**Each column is a variable, each observation is a row, each type of observational unit is a separate table.**

I have a bunch of unorganized questions about this. First of all, I'm not entirely sure what counts as an ""observational unit"" here. From what I can gather, I think I need to have one table for time-independent aspects of each patient, like classroom and age, and another table for the time-dependent ""events"", like an onset of prodromes. So something like this:

Table 1:

	Patient_ID  Family_ID  Household_ID Classroom   Age  Sex  House_X  House_Y    Died
		   184         51             6         2  13.0  NaN    182.5    200.0   False
		   173          9            35         0   6.0    M    212.5    107.5   False
		   178         17             6         0   4.0    M    182.5    200.0   False
		   177         17             6         1   8.0    F    182.5    200.0   False
		   174          9            35         0   3.0    F    212.5    107.5   False
		   183          4             6         0   4.0    M    182.5    200.0   False
			45         48            51         1   7.0    M     75.0     55.0   False
		   175          9            35         0   2.0    M    212.5    107.5   False
		   181          4             6         2  10.0    F    182.5    200.0   False
		   182          4             6         2  13.0    M    182.5    200.0    True

Table 2:

	Patient_ID    Event_Date  Event_Type
		   184    1861-10-30    Prodrome
		   184    1861-11-06        Rash
		   173    1861-11-01    Prodrome
		   173    1861-11-03        Rash
		   178    1861-11-07    Prodrome
		   178    1861-11-11        Rash
		   177    1861-11-07    Prodrome
		   177    1861-11-07        Rash
		   174    1861-11-08    Prodrome
		   174    1861-11-08        Rash
		   183    1861-11-11    Prodrome
		   183    1861-11-15        Rash
			45    1861-11-11    Prodrome
			45    1861-11-13        Rash
		   175    1861-11-12    Prodrome
		   175    1861-11-15        Rash
		   181    1861-11-13    Prodrome
		   181    1861-11-17        Rash
		   182    1861-11-15    Prodrome
		   182    1861-11-18        Rash
		   182    1861-11-18       Death

These two tables are linked via the `Patient_ID` variable. So, is this the right way of doing it? Is it appropriate to have the redundant `Died` column in Table 1, since it's implicit in Table 2? The linked paper says

>During tidying, each type of observational unit should be stored in its own table. This is closely related to the idea of database normalisation, where **each fact is expressed in only one place**. If this is not done, it’s **possible for inconsistencies to occur.**

Does this count as having ""the same fact"" in multiple places? Also, we could potentially compute things like the duration between the onset of prodromes and the beginning of the rash for each patient. To my mind, this is not really an ""event"", but more a property of a patient, so it belongs in Table 1. But again this seems like a duplication of information that's already implicit in Table 2 so how should it be stored? Or am I just overthinking it?

What about the `House_X` and `House_Y` variables? They're really more properties of a household, rather than a patient, so should they get their own table that looks like this:

Table 3:

	Household_ID   House_X  House_Y
			   6     182.5    200.0
			  35     212.5    107.5
			  51      75.0     55.0
.. etc?

Some help from people more experienced would be appreciated here.  This ""tidy data"" thing seemed like a simple idea when I first read about it but it's proving to be quite confusing. I don't know much about databases, maybe that's why.","How do I make this dataset ""tidy""?",9jnw3r,new,6,1,1,0
"Hey all,  


Quick query, If using regression analysis and I'm getting an R squared value of 0.69 (69%), suggesting that 69% of the variability in the dependant variable is due to the independent variables.  


What is the remaining 31% thats causing variability? Is it error? An independent variable that I haven't accounted for in my model?

&#x200B;

Any help on this would be great! Thanks!",Regression and R squared query,9jlgdj,new,20,14,14,0
"I am an engineer in a department with many more engineers and no statisticians.  Please consider the question and reply within the context of smart people who haven’t taken a stats class in a long time.  

Some fellow design engineers are attempting to domonstrate that a new product is equivalent with an old one. For the sake of discussion let’s say the property of interest is tensile strength.  They set up a t test with a small sample size (test samples are very expensive) of about n=7 predicate device and n=7 new device.  Then they do the testing, p value ends up greater than .05 so they say there is no evidence that the two products are different.  This is obviously gaming the system as it is harder to show a difference for lower sample sizes. Also it seems obvious that the test should be set up such that the default / null hyp is *not* the preferred outcome so the burden is on you to show the difference. 

Question: what test should be selected to demonstrate the two products are different (or not). I expect it would include some definition of what parameter would define equivalence (mean, variation, etc), a definition of how different actually matters (a difference of .01 on a scale of 1000 probably doesnt matter) and a null hypothesis that they are *different* so you would have to prove they are the same instead of the null being your preferred outcome.

Can somebody walk a novice through this process please? Thanks in advance.

Edit: clarified the last paragraph",Hypothesis Testing for Determining Whether or Not a New Product is “Equivalent” to an Old One,9ji1h7,new,9,9,9,0
"I've read that Minitab is great for making a bunch of graphs (I need to use it for an intro stats course for my mechanical engineering curriculum), but I can write scripts to batch output graphs.

What is the target audience(s) of Minitab and why is it useful for them?",Why even use Minitab?,9jhyrm,new,34,7,7,0
"So, I was having a discussion with another statistician and I realized I am probably confused about random walks.

&#x200B;

We were hypothesizing that we had a large number of mathematics test questions with the difficulty of the items having a normal distribution.

  
We have 2 test takers, each receiving a random question. Once they complete the question they get a new random question. After 100 questions you can look at the average difficulty of all the questions both took and they are equivalent. I argued that both of them received equivalent difficulty of a test since the average difficulty of all the questions was the same especially since they were pulled from the same normal distribution of test questions. He mumbled something about random walks and equivalent. He mentioned something about random walks but I honestly know nothing about how they function. Is there some feature of giving these test takers random items that would make them receive different difficulty of tests?",Equivalent random walks,9jfowh,new,14,2,2,0
"Hi,

I applied the EM algorithm on some data and created models with 2-4 components. I need to compare those to a non-mixture model. NormalmixEM won't run with k=1.

Do any of you have some advise on

a) how to get the algorithm to run for a non-mixture model

b) how to compare mixture and non-mixture models

c) how to access the clustered data from the normalmixEM output as to analyse the estimated histograms and densities further?

&#x200B;

Any help is appreciated, thank you very much.

&#x200B;

Edit: I did some thinking and I might have found something for a).

You can't set k=1. Setting lambda=1 doesn't help either, although it should, since the other component's lambda would be set to zero then. You can, however, set arbmean=F. This way R fits both components through the same mean and gives one component consitently a lambda close to one, the other is always close to zero. I just need to check if that could be called effectively a non-mixture model.

&#x200B;

Edit 2: I thought some more.

The idea from Edit 1 doesn't work, since the estimated densities don't fit with the data at all. But I just realized that there is no way to solve a). The E step of the EM algorithm is essentially the calculation of probabilites using bayes. If I only have one component, there would be no complementing event. Meaning I would have to divide by zero. Duh. ",Model selection: mixture vs non-mixture model in R,9jecn6,new,2,1,1,0
"I'm working with data that is pretty close to binomial but not quite.  Not all observations are 0 or 1, there are VERY few that have 2, 3, even 4. I have quite a bit of data points and feel that I could essentially use a beta-binomial model to model my conversion rates even if the data isn't exactly binomial.  This is a table breakdown of what I'm talking about:

control

0      1          2          3            4 

.935  .060  .0045   .0003   0.0001

Is it reasonable to just default to a beta-binomial? What kind of issues could I face making this assumption? ",Bayesian Testing question regarding which conjugate prior to use,9jean4,new,5,1,1,0
Thanks in advance for the help!,What fit indices should I include when reporting a multigroup latent growth curve model?,9jdwre,new,0,1,1,0
"I want to perform lookups, etc for each alpha, and degree of freedom but don't know how to reverse engineer FDIST. Thanks!",How to create an F distribution statistical table in Excel?,9jds9s,new,6,1,1,0
"I posted this on StackExchange Cross Validated but haven't had luck finding an answer. I was hoping someone could check if I'm understanding the process correctly. I am aware Arima and auto.arima functions will automatically do this for me but I want to be sure I understand how it works. 

The steps/questions I have are as follows:

1. Say I take the difference on my data set and it is now stationary. Now would I use an ARMA model for the differenced data and obtain MLE estimates for the model parameters?
2. Now applying the ARMA formula with the estimated parameters, I would obtain forecasts on the differenced data for say, h=1
3. To obtain the next forecast on the original data, do I simply add the last value of the original data set to the ARMA forecast from the differenced data set?
4. And does this work similarly for SARIMA models?

Any help and clarification is appreciated, thank you!",ARIMA on already differenced data: How does it work to obtain forecasts for the original undifferenced data?,9jcqao,new,0,6,6,0
"Hi

I have a mean of hrs/day. I have converted this to mins/day (by multiplying by 60). However I am not sure what I should do with the SD or confidence intervals which refer to the variance of hrs/day. 

How would I derive the SD and confidence intervals for mins/day?

Any help would be appreciated ",The effect of changing units of the mean?,9jc0ew,new,5,1,1,0
"Hi everyone, I'm a pharmacy student in the process of writing a research paper, currently evaluating drug literature. I have a relatively limited background in stats, but I understand what common statistical values mean in drug research and how to apply their relevance. However, the article I'm currently looking at has basically duped their results to help fit their motive and I was hoping I could get some direction as to what they are presenting.  
  
The authors claim to have used many different statistical models, but in their results section the only values presented are paired t tests with degrees of freedom that differ from what should be expected. The results are based on comparing experimental drug with placebo. As far as I can tell, the authors didn't even directly compare the experimental results to placebo to show that it was superior.
  
An example of what I am talking about would be:  
Group 1: 59 subjects  
Group 2: 38 subjects  
  
Results (based on # of correctly recalled answers in test):  
Group 1: 10.86% increase, t56=4.40, p <.001
  
Group 2: 3.79% increase, t32=1.059, p=.298  
  
Effect size (Cohens d): 1.63
  
Those results are the only results presented for that outcome. Am I correct in thinking that:  
1. degrees of freedom are off for both groups (not sure what this discrepancy means though)  
2. there is no comparison between the two groups, only within the 2 groups  
3. there is no value to show the variance/error like a 95% CI
  
Also, I am struggling to understand how to interpret the p values since one of them was significant while the other wasn't.  Any help on this would be greatly appreciated. I can send link to article if that would help. Thanks!
  
",Question about drug literature results,9j9jzc,new,4,3,3,0
"If anyone has any examples of an application where that's not the case I'd like to see it

I suppose that in the absence of any specific knowledge it's because normal is the maximum entropy distribution for the assumed fixed variance parameter?",Why are random effects always assumed to be normal?,9j98xh,new,35,12,12,0
"I have to come up with a project using real work data and either:

Forecasting using ARIMA models

Forecasting using multiple regression

Predictive analytics with seasonal series

I know I want to use data on our National Parks but am unsure what type of things I could use these models for. Could I use to predict visitors to the park? That seems like a reasonable one. Which model would I use for that? Can anything think of anything else that might be more interesting?

Thanks!",Can you guys help me come up with a project proposal for my Predictive Analytics class?,9j7i1n,new,2,0,0,0
https://stats.stackexchange.com/questions/368815/does-the-normal-product-distribution-have-subgaussian-tail,Does the normal product distribution have subgaussian tail?,9j7g3m,new,7,2,2,0
"Hi all

&#x200B;

I'm working on a project and was thinking about the following:

\- I have an independent variable, X, that influences Y both directly and through M

\- The effect of X on M and Y is nonlinear - it's quadratic.

\- The effect of M on Y is linear.

\- from previous models, I know that V is an important moderator for the effect of M on Y and the direct effect of X on Y.

&#x200B;

I found Hayes' medcurve macro, which includes an option to include the nonlinear effects I described  (and I do indeed find these effects). My next step would be to include the moderator to this model too, but the macro doesn't have an option for this. 

&#x200B;

Do you have any experience with this kind of model, and are you aware of any paper/macro on this?

&#x200B;

Thanks a million!",Is there such a thing as nonlinear mediation with moderation?,9j4glq,new,0,1,1,0
"So i have a within subject design in my experiment with 2 conditions  and 10 factors in a single condition. I already know that the conditions are different since i already did an ANOVA but now i want to describe the quality of difference. The things i'm most interested in are parallelism and flatness and i already know, that i could do that with a profile analysis. But those t-tests are designed for between-subjects effects, is it legit to use them for a within-subject comparison? Anybody got other ideas? 

Btw. my measurements are drawn from a skewed normal distribution.",Testing for parallelism and flatness in a within subject design,9j4ame,new,0,3,3,0
"Hey guys! I'll cut right to the chase here. I am a business student, and in one of my classes (forecasting) we have to find data and run a regression with one dependent variable and around 7 independent variables. Does anyone have any ideas surrounding what variables would be interesting to compare (ie company sales vs foot traffic, stock price vs economy growth etc), and where I would be able to find data on this?

&#x200B;

Cheers",Looking for topics for data forecasting project,9j3x0q,new,2,1,1,0
"Hello can I ask you the  difference between these  two?  I  have a couple of  questions:

1. Can  gradient discend  be used  to ''smooth'' a  non-stationary time serie?  When  I say ''smoothing''  I mean the  best fit that can describe  the  data,  beside   the regression method

2) I would like  to know   when it' s   better   to use  one instead of oneother... I know  that  gradient  discend  used in supervised learning,  but  can it  be used also to  predict a  dependent variable of  time serie?

What  is the difference  between the  two?

Sorry I am  at the beginning",Gradient discend VS smoothing,9j2u56,new,9,0,0,0
"  

Below is a practice problem i'm working on. I feel like I am really overthinking it and that is is actually very simple but i'm hitting a mental block. I'm also not confident in the answers I do think i've worked out? 

a) 3.25

b) \[(1-3.25)\^2 \*.65\] + \[(1-3.25)\^2 \*.65\] +\[(1-3.25)\^2 \*.65\] + \[(1-3.25)\^2 \*.65\] + \[(1-3.25)\^2 \*.65\]

c) ???

d) P X > 3 (into binomial formula) 

e) P X (greater than or equal to 3) into binomial formula

f) P X (greater than or equal to 1) into binomial formula

g) ???

*1. Suppose a certain treatment has a known response rate of 65% amongst adults. Further, suppose we randomly select 5 new adult patients to receive treatment. You may assume that these patients are independent.*

a) How many patients would you expect to be responders?

b) What is the variance on the number of responders from the sample? 

*Calculate the following probabilities (c-f):*

c) The first three patients are responders and the last two patients are not responders. (Note: Exactly this sequence)

d) Any three patients are responders and two are not responders

e) Three or more patients are responders.

f) At least one patient responds to the treatment.

g) Give two examples of how the assumptions of the binomial distribution could be violated if this were a real life situation.",I feel like i'm overthinking this?,9j2mby,new,5,1,1,0
"Can I use the CAGR (note: the ""A"" stands for ""Annual"") formula, and simply insert quarters into my formula instead of years for # of periods?  So, my formula would be:

((Ending Value/Beginning Value)^(1/Number of Quarters))-1

If not, why?  Thanks!",Calculating Compound Growth Rate for a Quarterly Analysis,9j2lf0,new,1,1,1,0
"Working with the  package brms in for multilevel modeling,  I'm trying to model a longitudinal observational study of disease progression in individuals, with a small subset of individuals opting for a particular treatment.

What I would like to test is if the timing of the initiation of that treatment relative to the initial diagnosis of the disease is significant in the disease progression. I would like to incorporate those that have the disease without surgery into the model in order to reap the benefits of mixed effect modeling.

&#x200B;

What is the best approach here? I was thinking of doing some sort of nonlinear modeling or piecewise modeling but not sure how to approach either method.",Piecewise Linear Mixed Effect Models using brms - Possible?,9j2kgr,new,2,1,1,0
"Please point me in the right direction if this does not belong in r/statistics.

I have a task for my ""research method and science theory"" class that I don't know how to do. It is part of a groupwork that will hand in a portfolio and this is my task:

In a research project, you want to examine how increased physical activity affects mental health in pupils ages 12-16. Describe and explain the characteristics of a research design that is suitable for assessing whether physical exercise has an effect on mental health.

Would an experimental or quasi-experimental method work for this, or have I completely misunderstood something?

Edit: It might be of relevance to mention that my answer to the question should be around 300 to 500 words. ",Help with quantitative research method,9j2hga,new,5,1,1,0
"This may be too easy for you guys, but I want  to make sure it's okay. I run a test twice to ensure repeatibility of the results, but now I want to calculate the percent of differences due to repeatibility. How to do that? And what is the OK range?Say 3.5 and 4.5 are my results.

 I want to compare each of these with actual measurements. Do I calculate the percentage of differences the same way? ",Percent of difference between two samples,9j1g9d,new,4,1,1,0
"Hello! I am sorry if this isn't the right subreddit.

&#x200B;

 I am currently working on my Thesis on  forecasting methods for sales about different products

&#x200B;

&#x200B;

\-Simple exponential smoothing (Brown method): 

&#x200B;

  
 I use Solver to optimize alpha (minimising RMSE) either using GRG-non linear solving method or evolutionary and it reurns optimal value a=0. (This happens  in many product cases - I have multiple worksheets: one for each product)

&#x200B;

&#x200B;

\-Holt's-Winters method:

&#x200B;

I use Solver to optimize alpha,beta,gamma (minimising RMSE) and it returns some times a=0, b=0.45 (for example), gamma=1 or combinations where  α, β,or γ is zero or one , and the other one is a ''normal'' number. On other worksheets (products) the optimal values are okay.

&#x200B;

&#x200B;

Does anybody can help me with that? I know that α, β,γ  must be >=0 and <=1. But in practice, why sometimes the optimal values in my case are extreme values? Do the actual values (data for actual demand) play any role on that? Is there any way to fix it or should i keep it that way?

&#x200B;

&#x200B;

Thank you very much!!

&#x200B;

  


&#x200B;","Forecasting: Excel - Solver optimization (problem with smoothing,trend, seasonal constants)",9j1asu,new,10,3,3,0
"Besides the obvious, like applied math and data science",What majors are similar to statistics?,9j00nq,new,11,2,2,0
"Things like market/creative fatigue are easy, the shelf life of things is pretty easily identifiable in a simple line graph. My gap in understanding came after using trend lines to look at performance over time. With the products I was comparing I made the argument with existing results that new product performance over time (1-2 years) will be volatile for a few months as we “find our way”, but should stabilize after 3-4 months and continue with a minor stable decrease in performance of about 2-5% a year due to a number of factors like audience fatigue, competition changes, market changes, etc. So, we use trend lines to easily see what direction something is going over time. 

When something is going really well and is stable I’m always asked how much more I can spend while maintaining results. What I can’t figure out how to project that. Generally if something is going well then more budget is allocated so I likely have spend increases thru the year. Remaining addressable market/share of voice are basic indicators of saturation but they can change frequently and there are so many variables within those numbers. There’s bound to be a point where I’m either over-saturating the audience, a plateau where excess budget becomes wasteful and doesn’t produce results.  How do I predict or make an educated guess on the optimal budget for something?  

Excuse the rambling too, I have some deep seated math anxiety. I typically can come to the right answer but can’t “show my work”. ","I work in marketing and use a number of data sets to forecast results. A lot of my assumptions are based on past patterns and while my expertise guides me right, I don’t know how to truly project future changes.",9izekg,new,4,16,16,0
Can anyone help me understand the difference between a event-driven trial vs time-driven ? ,Journal club help,9ixiji,new,1,0,0,0
"Does anybody have the amount of 12-15 year old male and females who were sexually abused in 2017 (separate stats for male and female, and sources if possible)? It's not for homework or a project or anything. I'm just trying to see if my friend is right about something.",Sexual abuse statistics,9ixidf,new,2,0,0,0
Would you use Bonferroni with 5 tests or not?,Testing 5 hypotheses in an analysis. Family wise error rate is 22% if I set Pr(Type I error) at 5%. Use Bonferroni correction?,9iwcfu,new,1,0,0,0
"Hi guys,

I am currently enrolled in my first data analytics class at university.

The first assignment involves finding out whether 7 input variables alone can be used to determine quality of a wine and compare the newly calculated quality to the one assigned using sensory data. How much agreement exists between  the two. Are they similar, very similar or more or less similar?  

I am doing the assignment on Excel, however I'm not very good at statistics. I go to the data analysis tab and I'm stumped with determine the correct path of action to solve this problem of whether input variables are a good determining factor to consider when calculating quality. 

&#x200B;

Thanks,",Please point me in the right direction,9iw4gi,new,4,1,1,0
"So when you run a regression model, you produce an ANOVA table, they both have sums of squares, degrees of freedom, etc.  So how are they related? Is it that regression is a special case of ANOVA where you're testing if the coefficients of the regression model are different than 0? ",How are regression and ANOVA related?,9ivgf0,new,6,17,17,0
https://www.reddit.com/r/AskStatistics/comments/9iu103/really_weird_result_with_what_was_assumed_to_be/,"Looking at the distribution of my panel data, I find the mean + mean/variance is approximately 1 for every location. Why? (Xpost /r/askstatistics)",9iuwbw,new,0,3,3,0
,The Schoenfeld volume study - results do not support the conclusion [x-post from r/weightroom],9iuhnw,new,2,9,9,0
"I am fundamentally missing something and would appreciate some clarification. I am reading [this text](https://www.amazon.com/Statistical-Processing-Multidimensional-Information-Statistics/dp/1441972935/ref=sr_1_1?ie=UTF8&qid=1537898330&sr=8-1&keywords=statistical+image+processing) on Regularized Least Squares. This text has a problem where I have to find z, a set of unknown elements. z in this case is a column vector of five 2D points.

To compute z, I use the following equation (provided by the text):

z = (C^T R^-1 C + ƛL^T L)^-1 C^T R^-1 m

Where C is the transformation matrix, R is a simple 2x2 Identity Matrix, L is a 4x5 Contraint Matrix and m is a set of measurements. The text goes onto say:

m = Cz + v = [z2;z5] + v where v ~ I and m = [10;50].

Here's the thing this problem is set on a 2D plane, so z is a vector of 2D (x,y) points. However, the text does not give m, a set of measurements on the 2D plane, in the same type. Instead m is a column vector of [10;50]. This reduces z to a column vector as well.

The question then asks us to plot z. How can I plot z if I have only a column vector?

What am I missing here?",Interpolation with Regularized Linear Squares,9iugx3,new,1,1,1,0
"hey stats reddit, I'm currently looking for Statistical books to read! I'm taking a year off between undergrad and grad school and want to keep my mind fresh and in tune to statistical topics and such. I'm thinking of starting with a book about machine learning but I am also looking for other reads. Thanks!",Statistics Books,9itqup,new,4,1,1,0
"Ok so I’m working on a question and I can’t figure out how to solve it.

Two baseball teams, team A and team B, have competed 20 times in one season. The average score was 5.5-4.2 in favor of team A. Assuming the same players play in all 20 games, what is the probability that team B wins the next game?",Probability without a Data Set Question,9it28p,new,2,1,1,0
"I'm currently a stats graduate student taking a mandatory consulting class (our only clients are other departments at the university). In our last class, the professor (a 70ish-year-old former FDA consultant who only teaches this class) gave one student advice on how to select which subject gets which treatment effect (based on some confounding variables) as a way to help the test become significant. The client came in asking for, essentially, the sample size he/she would need for their study. 

I think this advice was given as a way to lower the amount of people needed for a significant result (other literature shows the new treatment method being more effective than traditional treatments), but the idea of non-random sampling doesn't really sit well with me. Is this is p-hacking (or some other misuse of statistics)? ",Concerns about being directed to p-hack,9it24r,new,6,12,12,0
"Here's the link [https://fivethirtyeight.com/features/how-shoddy-statistics-found-a-home-in-sports-research/](https://fivethirtyeight.com/features/how-shoddy-statistics-found-a-home-in-sports-research/)

And a choice quote: 

> In doing so, (MBI) often finds effects where traditional statistical methods don’t. Hopkins views this as a benefit because it means that more studies turn up positive findings worth publishing. 

&#x200B;","Thought you might enjoy this article on the worst statistical test around, Magnitude-Based Inference (MBI)",9irh8x,new,13,71,71,0
Which is better career option for career beginners Masters in Data Science or Masters in Statistics?,Masters in Data Science or Masters in Statistics?,9iqayu,new,6,3,3,0
"I'm conducting a simulation of the evolution of a population and calculating values for heritability and confidence intervals for a type's frequency (the number of individuals of a type for each generation) as the program is running. The program runs for an x quantity of generations for each of a y quantity of times (maybe x=200 and y=5000), but the result is different each time because a random process is modeled.

Heritability=covariance of parent values and offspring values divided by variance of parent values

I believe that I am taking a sample because it is a random process, yet covariance and variance are defined in terms of expectations (in which the result is divided over all N pairs of values). So, is the denominator of the covariance N or N-1 and is the denominator of the variance N or N-1?

Also, for the confidence intervals, the mean of the confidence interval is the average frequency during a particular generation i over the y times the program ran. (With x generations, I would have x confidence intervals, each taken across the y times.) Confidence intervals use a value for standard deviation. Am I correct that I should be using the denominator N-1 for the standard deviation?  

Thank you very much for your input. ",How many degrees of freedom for a simulation?,9iq4v2,new,0,2,2,0
"Say you want to analyze the effect on sunlight, amount of fertilizer, amount of water, and so forth on the grow of crops.

You conduct your experiment by separating corns into 10 groups, each group has 100 baby corn stalks.

Each of the 10 groups will be subject to varying amount of sunlight, fertilizer, water, and so on. After x number of days, you count how many stalks in each group has reached a certain height.

So group 1 might have 55 stalks above a certain height, group 2 might have 80...

Is it appropriate to run a regression model with the response variable being the number of stalks in each group above a certain height, and the predictors being amount of sunlight, water, and so on?

If not, what's the best way to determine the significant variable among water, sunlight, and so forth. In addition, I also want to find the optimum combination of water, sunlight, and so forth. 

Thanks",What's the right statistical tool/method to find significant variables for an multi-group experiment?,9io9rg,new,3,1,1,0
"I'm working on a little video game project that involves a slot machine minigame. I want to skew the rewards a bit in favor of the player. It's a very basic setup:

- There are three symbols, a Lemon, a Cherry, and an Apple.
- Each one has an equal chance of showing up. (1/3)
- There are three ""slots"".

I'm extremely bad at even basic math, but I *THINK* that the chances of getting three of any fruit should be approximately 1 in 3.

Am I correct?

Edit: Decided to just brute force it in MS Paint drawing little colored circles.  
There's only 3 possible win combinations across 27 total combinations. I don't belong anywhere near a casino.

Edit 2: I decided to make the first and second slot roll the same fruit every time, and add a fourth ""dummy"" symbol to make the chances of winning 1 in 4, but the prize will quintuple your bet, so the players will get a slow crawl of income by playing.",Very basic question from an absolute math imbecile.,9invdi,new,12,6,6,0
"And if not can it be approximated? Sorry if this is a brutal question, I'm a grad student and have a weak stats background.

&#x200B;

EDIT: [https://www.bmj.com/content/343/bmj.d2090](https://www.bmj.com/content/343/bmj.d2090)

Cant make too much sense of this, but looks like you can approximate it.","Given a sample size, odds ratio, and p value is it possible to calculate a confidence interval?",9imild,new,7,2,2,0
"My adviser (and many other professors as well as people on the Internet) said this is the most important class in the Stats curriculum. I really want to get the most out of this class, and come out with a grade that shows such correct effort.

As a side note, my midterm is on Thursday and I'm looking for last-minute (or rather, days) advice for acing the midterm. The book I use is *A First Course in Probability* by Ross. The material tested will be on Combinatorial Analysis, Axioms of Probability, Conditional Probability and Independence, and Random Variables.","To those who got an A in Probability Theory, how did you do it?",9im13a,new,32,45,45,0
"Just got a really crappy grade on my first exam, and I'd like to see how I sit compared to everyone else. For each section of the test we can see the class Average, Median, total points in the section and how many points we scored. The class size is about 35. Is there a way to use proportions/ratios to see where my score sits? ex. Sec 2: 15/30, AVG=9.44, Med=7.5. ex2: Sec 3: 10/30, AVG 2.22, Median 0.

I figured if the median is zero I can assume the bottom 50% of students all scored a 0 in that section, I dont remember enough stats to further analyze. Thanks!","Can you use median, average, group size, and score to calculate what percentile I am in?",9ilf7a,new,0,0,0,0
"The problem I am doing is as follows,   


A message is to be transmitted as a binary code of 0's and 1's. Past experience with the equipment being used suggests that is a ""0"" is sent, it will be correctly received 90% of the time (and mistakenly decoded as a ""1"" 10% of the time). If a ""1"" is sent, it will be correctly decoded 95% of the time. The test being sent is 60% 1's and 40% 0's.   


Suppose the next signal is received as a 0. What is the probability that it was sent as a 1?",Homework help please!,9ik9tp,new,2,0,0,0
"I'm in Stat 151 and not very good with computers and such. So statcrunch is a bit confusing.
Are there videos that break it down to simple terms?
Thanks!",Nees general help with Statcrunch,9iiv9d,new,0,0,0,0
"Using a process *X*, I have trained a neural network from scracth 30 times, each time recording the performance of the trained neural network. Using a different process *X'*, I have trained the same neural network once.  

Given that *X* and *X'* are non-deterministic and the performance of the neural network trained by *X'* is greater than any of the neural networks trained by *X*,

Can I use a one sample T-test to show statistical significance?
i.e process *X'* is likely to be better than *X*. 

I cannot repeat *X'* due to time restrictions. 

I am hesitant to use a one sample T-test as it appears to test against a known population, yet I have a sample. 

Thanks. 

",Question regarding one sample T-tests,9igtex,new,1,0,0,0
"I'm finishing my phd next month (computational engineering science ~ math+cs+aerospace engineering) and have a couple of months off in which I'd like to properly learn probability theory and statistics from the ground up. My objective afterwards is to move towards statistical turbulence modeling, uncertainty quantification (solving stochastic systems of PDEs, optimization problems involving those), and _maybe_ at some point machine learning. 

So I'm trying to build a curriculum for myself, but it's hard to do so when you don't know what you don't know or what you need to know and how to get there so I wanted to ask for advice: books, online courses, and how to proceed from each level to the next. 

Also would you recommend online courses at all? I was looking at some of the courses in edx, coursera, and udacity but under the keywords ""probability"" / ""statistics"" the courses I found felt a bit superficial. It's hard to find out from their descriptions alone how deep they go. ",rigorous program for learning statistics and probability,9ig7ro,new,2,4,4,0
"Morning everyone!

I'm slightly confused at this point, I think I get the gist of MCMC, but I can't see how it really bypasses the normalizing constant? This makes me not understand how we approximate the posterior using mcmc. I've read through a good chunk of kruschke's chapter on MCMC, read a few articles and watched a few lectures. But they seem to glance over this.

I understand the concept of the random walk and that we generate random values and move to this value if the probability is higher than our current value, and if not, the move is determined in a probabilistic way.

I just can't seem to figure out how this allows us to bypass the normalizing constant. I feel like I've completely missed something, while reading.

Any additional resources or explanations, will really, really be appreciated. Thank you in advance!

EDIT: Thank you to everyone for there responses (I wasn't expecting this big of a response), they were invaluable. I'm off to study up some more MCMC and maybe code a few in R. :) thank you again!",MCMC in bayesian inference,9iedaz,new,22,24,24,0
"Has anyone been able to get EasyFit tow work with a new version of Excel? I used to use it with 2010, but it does not seem to work with newer versions.",Easyfit software of excel 2013/2016?,9ie6tf,new,0,1,1,0
"Hi, I'm a student in a Master's of Health Informatics program. One of my requirements is to take biostatistics, which I'm doing now. The problem is that I have zero statistics background beyond like, how to calculate the mean, and I'm finding this class extremely difficult. 

The terms I can sort of grasp sometimes, but then in addition we have to use SAS and I barely even know what SAS is, much less how to use it. I managed to install and use it the first week because they gave out the code, but going forward the class seems to require that you already know it. The SAS problem set due today had me in rivers of tears.

So my question is: Does anyone know of a good intro resource to SAS and/or general statistics? I like videos and watched some Udemy lectures today. They were somewhat helpful but still missed the mark sometimes - I think people forget what it's like to truly know nothing about this subject and they'll rattle off a new term and once again I'm lost.

I consider myself smart and I hate not being able to understand something. Thanks for reading.",Graduate level statistics help,9idx4u,new,16,0,0,0
"Background: BME major trying to get into machine learning. I want a general probability/stats foundation, since it's so useful in research/academia. Ultimately interested in Computational Neuroscience. I have some intermediate matlab experience and have scratched the surface with Python after starting (since finished) Andrew Ng's ML Coursera. 

I'm a little constrained for time, which is why the Intro material is kinda compressed, but I tried to give as much time as possible to ESL.


Week|Material
:--|:--
1-4| Khan Academy Probability
 ""   | Udacity Descriptive & Inferential Statistics
  ""  | Statistics (by Freedman)
  ""  | Coursera Predictive Analytics
   ""  | Introduction to Statistical Learning (with online Lagunita R exercises)
5| Statistics (Freedman)
 ""| Coursera Predictive Analytics
 ""| Introduction to Statistical Learning (with online Lagunita R exercises)
6-14| Elements of Statistical Learning
      ""|  Analysis of Neural Data (Kass)
""| Bayesian Data Analysis (Gelman)

      
I'm planning on doing a shit-ton of basic probability stats problems during the first 4 weeks in addition to the content review. Is there anything I'm missing?


Thanks a bunch for the advice!


edit: can't find many books on causal inference so any recs regarding that would be nice",Feedback on my Study Plan?,9idv2j,new,5,2,2,0
"Hi guys! In R, if center=F and scale=T then how does this affect your mean and standard deviation?",question about R,9idur5,new,9,0,0,0
"I am pretty sure that I need to use the combinations rule to solve this problem:

>Winning the jackpot requires that you select the correct five different numbers between 1 and 75 and, in a separate drawing, you must also select the correct single number between 1 and 15. Find the probability of winning the jackpot.

This is my work:

(75!) / (75-5)!5! \* 15

I am putting this info into my TI-84 Plus Silver Edition calculator as ((75!)/(((75-5)!)(5!)) and then I was to multiply it by 15 afterwards. However, once I click ENTER, my calculator says ""ERR: OVERFLOW 1:Quit 2:Goto"". Am I doing something wrong? Is it my calculator? If it is, is there a way to fix it?

The answer to the problem is 1 / 258,890,850.",Combinations Rule Help,9idu0b,new,4,1,1,0
"Hi all - new here

I seemed to be stumped by a simple stats question. Am I overthinking? 

Setup:

There are 3 groups (A, B, C). The disease in question is of little concern in those in group A, more concerning in B, and scary in group C. We are looking at findings from a scan they received. This finding has 3 levels. Positive scan, negative scan, equal scan.

Question:

How likely is group A to have a positive/equal/negative scan? Group B? Group C? 

My Answer:

I think these are just simple fractions?... e.g 39/100 had a + scan, 41/100 had a - scan, rest had equal scan. Is there a test I can perform (I use R for my analysis), to better answer this ""likelihood"" question?

I am around if anyone has questions, or if I was not clear enough. Happy to clarify. ",Simple stats question on multiple categorical groups,9idowl,new,1,1,1,0
"Hello, I am wondering in what conditions do cluster sampling work well and in what conditions does it not especially in marine and aquatic sciences. Thank you so much. ",Cluster Sampling,9idk6x,new,1,0,0,0
"I have data on a certain type of races. Participants start from a specific track (1-8) and finish in a specific place (1-8). Given the betting before the race each participant can be ranked according to their odds of winning (1-8).

&#x200B;

I have already have the  distribution of winners per track, I want to compare this with winners  per track **given that a specific track has the lowest odds** (ie is the favorite)

&#x200B;

What statistic test is appropriate?",Test of difference between non-normal distributions?,9idhha,new,4,1,1,0
Hi guys - would there ever be a situation where your z-score calculation would be the same as your raw data?,Z score question,9iddc3,new,9,2,2,0
"Could the following options in an online survey be considered as a Likert Scale?

Frequently

Sometimes

Rarely

Never

Not applicable",Likert scale question,9ic6q1,new,10,1,1,0
"I was tasked with doing an A/B test, which I haven't done before. My control and test sample were quite small (\~1600 and \~1000), and had very few unique values / variables. (Only three variables {a,b,c}, and the number of unique values per variables is about {50,12,7} for both groups.) The goal is to see if the test group's {a} is, on average, higher than the control group's {a}. To check if this is the case, I've designed the following two-part test:

1. Transform the test data into M**Ω** , where M is the matrix made up of the test data's {a,b,c}, and **Ω** is the matrix made up of eigenvectors of the c-cov matrix of M. (Aka the matrix that diagonalizes the v-cov matrix).
2. Compute the numerical pdf of M**Ω** 's columns
3. Simulate columns of size 100 000 and bind them to form a matrix N
4. Obtain Test\_hat = N**Ω**\^(-1), and take it's {a} column. Label it Test\_a\_hat
5. Repeat the above steps for the control data, but thrice. Call the output Control\_a\_hat\_1, and Control\_a\_hat\_2, and Control\_a\_hat\_3

(For anyone wondering why I used the transform matrix **Ω**, I did it to cheaply circumvent the issue of drawing random vectors from a multivariate distribution with a non-diagonal v-cov matrix **Σ**.)

&#x200B;

At his point, I have 4 sample of sizes 100000 that have virtually the same mean vector **µ**  and v-cov matrix  **Σ** as their respective original distributions {test, control, control, control}. However, there are values being produced that are nonsensical (i.e.: ""a"" is related to transactions, and you can't make 1.3 or -0.2 transactions. However, the artificially generated samples can contain such nonsensical values.) This is issue #1.

&#x200B;

Issue #2 is that the entropy goes up. (Obviously, generating random linear combinations of {a,b,c} increases randomness.)

&#x200B;

Next, I perform the following test:

1. Compute (Test\_a\_hat - Control\_a\_hat\_1), and obtain P\[Test\_a\_hat\_j - Control\_a\_hat\_1\_j > 0\] = p\_1
2. Compute (Control\_a\_hat\_2 - Control\_a\_hat\_3), and obtain P\[Control\_a\_hat\_2\_j - Control\_a\_hat\_3\_j > 0\] = p\_2
3. Assume a Normal distribution with **µ** = p\_2,  **σ**\^2 = p\_2 (1 - p\_2) / 100000
4. Do a simple p-value test with my hypothesis being p\_1 = p\_2 with respect to the distribution mentioned above

Alternatively, I could simply draw at random from the samples in lieu of part #1, but I feel like it's a bit too simplistic in nature. I would be limited to the a posteriori observed values, rather than being bound to their fundamental structure {**µ, Σ**}, allowing the existence of new coordinates {a,b,c} to be generated.

&#x200B;

If anyone well-versed in A/B testing could point me towards the right direction and / or criticise / validate my approach?

&#x200B;

&#x200B;",Validity of an A/B test approach,9ia9py,new,5,1,1,0
"Looking for a good intro book to do a bunch of practice problems and get a solid foundation for basic, intro statistics. 


I saw that ""Discovering Statistics Using R"" was previously recommended by this sub, but is it good for learning to use R+some stats, or will it give me the solid foundation I'm looking for? 


I found ""Statistics and Data Analysis: An Introduction"" from CMU's intro stats course syllabus. Unfortunately, I can't find it online so if it happens to be the better book I'd appreciate recommendations for substitutes.


Thanks!",Discovering Statistics Using R vs Statistics and Data Analysis: An Introduction?,9i9vm1,new,4,6,6,0
"Hi everyone.  There was a study that was published in the field of exercise science this year that has caused a fair amount of controversy.  I wrote up a critique (link below), and I’d like to get feedback from those more knowledgeable and experienced than I am about whether I interpreted the statistical results appropriately.  Always good to have experts review your work, thus, posting here to request feedback from you all.  I’m especially interested in feedback about the Bayes Factors, but if there’s something else you think I should update my thinking on, please point that out.

The paper is:
Schoenfeld, BJ., et al., Resistance Training Volume Enhances Muscle Hypertrophy, Medicine & Science in Sports & Exercise (2018)
https://www.ncbi.nlm.nih.gov/pubmed/30153194

The paper reported multiple kinds of measurements (strength, endurance, hypertrophy), but I’m only looking at the hypertrophy-specific measurements.

The link to the full analysis is here, in case you want to read the whole (long) thing:
https://www.reddit.com/r/AdvancedFitness/comments/9ina5h/the_schoenfeld_volume_study_results_do_not/

# BACKGROUND #
Three groups of college age males (45 total, 34 after dropouts) executed either a low, medium, or high volume resistance training workout for 8 weeks.  LOW was 1 set per exercise, 3 days per week.  MED was 3 sets/exercise.  HIGH was 5 sets/exercise.  Measurements were taken for biceps, triceps, and two leg measurements.  Because more than one exercise would count towards a muscle’s measurement, the number of sets per week for each metric was:

Low/medium/high sets per week  
6/18/30 for Biceps  
6/18/30 for Triceps  
9/27/45 Leg1 (rectus femoris)  
9/27/45 Leg2 (vastus lateralus)  

So, for example, the medium-volume group did 27 sets per week relevant to the Leg1 metric.

In their conclusion, the authors claim that “Alternatively, muscle hypertrophy follows a dose-response relationship, with increasingly greater gains achieved with higher training volumes.”

# EVALUATION #
Here’s a summary of my logic:  
To support the claim of “increasingly greater gains achieved with higher training volumes” as opposed to, for example, finding that there was a plateau and HIGH was no better than MED, the results in the paper needed to show that the HIGH group was superior to the MED group with respect to muscle thickness growth.  They would also have needed to show HIGH>LOW and MED>LOW, but I’m going to focus on HIGH>MED.
(well, you get HIGH>LOW for free if you show the other two, obviously)

Toward the paper’s claim, two different statistical approaches were presented:  a) a Frequentist (aka classical) approach, and b) a Bayesian approach.  While both statistical approaches (using ANCOVA) were executed correctly, neither approach provided results sufficient to support the stated conclusion.

The frequentist approach didn’t show anything statistically significant for the pairwise comparisons between MED and HIGH for any of the four metrics.

The Bayesian approach produced a BF in favor of no difference for the triceps ANCOVA (1.51), a BF in favor of no difference for the biceps pairwise comparison (1.67), and BFs in favor of a difference found (HIGH over MED) of 2.34, and 2.25 for the two leg measurements.  Since the two “HIGH>MED” results were so low, being described as “weak” or “not worth more than a bare mention” in Jeffreys’ description, it is my evaluation that the authors made a strong conclusion based on insufficient evidence in the statistical results.

As far as I can tell from my reading, nobody would think that Bayes Factors below 3 would count for any sort of reasonable evidence.  I mean, here’s what Wagenmakers’ says using his Poke-a-Pizza example:

>Another way is to transform the Bayes factor to a probability, assuming H0 and H1 are equally probable a priori. For instance, H1 was at 50% before seeing the data; after seeing the data, that probability has increased to 2.2 / 3.2 = 69%, leaving a substantial 31% for H0. But even these percentages may not drive home, in an intuitive, visceral way, how little evidence this really is. In order to help people interpret the percentages, JASP presents a “pizza plot” on top of the graph.”
https://www.bayesianspectacles.org/lets-poke-a-pizza-a-new-cartoon-to-explain-the-strength-of-evidence-in-a-bayes-factor/  

The only way I could see getting to a reasonable amount of evidence would be if the metrics were independent and you multiplied them.  However, since the four metrics used the same subjects, they are not independent, and I don’t know if it would even be possible to correct for that. (Is there?)  

All feedback is welcome.  Thanks.  
(edit: change the link to a revised version of the post in advancedfitness and fixed my previous error, which can still be found at the bottom of the r/weightroom post)
",Request for feedback on my analysis of Bayes Factor usage in Schoenfeld (2018) paper,9i9k09,new,0,1,1,0
,My Interview with Kenneth Cukier on the rise of Big Data,9i8d52,new,0,1,1,0
"Hello!   
I am a community college student looking to major in statistics and transfer with a 3.4 CSU gpa. My top choices are San Jose State, San Francisco State, and Cal State East Bay. Sometimes I think about Cal Poly SLO but I won’t finish their lower division requirements by the time I apply for transfer, and my GPA isn't high enough). I prefer to major in Statistics, not have a concentration in it.  All I want is to choose the best school for Statistics and hope to not overlook an underrated one. SJSU is always a highly ranked CSU; SFSU and CSUEB aren’t, which is a bummer since they have statistics as a major. I feel like CSUEB is underrated as hell. They have a Bachelor’s in Statistics w/Data Science concentration, and a Masters in Biostatistics. I will attach links to each of their websites with a breakdown of their curriculum and faculty. If there are any statisticians/future statisticians out there, please help me by looking at the curriculum, faculty, etc.. and let me know your opinion of them and any other suggestions you have. I would really appreciate it!

CSUEB - [Statistics w/Data Science Concentration](http://catalog.csueastbay.edu/preview_program.php?catoid=19&poid=7727),---- [CSUEB Faculty](http://www.csueastbay.edu/statistics/people.html) 

SJSU - [Applied Math w/Stat Concentration](http://info.sjsu.edu/web-dbgen/catalog/departments/MATH-section-5.html), ---- [SJSU Faculty](http://info.sjsu.edu/web-dbgen/catalog/departments/MATH.html)

SFSU - [Statistics w/Emphasis in Business/Economics/Science](http://bulletin.sfsu.edu/colleges/science-engineering/mathematics/bs-statistics/#degreerequirementstext) \---- [SFSU Faculty](http://bulletin.sfsu.edu/colleges/science-engineering/mathematics/#facultytext) 

&#x200B;",Please help me choose the best California State University for Statistics,9i6wxo,new,8,7,7,0
"I'm going to do my best to explain what I'm trying to do: 

I have model predictions for county-level fertilizer sales from 1945-2012, published by a group that didn't include an estimate of uncertainty of the predictions.  I have the actual state-level data from 1997-2011, which I can compare to the predicted values (predicted state values = sum of all county values), and I would like to estimate the uncertainty (preferably standard deviation) of the modeled predictions. I only have actual data at the state-level, but want to estimate the uncertainty for county-level predictions based on the uncertainty between the modeled and observed state totals (as a best-guess). 

I feel like I should be able to do this somehow, using the difference between the predicted and observed values?

I've tried googling this, but I don't think I'm using the correct terms. The best I've come up with is using the standard error of the estimate, but that is the absolute error of the state-scale values and the county-level values are \~60x lower. I could weight this by the average to get a CV, but which average? The average of my predicted or observed values? Or is this the wrong way all together? Please help! ",how to estimate uncertainty/standard deviation of model estimate?,9i5whf,new,8,4,4,0
"I was thinking about this question, and realized that it's probably a pretty interesting statistics problem.  Intuitively I feel like the likelihood is pretty high, maybe even approaching 100%....","What are the odds that there is someone on earth that was born on the same day you were, and will also die on the same day?",9i5qhf,new,32,32,32,0
"Suppose I want to measure the effect of tornadoes on voter turnout at the county level. I suspect counties that suffer tornadoes will have lower turnout in the next election, but that the effect will be stronger the closer the tornado is to Election Day.

I want to estimate the effect of the ""treatment"" (tornadoes) on turnout, but I also want to estimate whether the recency of the treatment influences the size of the effect. I could use a linear regression with dummy variables for specific time periods (less than 1 month before Election Day, less than 2 months before Election Day, etc.), but these seem arbitrary and blunt. Is there a model that could incorporate not just the presence of the treatment but its recency as well?",How do I estimate a treatment effect when not all units are treated at the same time?,9i4r3i,new,4,2,2,0
"https://imgur.com/a/cuAXi4e

Hi, I don't really understand the proof of this theorem, why is teh probability flipped after substituting X_n ?
Thanks!",Stat Theorem,9i2y8t,new,6,2,2,0
"Hello,

I have series of highly scattered sensor data, I want to find the least computational intensive way, to bring them closer to a selected value within the range ( i.e. mode), without affecting their order. 

like this : [https://imgur.com/6LQfsVT](https://imgur.com/6LQfsVT)

Can someone point me to the right direction???

thank you so much",[Question] How do you bring scattered data close to a selected value while preserving order?,9i1c7v,new,4,1,1,0
"Recently a teacher gave me this [N=18 question multiple choice assessment](https://www.thevantagepoint.ca/sites/default/files/High%20Performing%20Teams%20-%20Self%20assessment%20handout.pdf)  


In short, each question has 4 possible responses that lead to a ""type,"" of person.   
When I took the assessment, my scores were split 5,4,5,4 which to me indicates, that my test was inconclusive.   
However, it brought up some questions for me that I couldn't answer.  


1) What formulas do I need to use to determine how many responses for a type I would need to have a conclusive result? 

&#x200B;

2)  What would be the cutoff for significance between what I scored and an 18,0,0,0 ?

&#x200B;

3)  What would a score like 6,6,0,6 indicate? Would you be able to say with any degree of certainty that the person is not type 3?",How to determine if the results of a multiple choice test is significant,9i0oed,new,4,1,1,0
"Hi. So I'm a university student who has taken a few stats classes. I want to go to grad school for statistics (or epidemiology, I'm not sure), but I still feel like I only know the basics.

Anyway, I'm working in a microbiology lab, and my professor knows that I like stats and is having me do the analysis on a paper. I don't want to eff it up.

The data:

There are different bacteria species and combinations of bacteria - seven total. Each ""environment"" has five treatments plus a control with no treatment. Each treatment was done three times on the bacteria. There were a few things measured, but as an example, one thing that was measured was optical density.

Tests:

For each bacteria environment data, I first tested normality and homoscedasticity on the residuals - I'll put the R code, but it was a function that did Shapiro-Wilk, Kolmogorov-Smirnov, Cramer-von Mises, and Anderson-Darling for normality, and then I just did an F test for homosc. If any one of the normality tests showed a p-value>.10, I did a qq plot to judge for myself. Same for the F test, but a graph of residual vs fitted values. 

If the data was normal and homosc., I ran an ANOVA with post hoc Dunnett's test because I was only interested in the comparison of treatments vs. control.

If it wasn't normal and homosc., I did a Kruskal-Wallis test with post hoc Dunn's test. I couldn't find a non-parametric equivalent of Dunnett's. 

So definitely correct me if any of that is wrong, but I'm mostly concerned about what I considered significant. My professor told me that it was common in the field to accept unadjusted p-values, so even though Dunn's test gave adjusted and unadjusted, I accepted all significant unadjusted values. I know my professor told me to do this, but she's told me she doesn't know stats that well, and I just feel weird about it. Plus, I know that if I did a Tukey HSD post hoc, it would give adjusted p-values, but I'm not sure if it does that for Dunnett's test. 

Let me know what you think. Thanks.","My analysis will be in a paper, and I want to make sure I'm not doing anything blatantly wrong.",9i0m29,new,32,28,28,0
https://highlyimplosible.com/bypassing-convolutions-an-application-of-fourier-tranforms/,"Bypassing Convolutions, An Application Of Fourier Transforms.",9i07c8,new,4,2,2,0
Can anyone help me solve the sample size? The research we're working on is about the effects of social media on the mental performance of 1st year students in our school. We're gonna be gathering data from 3 sections of 1st year students in our school and I'd like to know how to get the estimation of the sample size. Help me please . I don't know where to start. I figured out the part where I use a 95% confidence but after that I'm kinda lost on how to get the population standard deviations as the examples I read in the internet usually talk about scores of people. I don't know how to measure ours. Additionally I also don't know how to get the margin of error of people.,Help me do sample estimation in our study please,9hy129,new,7,7,7,0
,Why do more independent variables become significant when sample size is increased?,9hvt3c,new,7,0,0,0
"I have this dataset of patients who underwent a primary procedure from 2005 - 2014, along with the patients who required a revision procedure. 
                
               Primary   Revision
    2005    10,897    2902
    2006    11083     3459
    2007    12608     3435
    2008    13688     3633
    2009    14261     3987
    2010    14770     3232
    2011    15279     3608
    2012    16350     4074
    2013    16550     4561
    2014    15588     4606
    total     141,074  37,497

When I input the primary and revision data to do a pearson correlation coefficient, I received an R score of 0.7085 with a p-value of 0.021. I was wondering if the proper wording would be ""Incidence of primary and revision procedures increased through the study period (R = 0.7085, p = 0.021).""

",Is this done correctly?,9hvpd0,new,8,0,0,0
"[Saw this article the other day.](https://www.quantamagazine.org/statistician-proves-gaussian-correlation-inequality-20170328/) It says this:

>Royen hadn’t given the Gaussian correlation inequality much thought before the “raw idea” for how to prove it came to him over the bathroom sink. Formerly an employee of a pharmaceutical company, he had moved on to a small technical university in Bingen, Germany, in 1985 in order to have more time to improve the **statistical formulas that he and other industry statisticians used to make sense of drug-trial data.** In July 2014, still at work on his formulas as a 67-year-old retiree, Royen found that the GCI could be extended into a statement about statistical distributions he had long specialized in. On the morning of the 17th, he saw how to calculate a key derivative for this extended GCI that unlocked the proof. “The evening of this day, my first draft of the proof was written,” he said.

In order to have proven such an elusive theorem, I imagine the statistical complexity of the models he dealt with was not trivial. Most of the analyses I've seen for clinical trials seem fairly simple, so I'm thinking that ""drug-trial data"" might be referring to something else.

I tried looking books about pharmaceutical data and statistics but they all seem to be basic stuff geared toward pharmacy students rather than statisticians.",Does anyone know of a textbook about advanced statistical treatment of pharmaceutical data?,9hvcjl,new,1,0,0,0
 **What are the consequences of heteroscedasticity and multicollinearity in regression? What are possible remedies?** ,Heteroscedasticity and Multicollinearity in Regression,9hutfs,new,5,0,0,0
"I was asked by my supervisor to take both [Basic Statistics](https://www.coursera.org/learn/basic-statistics) and [Inferential Statistics](https://www.coursera.org/learn/inferential-statistics) before I start my master's degree in Political Science. There is a lot of dropping of technical terms (I have zero background on statistics) and I end up feeling overwhelmed and confused by the content (especially now that I am stuck in probability variables and have no clue about the R Lab homework.

I do not have time to study the alternatives in detail so I'd deeply appreciate it if anyone can recommend some better courses (which must be offered by a reputable academic institution)

Many thanks in advance!!!!!!!!!!",Can anyone recommend a good equivalent of this Basic Statistics course offered by Uni of Amsterdam?,9hu3n6,new,4,0,0,0
"I remember that the following property is named for someone, I think as a ""paradox"", but maybe differently. But I don't remember what that name was. The property I am talking about is, that in general

1/N sum_i (f(x_i)) != f ( x_mean)

where x_mean = 1/N sum_i(x_i)

See https://imgur.com/a/3UpmeUC for rendered equation.","What is the name of this ""paradox""?",9htkzs,new,7,1,1,0
"I'm working on an academic project which requires me to rule out or minimize false negative readings in rain gauges. Unfortunately, I know very little about statistics outside of what was taught in high school and I've only just begun learning about statistics taught at undergraduate level (through 'Discovering Statistics using R'). Though I love that book, I'm also time-bound (\~4 months left). I'm not sure if I should continue with that book since I don't even know if I'm on the right track.   
Could someone please tell me what kind of statistics I should be focusing on? Bayesian? Frequentist? Both? From what I understand, we're trying to rule out false negatives in automated rain gauges due to reasons like clogging. Would really appreciate any advice on where to begin.  
",Need guidance: Want to learn about false positive/negative tests. Where do I begin?,9hsgs8,new,7,1,1,0
"Just wondering how well you actually need to be able to know measure theory to get use out of it (In relation to statistics, maybe even stochastic calculus, stochastic differential equations), etc.

So I'm currently taking real analysis, and the exercises are pretty hard. At this point it feels like I'm understanding the theory, but it's difficult to create the theory (via doing exercises) if that makes sense. I believe I'll be in a similar boat in measure theory. Let me frame that another way. I will be able to understand measure theory, but I won't be able to ""create"" measure theory. I will be able to read statistics papers with measure theory, but I won't be able to create those papers? I can pass oral exams in measure theory, but I can't pass complicated written exams.... etc. So I'm losing out on one side of the course which is the part of doing exercises on your own, but is that really so important in relation to statistics etc? Even just having been through measure theory will give me a leg up on the competition? no? I guess I'll become more mathematically aware? Have a bit of a clue about what is going on when reading advanced papers/books? idk?",Question about Measure Theory,9hqrbe,new,6,17,17,0
"Hi,

I've decided to start posting weekly probability puzzles on [my statistics community](https://www.blackswans.io) and thought I'd share them on r/statistics for those who are interested. 

Here is the first puzzle: [https://blackswans.io/post/29/](https://blackswans.io/post/29/).

Enjoy!",Black Swans Puzzle #1,9hqb57,new,8,0,0,0
"Hi guys, I am doing a Bayesian multiple regression with 4 metric predictors and 1 nominal predictor. The nominal value can either be 0 or 1 for the type of thing it is, where 1 will add 150,000 to the predicted value (very strong prior).

&#x200B;

Per my lecturer's advice I am standardising the the values of the predictors and beta parameters. I've gotten by fine with the metric parameters using this formula so I can set the priors for the coefficients:

zbeta\_i = beta\_i / (SD\_y/SD\_x)

Where y is the predicted variable and x is the predictor variable, beta is the coefficient.

&#x200B;

However I have no idea how to standardise the beta coefficient for the nominal predictor as I think it doesn't make sense for it to have a standard deviation. If I put 150,000 straight in as the mean to the prior distribution of the beta coefficient it causes the predicted value to jump waaaaaaaaaay out of range of what would be an acceptable answer, if I were to leave the prior vague with a mean of 0, everything is happy.

&#x200B;

I've tried to ask him via our course forum, he's replied to other people but not me so I'm not sure what's going on there. Can anyone offer some advice? Please let me know if this is enough information, I can try to provide more if needed.

&#x200B;

BTW I'm using R with runJags library.",Help with standardising coefficient for nominal predictor in Bayesian Multiple Regression,9hnl6j,new,1,1,1,0
"Hey statistics community,

&#x200B;

Im currently working on a study about postmortem liver weights and have measured organ weights, body weight and height, age, and CT-radiodensity of different organs.

I have a group of 200 cases, and I already know that liver weight depends on blood loss (compared liver weight by t-test). Thats really neat, but that is not the only thing.

&#x200B;

Liver weight of course also depends on, for example, body height and weight.

When I do a multiple linear regression to point that out, the best model I can find (by backwards elimination)  also includes \[spleen weight\], and  \[kidney radiodensity\].

The question is:

Does it make sense to include spleen weight  and kidney radiodensity in the multiple regression? I know that spleen weight also correlates with body weight and height, and I am afraid that the significance of kidney radiodensity is somewhat random (at least i cannot explain it).

I checked the VIF but none of them was above 3.

&#x200B;

Help is much appreciated.

konehta",What influences liver wieght at autopsy? A question regarding multiple linear regression,9hmwdh,new,6,3,3,0
"Hello,

&#x200B;

I have this data set below, and I want to create a model of the proportion of arrest between the two PD districts (Location) and Month. What is the best model you would use? (Assuming independence between and within locations and months)

&#x200B;

I just tossed in in R as -----------> 'aov(prop \~ Location + Month + Month:Location, data = data)', however, I think I may be oversimplifying it.

Would time series or repeated measures be a better approach?

&#x200B;

   Month Location      prop

   <dbl>   <fctr>     <dbl>

1      1  Ames PD 0.5483871

2      1   ISU PD 0.4516129

3      2  Ames PD 0.3500000

4      2   ISU PD 0.6500000

5      3  Ames PD 0.6000000

6      3   ISU PD 0.4000000

7      4  Ames PD 0.5000000

8      4   ISU PD 0.5000000

9      5  Ames PD 0.4761905

10     5   ISU PD 0.5238095

\# ... with 14 more rows",ANOVA Help,9hm20a,new,2,2,2,0
"I know how to go about doing equivalence test comparing 2 means from independent groups. My question is, how can I do the same if I have k independent groups, with k>2?","Equivalence tests for k-means, with k> 2",9hlq06,new,1,1,1,0
"I'm currently taking a class called Stochastic Process and it's a very theoretical class and I'm having quite a bit of trouble. In the course outline it says we're gonna be going over

* Basic probability
* Random variables and stochastic process
* Expectations
* Random walks
* Generating functions
* Convergence
* Gaussian processes

I've take an intro to probability class and it was not too bad, but we just finished the probability theory section in this class and it was insanely difficult for me. There are so many proofs and just stuff in general is pretty confusing. [These are some of the exercises we have gone over.](https://i.gyazo.com/de69c69c2439ffd2bfc1a50b9074832e.png) I understand them, and I can prove it using something like a venn diagram, but with an actual proof using equations, that feels beyond me.

[As another example, these are a few of the questions from my latest homework which I found difficult.](https://cdn.discordapp.com/attachments/388789255205421059/492455849156149260/Screenshot_20180920-180256.png)

Can anyone recommend me a book or books that cover those sections",Best book for learning stochastic process (Probability theory),9hl0hp,new,10,24,24,0
"I'm trying to understand some papers on estimating mutual information (i.e. [https://arxiv.org/pdf/cond-mat/0305641v1.pdf](https://arxiv.org/pdf/cond-mat/0305641v1.pdf)), but I'm having trouble filling in their derivations, or having any intuition about these estimators. I seem to be missing something. Can anyone suggest some references just on this area?",Where to learn about estimators/estimation theory?,9hkjkr,new,9,12,12,0
"A total of 23 of 187 patients randomized to drug A met the primary outcome of mortality at 30 days compared with 7 of 191 randomized to Drug B **(risk difference, 8.6% \[1-sided 97.5% CI, −􏰀 to 14.5%\];** ***P*** **= .90 for noninferiority).**

The bold part is really what I need help understanding ",Can someone explain these results from a study,9hkj70,new,2,1,1,0
"I've been looking into probabilistic programming etc., but haven't really managed to wrap my head around how message passing algorithms work. Online searches for example code mostly lead to optimized packages which are painful to parse. Is anyone aware of tutorial-like implementations (preferably in a relatively high-level language) of belief propagation?

​

Thanks!",Code (tutorial) implementation of Belief Propagation?,9hjolw,new,1,5,5,0
How does a site like [playoffstatus.com](http://playoffstatus.com/nfl/afcstandings.html) calculate the chances for each team getting into each position? If anyone has any resources or advice on how this is done it would be greatly appreciated. I'm a huge fan of this site but would like to make a more modern version of it.,"Interested in learning how to calculate sports playoff probabilities, looking for any guidance I can get.",9hj6mm,new,2,2,2,0
"Hey guys,

I've never posted here but I'm looking for help on a homework question.

""
What is the average age of all nickels currently in circulation? Before analyzing data, we need to procure data. How much data? We want to be 99% confident that our sample is within 1 year of the population mean.

The book suggests a pilot study(""google pilot study"" to learn about it). Conduct one - a sample of size 20 or so will do. Report the relevant result below.
"" 

I'm quite confused since we did not cover this in class and our book has 0 mentions of the word ""pilot"". 

I understand a pilot study is supposed to be a look into the cost, feasibility, time, and other factors that may impact a larger-scale study.

To conduct a pilot study on the age of all nickels currently in circulation would not be feasible, time efficient, nor cheap. To inspect every nickel in circulation would be impossible and the resources needed to get them would be extraordinary.

Is this the answer my professor is looking for?

The problem then goes on to ask what size sample is needed but I know how to calculate that.",Pilot study? Estimating needed sample size.,9hiy2k,new,5,1,1,0
"I need help finding the proper regression equation for the following data:

{x,y}

{0,100},{1,34.31},{3,28.54},{7,21.64},{14,17.43},{30,13.62},{60,10.74},{90,8.99}

Just visually, I thought an exponential regression would be the best fit since all other regressions appeared even more inaccurate. So, I used a TI-Nspire calculator (in case anyone is very familiar with that calculator) to calculate the following exponential regression equation (for the provided data above):

""Title""""Exponential Regression""
""RegEqn""""a*b^x""
""a""34.381247988603
""b""0.98214276343992
""r²""0.59394400486561
""r""-0.77067762706959
""Resid""""{65.618752011397,0.54270608996019,-4.0320805443396,-8.6670722470596,-9.2856860766941,-6.4044348805519,-0.92269451354362,2.1973766811596}""
""ResidTrans""""{1.0676588868492,0.015944158640768,-0.13214888436228,-0.33683763879546,-0.42705802687665,-0.38541397113828,-0.082420155499668,0.2802756311824}""

When I graph this equation, it doesn't seem to fit well with the data. What is the best approach to finding an equation that best matches my data which visually does seem to be in an exponential pattern?",What is the best approach to finding an equation that best matches my data which visually does seem to be in an exponential pattern?,9hihuo,new,3,0,0,0
"For context: I'm analyzing a series of microbiological results, more specifically plate counts. A plate count can return values of 0 if no microorganisms grow, 1, 2 yada yada yada until 300. A plate that has more than 300 colonies is only counted until 300 is reached and then it's marked as >300. The same for other test methods that can return from <1 to >2000.

&#x200B;

So I need to calculate an average value of microorganisms, but obviously >300 and <1 are not numbers. What can I do in this situation? Also, some sample groups are all >300 values or all <1 - what should I do then?

&#x200B;

Thanks!","How to I calculate average, median and deviation with a set that has "">X"" values?",9hh4ro,new,16,7,7,0
I have a project that asks me to collect 50 observations of two sets of quantitative data that I think have a relationship and that relate to my life. I suggested hours of sleep each night and GPA to my professor but he says that would not be good enough because they may not be related. I don’t really understand what he’s looking for exactly can anybody point me to the right idea or to some data that matches my specific situation?,What would be a good data set?,9hgl5l,new,11,2,2,0
"I am  trying to  concentrate  my brain  the best  that I  can, but even doing this  I  can't  really understand  what's  the meaning   and  the usefulness  of ''prior  distribution''    and ''posterior  distribution''....  I  am  new to statistics, please could  some one  be so gentle to try to let me   understand those  concepts  in a  simple way?  Because  I really can't   understand them

I know  that   inferencial  statistics is based on assumption  about a  distribution of data,  but this   distribution is  real, it  exists , you can see this plotting  your  data set

My question is  what is  this  ''a prior''   and  ''posterior'' distribution?  

&#x200B;

&#x200B;","New to statistics, Can't really understand prior distribution/post distribution",9hd98h,new,15,16,16,0
"I'm looking to improve my DOE skills, and found this training workshop run by Stat-Ease: [https://www.statease.com/training/workshops/class-mdoe.html](https://www.statease.com/training/workshops/class-mdoe.html)

Has anyone attended it, or have any other training suggestions? I'm specifically looking for DOE training in general with focus on response surface methodology. I'm also interested in genetic algorithms for optimization. I've found that trying to learn on my own has been slow going.","Can anyone recommend a good training seminar or workshop for improving DOE skills, specifically for response surface methodology and GA?",9hd846,new,0,1,1,0
"*I know the question is easy but it seems impossible to find an answer online! :)*

&#x200B;

I have a simple logistic regression model with 2 categorical predictors.

* predictor 1 = age group = young/normal/old
* predictor 2 = city = rome/paris/london
* target variable = the user converted (1) or didn't convert (0)

I use dummy variables (with the n-1 rule) so my model is:

**target = b0 + b1\*age\_young + b2\*age\_old + b3\*city\_paris + b4\*city\_london**

My reference category for the age group is normal and for the city is rome.

Let's say I get the following results:

&#x200B;

* b0 (intercept) = -2.9429
* b1 (age\_young) = -0.0624
* b2 (age\_old) = -0.1618
* b3 (city\_paris) = 0.4060
* b4 (city\_london) = 1.0060

So e\^b0 is the odds ratio when all the variables are 0, i.e. when the user is from rome and belongs to the age group normal.

1. **What is the odds-ratio for rome?**
2. **What is the odds-ratio for age\_normal?**
3. **What is the odds-ratio for paris (I want the ""absolute odds ratio"", not the odds ratio of paris compared to rome)?**",Extract odds-ratios for the reference category from a logistic regression model with multiple categorical variables with 3+ levels,9hc0m5,new,3,2,2,0
"# Why is it that in order for the AR(P) process to be stationary, the roots the autoregressive model must lie outside the unit interval?

That is, the roots of 1 - \\phi\_{1} B - \\phi\_{2} B\^{2} - ..  - \\phi\_{p} B\^{p} must lie outside the unit circle.",Time Series Question,9hbpok,new,4,4,4,0
"Hi all,

I have data on 25 food items (variables) from 60 families, and want to investigate whether there is a relationship between purchasing high fat foods (one of the variables) and other variables. The variables are reported in proportional frequency, so all variable values within a family add up to 1.

I tried running a Pearson correlation and did find some significant correlations, and wanted to proceed to another test to discover more information. I looked into factor analysis, but the N(=60) is not strong enough for factor analysis. I'm investigating cluster analysis and regression analysis, but not sure which one would be more effective? I'm an undergrad student with only introductory statistics background, so all of this is very overwhelming.

Thanks everybody.",Which test to use?,9hbjws,new,9,2,2,0
"Is there a rule for when to treat Year as a class variable versus a continuous variable in a regression model? For example, if there are two years versus if there are ten years should it be treated as a class or continuous?",Class versus continuous variable for Year,9h9q9i,new,2,1,1,0
"# I am using the gllamm command in stata to estimate a model where my observations (level 1) are nested within neighborhoods (level 2). I am also incorporating fixed effects at level 1 and putting indicator variables in the model to control for the coder. Given the information I provided, does that sound like an unreasonable approach in any way? ",Using a generalized linear latent and mixed model with fixed effects,9h928z,new,0,1,1,0
[Lecture video](https://www.youtube.com/watch?v=wXNWVhE2Dl4) and [proceedings article](https://eta.impa.br/dl/PL012.pdf).,"Dynamical, symplectic and stochastic perspectives on optimization – Michael Jordan – ICM2018",9h8q9n,new,7,25,25,0
"Hi, I am conducting an investigation concerning the labor situation of civil engineers in Honduras.

I already calculated the alpha Cronbach with 10 samples, but I'm not sure the ideal amount to perform the test. Also, I would like to know if I should make another statistical test to validate the questionnaire.

&#x200B;

Here is an explanation of the hypothesis and the objectives of this investigation:

The hypothesis to prove is: ""Being a member of the Honduran Association of Civil Engineers DOES NOT generate value in getting better job conditions"" (understood by ""conditions"" as better wages, less overtime work, less gender gap, extra benefits like health care, labor rights, etc.)

The main objective is

Understand the labor reality in Honduras for the professionals of Civil Engineering

Specific objectives :

Define the total number of civil engineers that are members the Honduran Association of Civil Engineers vs the number of professionals of civil engineering that are not part of the organization

Define the percentages of men and women that work as civil engineers

Determine the age of the population of civil engineers in Honduras

Calculate the unemployment percentage of civil engineers

Calculate the percentage of civil engineers that are employees, employers, independent, etc

Define in which area of the country are more civil engineers working

Determine from which university is graduating the majority of civil engineers

Define the percentage of civil engineers that can speak more than one language, have master degrees and PhDs

Define the real minimum wage that it is being paid to the civil engineers

Analyze which aspect is more determinant to obtain a better salary (labor experience, academic level, gender, knowledge of more than one language, age, working in far-away projects )

Analyze the labor situation of the civil engineers by each sector of the industry (road construction, project management, education, research, structural, supervision, hydraulic, earthworks, residential, materials production, commercial, etc.)

Analyze the situation of the companies where the civil engineers work (quantity of employees, local or overseas firm, stability)

Determine the average working time for a civil engineer

Determine the existing gender gap in the industry","How many samples should I apply to calculate alpha crombach, and should I consider another statical test to validate a survey questionnaire concerning an Investigation of the labor situation of civil engineers in Honduras?",9h7dmn,new,0,1,1,0
"Hello All,
Chances are my title has already exposed how new I am to Mathematics.  I would like to pursue a career in Data Analytics.  It seemed to be ""Computery"" and not knowing anything I just assumed that majoring Comp Sci is the obvious choice.  Now it seems to me that I should have majored in Math. I have 74 credits and double majoring would add about a year, I must minor in mathematics so will certainly take as many stats classes as possible but I think I will have to do most of my learning on my own, if I am to obtain, at least a respectable percentage of the seemingly endless list of skills needed to be a (good) Data Analyst.

Any information, guidance or resources would be very much appreciated.  I have gotten great advice but am still unclear on how to compliment my Comp Sci curriculum with knowledge needed to , one day, be a professional data analyst.  Thanks! ",Any suggestions for resources which will help me practice and learn Stats esp as it relates to Data Analytics,9h71xp,new,20,0,0,0
"I plan on taking Stochastic Differential Equations next summer and need to know what pre requisites I should have under my belt? I'm in an Applied Math Masters based in the engineering school.

I have taken Multi-variable Calculus, Calculus-based Probability and Statistics, and Applied Liner Algebra in my undergrad.

Right now ( first semester in M.S): Statistical Regression, Computational Statistics ( both require Calc 3, Linear Algebra, and programming, and are about 50% proof based each)

Spring of next year: Matrix Theory (Advanced Proof Based Linear Algebra), and I plan on taking either Monte Carlo Methods or Ordinary Differential Equations.

The ODE class is a graduate level class that dives deeper than a typical undergraduate course. I am wondering If you guys think I can skip ODE, and take Monte Carlo Methods( very applicable to my future in Financial Engineering), and just self study the ODE, and be ready for stochastic calc?

&#x200B;

I can post the syllabus for the courses if needed.

&#x200B;

this is the SDE book: [https://www.springer.com/us/book/9783540047582](https://www.springer.com/us/book/9783540047582) by Oskendal",Prerequisites for Stochastic Differential Equations?,9h5kcu,new,6,16,16,0
"Hi,I would love to minitab assistance for just some simple boxplots/histograms using data that looks like the following:
https://imgur.com/a/HEcDzZL

When I attempt to do boxplots that are sorted by groups, different categorical variables, it always plots one chart with the three groups as one predictor, but I want three different boxplot groups in the same graph. I don't know if that makes much sense, but any help would be appreciated.",Seeking Minitab assistance,9h5847,new,6,1,1,0
"I have a small sample (\~30 respondents) of survey data (25 questions) which is all in likert scale (1-5). I have 5 'business performance' questions and several variables I'm testing to see if those who utilized certain techniques had better performance.

This instrument was designed well, and covers the topic of my thesis, surveying industry professionals. I have my Literature Review done, and my Methodological review almost done, and so now is the time to run the numbers. 

The only thing is that my thesis adviser left the university over the summer, and I don't know how she planned to run these, 

I've played with regression before, but I recall her saying we could do T testing and Anova on this. I tried to download SOFA and run some numbers there, but it's not like things I've done before and so I'm not sure which test to choose, when I use Anova it says I don't have data in certain groups depending on the variable.. which is also likely very true, it is a small sample. 

Basically, is t here any software that you can point me towards where I can run some low level correlation as I described in the first line? I'm a total newbie and I don't know where to start.. but can make sense of results once I know what number to look at.",Thesis Adviser bailed on me.. I have survey data and not sure what to run,9h521x,new,9,2,2,0
"I am looking at the effects of both depth of turbines and distance offshore wind projects are built to shore on the overall capital expenditure of the projects. I am looking to test this via regression analysis. 

Both distance to shore and turbine depth are pretty strongly correlated with an r of .784, suggesting multicollinearity, meaning perhaps a multiple regression analysis cannot be applied here. However when performing a collinearity diagnostics in SPSS, the VIF for both independent variables is 2.510, which suggests the collinearity is low. 

however I am not sure whether testing them via multiple regression is the right approach, due to the high correlation. Would I be better off performing two separate single regressions? The high R number and low VIF has me stumped. Ideally I want to be testing both of these variables. Can anyone please advise? My statistics knowledge is fairly basic and I am struggling to wrap my head around all of this. 

Thanks",Multiple linear regression - correlating independent variables but low VIF,9h0zx1,new,2,2,2,0
"What’s the best way to explain correlation to a non-technical audience? Would you call it a relationship between two variables? Is it the probability of one variable responding to another a certain way?

I’m looking for sound, but non-technical verbiage. Thank you.",Explaining Correlation,9h0zkk,new,7,3,3,0
"Let's say you have a bag, with 100 balls and with 4 colors of balls, red, white, green, blue. 

The actual probabilities for each color is      

\[0.36438797, 0.12192962, 0.19189483, 0.32178759\] 

&#x200B;

But let's say my model comes up with two predictions for this distribution as follows, with the KLD and MSE values given

P(x)   : \[0.37300551, 0.12188121, 0.18509246, 0.32002081\] 	KLD:  0.0002 	MSE: 0.688 

Q(x)  : \[0.33014522, 0.03053611, 0.30264458, 0.33667409\] 	KLD:  0.1028 	MSE: 0.1459

&#x200B;

Why is P(x) better than Q(x) or vice versa ?

&#x200B;","Why is Kullback Leibler divergence a good way to compare two probability distributions? I'm unable to understand intuition behind the advantage/appropriateness it has over say, mean squared error or cross entropy.",9h07jo,new,12,14,14,0
"I got into a debate with someone  online and it eventually came down to this question.

I was saying that in the absence of information about the likelihood of an event, we should assume equal probability of all outcomes. So, for example, if we're shown a house and told that behind the house is either a fox, a bear, or a dog. But we're given no information about, say, the population of foxes, dogs or bears in that area, then it would be correct to say that, given the information we have, there is a 1/3 probability that there is a bear behind that house.

It may be possible, that there's actually a hundred times as many dogs as bears or foxes in the area making the probability, given that information, of the animal being a bear much lower than 1/3, but, since we're not given that information, I said it would be correct to say that the probability is 1/3, since all probabilities are contingent on the information given.

The person who I was talking to was saying that that would be inaccurate and that you would have to say that the probability is ""undefined"" so to speak, since we're not given enough information to come up with a meaningful probability.

I was hoping someone with a statistics degree could tell me who's right, and if there's some theorem or postulate in statistics that we can point to to prove who's right",Should one assume equal likelihood of all possibilities in the absence on information?,9h079d,new,20,14,14,0
I've been working on a survival analysis basically looking at the probability of hospital discharge between two different treatment modalities and used a log rank test (as well as wilcoxon rank sum) to evaluate the difference between the two. Is there a test (or way) to compare the area under the curves and find a difference?  ,Survival Analysis Question,9gzl39,new,5,3,3,0
"Hello, I am not very keen in simple regression.

My simple assignment given was to find intercept and beta through SAS with given dataset. I used **proc reg**.However, the results I got was ""parameter estimate"", ""standard error"", ""tvalue"", and ""pr>t"".

I am going to assume pr>t is pvalue but which one are intercept and beta? Google didnt help me much.  Thank you.",Finding intercept ans beta from simple regression,9gz68k,new,8,2,2,0
"No prior data to estimate variance or means from. ~30 questions, one of them is a grouping question to be used for the comparison (rural vs urban respondents). Many questions are correlated since they cover just 3 topics, i.e. if someone says ""very likely"" to question 14 we definitely expect them to answer similarly for question 17. Sounds like a lickert scale will be used.

I'm a junior statistician who never took a survey methodology course, so I've done sample size calculations but it was for simple comparisons of mean for group A vs mean of group B. So feel free to send references to papers/texts.",How do I get started finding appropriate sample size for basic survey research with ~30 questions covering 3 topics with a purpose of comparing two groups?,9gywo6,new,3,5,5,0
"M/23/Baltimore,

I'm good at math and have only an Associates degree in Transfer Studies. I plan on getting a min wage job to pay for a WGU degree in data analytics.

Assuming I do these 2 things right, is there a chance I can get a career as a statistician? I like math, I'm good at it, and I understand that ststisticians have some of the highest job satisfaction rates in the US.

Also, what is a reasonable entry level position to search for upon receiving a degree?",What is the career path for someone who ultimately wants a job in statistics?,9gyl3i,new,34,11,11,0
"My textbook says

For 2nd order weak stationary process there will be a constant mean and variance, the correlation and covariance being a function of time difference alone

For 2nd order strict stationary, the mean and variance will be finite and the correlation and covariance being a function of time difference alone

Wouldn't this imply that any weak stationary also means strict Stationary. Even though that's not correct.",Strict vs Weak Starionarity,9gxwbo,new,1,2,2,0
"I want to answer the question of whether the effect of family structure (living with both parents, just mother, just father, step parents, etc.) on educational attainment has changed over time. I have broken my sample into 5-year cohorts based on their birth year, and then I created an interaction term between their family structure and their cohort.

I understand how to combine the main effect and the interaction effect to understand the total effect, but I'm curious about how to interpret the significance of the variables. Sometimes both the main effect and the interaction are significant, sometimes only the main effect is significant, and sometimes only the interaction is significant. What do these three situations mean?",Interpreting the significance of main effects and interaction effects?,9gx4x7,new,3,4,4,0
,"After running a linear mixed effects analysis that returns a p value > .9, are there any other steps I can take to ""verify"" this p value?",9gwtrd,new,2,2,2,0
"Hi there. I am currently a PhD Fellow in science educational research. I am currently conducting a study on the effects of inquiry learning on L2 speakers in lower education. In this regard I am trying to assess my dataset through a propensity score analysis following the marginal mean weighting through stratification approach, [based on the method in an article I found](https://www.tandfonline.com/doi/abs/10.1080/09500693.2014.1000426). 

&#x200B;

As someone relatively new to statistics, I have been wondering which tools would be best suitable to solve my research question and, in the greater perspective, which would be most beneficial for someone pursuing a career in educational research. After initially starting out with SPSS, I found that it's a bit inflexible for my purposes. Based on recommendations from researchers at my university (among them someone skilled in SPSS), I was recommended learning to use R instead. I believe R presents a powerful tool suitable to my purposes, and probably more rewarding in the long run. From what I gather, R is a well-established powerhouse in statistical computing. However, I now see that there are other programming languages that also have emerged as tools for statistical analysis. Python, as a popular general purpose language, seems like an interesting option given its greater versatility. I recently read about [Julia](https://julialang.org/), which seems rather promising if it is everything it is hyped up to be, with regards to be significantly faster, compiling, easier syntax etc. From what I understand, Julia has been gaining in popularity in the last year, and some even describe it as the future of statistical programming. In that regard, learning Julia seems like a good idea, but I have to question the prudence of learning a small language with relatively few packages available for someone with limited knowledge and skill in programming and statistics.

&#x200B;

Given that I have to learn statistical programming, I guess my question is: Where is my effort best spent both with regards to my current needs and for being best prepared for the future? Should I go for the old, but significantly more popular and well-established R, or should I go for the general-purpose language Python, or should I go for the ""new-kid-on-the-block"" Julia (or should I stick with some statistical software like SPSS or SAS or some other option)?",Which software/programming language for quantitative analysis would you recommend? R vs Python vs Julia.,9gvres,new,38,9,9,0
,Can somebody fill me in on why the p-value is controversial among academics?,9gvnqs,new,14,0,0,0
"The goal of the experiment is to show that the time to complete the general task is substantially less with the experimental arm than with the control arm.

The task itself varies in difficulty depending on subject experience and specific parameters associated with that instance of the task.

How many tasks should the subject complete to incorporate that variability if there is no prior data? Is there a paper that has explored this idea?",Two arm experiment to compare times to complete task of varying difficulty. How many observations per subject?,9gvlgd,new,0,1,1,0
"Let's say I have presented 3 men to participants (a given participant sees all 3 conditions): Andrew is 5'9""; Bob is 6'0""; and Craig is 6'3"". I ask participants about the **desirability** of each man, and also to rate his **physical attractiveness**. I want to find out if height influences a man's **desirability**. I want to somehow 'control for' the man's **physical attractiveness**. How do I go about this - some kind of ANCOVA perhaps?",Is there a way to statistically control for a variable (attractiveness) in a within-subjects design?,9gv5oc,new,3,0,0,0
"Hi Guys,

Suppose I want to calculate the average rating of a movie  but considering how the user usually vote ( optmistic/pessimistic):

&#x200B;

|User|Movie1|Movie2|Movie3|
|:-|:-|:-|:-|
|User1|5|3|3|
|User2|4|4|4|
|User3|5|2|5|
|User4|1|1|1|

&#x200B;

How can I reduce the impact of User4 vote on Movie3's average rating considering that User4 always gives 1 to all movies?

Best,

&#x200B;

&#x200B;

&#x200B;",How to normalize Movie Ratings according to how users vote?,9gv43s,new,10,3,3,0
"Hi,

I just did an interview with Tim Harford, author of *The Undercover Economist* and senior columnist for the Financial Times, and thought people might like to read it.

[https://blackswans.io/post/26/](https://blackswans.io/post/26/)

He explains why Vote Leave’s bogus £350m NHS claim was so effective, why it’s so difficult to predict stock prices and why early researchers often get too much credit. 

Enjoy,

Jack","My Interview with Tim Harford on Misleading Statistics, Naive Predictions and Sticky Research",9gtj7h,new,2,36,36,0
"Dear redditors,

&#x200B;

I have to hand in a paper on how statistical analysis should be performed when the data is non-gaussian. Bootstrapping will be included, as will the Wilcoxon signed rank test. However, what else can I write about. My main issue is that I need **at least** 5 pages of plain text in an 11pt font. 

&#x200B;

I anyone has any suggestions on how I can get up to a page number like that on this topic, please share your insights!

&#x200B;

Thanks in advance!",Stuck on paper about non-gaussian data,9gssej,new,6,1,1,0
"Okay, so the question I'm working on is in the link below.

[http://prntscr.com/kvm2bp](http://prntscr.com/kvm2bp)

Can we actually determine if it is mutually exclusive in this instance? I feel like I need P(A AND B) in this case.",Am I missing something? (Mutually exclusive events),9gqr0z,new,2,1,1,0
"I’m starting a MS degree in stats and the book we are using is Casella and Berger. I was a humanities major and took a gap from school before starting so my math is rusty. What math topics would you all recommend to review so that I’ll be able to keep up in the course? I know, should have started earlier but here we are... Thank you!",What math to review for a statistical inference course using Casella and Berger?,9gq3xw,new,23,26,26,0
"Hello friends!

I have a time series that is very volatile and I think it's perfect for a ARCH/GARCH model. I have read and I know that my series has to be stationary, but I don't know how can I test for unit root in a series that might follow a GARCH representation. 
Can I just use ADF or KPSS unit root tests? 

In addition, do you know about conditional forecasts using GARCH or MGARCH models?",Unit Root Test for volatile data,9gpyy6,new,0,2,2,0
"What do the numbers say about the data. For example. Some are over 200, and some are only .26. Is this the same as an analysis of variance test where the .26 would tell me that the data is significantly different? I tried watching some khan videos/youtube but can't make sense of it. Any help appreciated.",Need help understanding the =CONFIDENCE function in excel. I collected a bunch of data for a course which the professor then analyzed for me. I have to report on the results now. One section he uses =CONFIDENCE which takes into account the standard deviation at a 95% confidence level.,9gpbhv,new,1,0,0,0
"I was wondering if anyone had any resources/examples with spike and slab priors in the context of variational inference. A lot of the details/computations seem to be left out of the papers I've been reading, and I wanted to go through (by hand) the explicit calculations to get a better sense of the variational updates.",Spike and Slab Prior,9govw7,new,3,1,1,0
"It's my understanding that R is not CFR compliant in terms of creating, editing and maintaining study RECORDS. That's pretty much what EDCs are for. 

However, I can't find guidance to back up the statement 'clinical decisions cannot be made based on analysis from non-CFR 11 compliant systems.' Based on the link, it seems base-R is CFR 11 compliant for graphing and interpreting data. 

https://www.r-project.org/doc/R-FDA.pdf",Is R CFR part 11 compliant for making clinical decisions based on R analysis/graphinh in clinical trials?,9go54h,new,4,5,5,0
"I'm in the midst of studying for my first stats exam, but i'm unable to find material that i can repeat over and over. All of my google searches result in either coursehero and chegg answers, or PDFs from an array of random professors lectures. I feel as though i can read the material all i want, but at the end of the day this subject is about repetition. If anyone has any recommendations on sites that offer practice problems i can print off PLEASE let me know.",in search of practice problems/PDFs to practice for business statistics,9gmgqg,new,17,10,10,0
"Hello all,   I was wondering if anybody could help me write out the explicit model form of the following fixed effects specification.  Basically I am analyzing data on 50 sites across 7 years.  I have grouped observations into 3 groups such that any given location may belong to a different group (e.g. Site 1 could belong to group 1 in year 1, but maybe group 3 in year 2).  I am setting the time dimension and group as fixed effects.   Any help would be greatly appreciated.   ",Model notation,9glar2,new,2,4,4,0
"Now that you guys have explained what cumulative sum is, can you also explain how a cumulative sum control chart works? Taking a sample input of say {1,2,3,4,5}?

&#x200B;

Thank you",Explanation of cumulative sum control charts,9giimi,new,5,0,0,0
"[The answer in context.](https://stats.stackexchange.com/a/145277/)

>Without any prior knowledge, you must assume a categorical distribution for this support and sufficient statistic. It is an exponential family. (All minimum assumptive distributions for a given support and sufficient statistic are exponential families.)

The only results for ""minimum assumptive distribution"" on Google all come from this same guy. Elsewhere he defines it to be ""the maximum entropy distribution that is concordant with your measurement"".

Does anyone know if there's a more mainstream term for what he's referring to? Where could I find a proof of his statement about exponential families?",Would appreciate help understanding part of an answer from the CrossValidated,9gc20h,new,8,2,2,0
"For me, it's mostly SAS and R.  Would you guys recommend learning any other languages?",What programming languages do you use for your work/ in school?,9gbzdb,new,81,24,24,0
"We know this famous equation, estimate= parameter+ Random Error+ Bias 

But I also know that quota sampling is a non-probabilistic sampling method.

But I am not confident to conclude that there is zero random error. 

If the statement holds true, can I safely assume that the bias can be calculated from the difference in the value of the estimate and the parameter.
",Does quota sampling has a random error?,9ga863,new,1,0,0,0
"I am using the following model: 

Y_i= α+ γ_1 D_1i + γ_2 D_2i+ ε_i

When looking at the standard error of my estimated y_1 and y_2 they are exactly the same. Furthermore, looking at the standard error of the mean of the group ""α + γ_1 + ε_i"" and ""α+ γ_2 + ε_i"" I get different values for their stand errors. While it makes sense that the mean of the groups differ from the estimated parameters (since they are in reference to α) I am wondering why/if a difference in SE is actually plausible.",Standard Error and Linear Regression,9g98ms,new,0,1,1,0
"I  don't  know  when to use  a non parametric regression, is  somenthing  I am  studtying now  and  still new  for  me, so any  help  would  be  great because  I don't  know  when  to use a  non parametric  or  when  to use just  a parametric regression.   From wikipedia:

**Nonparametric regression** is a category of [regression analysis](https://en.wikipedia.org/wiki/Regression_analysis)  in which the predictor does not take a predetermined form but is  constructed according to information derived from the data.  Nonparametric regression requires larger sample sizes than regression  based on parametric models because the data must supply the model  structure as well as the model estimates.

&#x200B;

**My question.   What  does it  mean   '**\*\*\*'\*\*\****in which the predictor does not take a predetermined form but is  constructed according to information derived from the data'' ?***

*Nonparametric regression requires larger sample sizes than regression  based on parametric models because the data must supply the model  structure as well as the model estimates.*

&#x200B;

What does it mean ''the predictor does not take  a predetermined form''?  What form is  he talking?  The  distribution?

&#x200B;

What  does it mean  ''is constructed  according to  information derived  by data'' ?

&#x200B;

''the data  must supply the model structure''...   does  it mean  the data  must supply the  predicted values?  I can't  understand  what does he mean  with  model structure

&#x200B;

&#x200B;

In this  pic: [https://upload.wikimedia.org/wikipedia/commons/0/07/NonparRegrGaussianKernel.png](https://upload.wikimedia.org/wikipedia/commons/0/07/NonparRegrGaussianKernel.png)

Example of a curve (red  line) fit to a small data set (black points) with nonparametric  regression using a Gaussian kernel smoother. The pink shaded area  illustrates the kernel function applied to obtain an estimate of y for a  given value of x. The kernel function defines the weight given to each  data point in producing the estimate for a target point.

&#x200B;

&#x200B;

What is this  kernel  function and  how  does it  work?  

 It defines the weight given to  each point  data (observed data points)  in producing  the estimate (predicted value)  for a  target point

&#x200B;

Isn't it  some kind of  weighted least square regression?

&#x200B;

Basically I don't  understand  when  to use  a  non parametric regression,  any example of  dataset  which would be an ideal ''candidate'' for a not parametric regression, would  help me to understand, thank you

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;","Non parametric regression, could you please help?",9g8oyu,new,10,2,2,0
"Should be easy, I am just being stupid.

&#x200B;

""It is the dimension along which the data vary the most, and it also

defines the line that is closest to all n of the observations.""  


These feel like directly opposite, contradictory statements to me.  What am I missing?  


Thanks v much.",Dumb question on Principal Components Analysis,9g8jua,new,4,4,4,0
"I am currently working on a psychometric meta-analysis and have a question which I couldn't find the answer to on Google. Hope this is the right forum for this question, if not mods can remove.

&#x200B;

My construct of interest is multi-dimensional. In many of the unpublished dissertations I am including in my meta-analysis, the authors only report the reliability (Cronbach's alpha) of each dimension. Because the overall reliability is what I need, I am wondering if it's possible to calculate the alpha of the aggregate construct from the alphas of each dimension. I have searched for the answer to this in Hunter and Schmidt's book, but couldn't find anything.",Can the overall reliability of a multidimensional construct be calculated from the reliabilities of each dimension?,9g7qsk,new,1,1,1,0
"We have a Slack study group in which we are doing a slow read of McElreath's marvelous book on Bayesian statistical programming. We are both discussing the conceptual content and working through the examples and exercises in R, PyMC3 and Stan. If you would like to join us, let me know and I will send you an invitation: [pluviosilla@gmail.com](mailto:pluviosilla@gmail.com) ",McElreath's Statistical Rethinking Slack Study Group,9g6l1i,new,2,4,4,0
"We have a Slack study group in which we are doing a slow read of McElreath's marvelous book on Bayesian statistical programming. We are both discussing the conceptual content and working through the examples and exercises in R, PyMC3 and Stan. If you would like to join us, let me know and I will send you an invitation: [pluviosilla@gmail.com](mailto:pluviosilla@gmail.com) ",McElreath's Statistical Rethinking Slack Study Group,9g6cv6,new,11,43,43,0
"I’m looking for comparative houses prices, salary, debt, interest rates... is anywhere one can look for it?",We’re can one find up to date statistics on the Portuguese market?,9g5phd,new,3,0,0,0
"So.... I'm toying with a much larger idea, but I'm wondering if it would be possible to do a very ROUGH 3-dimensional visualization (spherical shaped) of various philosophical axioms, their confidence levels, and then following up with a limited number of claims that are necessarily nested within each axiom or group of related axioms?

&#x200B;

I have... a much larger idea that is related to this that I'd be happy to get into, but it's very rough, not researched, and definitely outside of my bailiwick. :p Still, after a few conversations, I am beginning to think its possible.

&#x200B;

But it would necessarily start here.

&#x200B;

&#x200B;",Visualization for Confidence levels of Philosophical Axioms,9g5gse,new,10,0,0,0
"I would like to know how to answer the question ""by how much do two different character frequency lists differ"". For Chinese language aquisition, different people publish different list of character frequencies, using different sources (e.g., newspapers, Weibo, etc) for the calculation. I'm interested in figuring out how to quantify how close or far away two character lists are from each other. Both lists are guarenteed to contain the exact same number of characters, and each character appears exactly once; the only difference is the order in which the characters appear may be different.

For example, take the following two hypothetic character lists, where the index represents the frequency ranking (e.g., the first item in the list is the most frequenct character, the second item in the list is the second most frequent character):

    a = ['的', '我', '了', '一', '不', '是', '你', '人', '有', '在']
    b = ['我', '的', '一', '了', '是', '不', '人', '你', '在', '有',]

By eyeball I would say these two character lists are ""not that different"", but how would I be able to quantify it?

On the other hand, I would say that the following two character lists are ""very different"":

    a = ['的', '我', '了', '一', '不', '是', '你', '人', '有', '在']
    b = ['人', '是', '我', '你', '的', '有', '在', '一', '了', '不'

-----

Alternatively, the lists could contain the absolute counts for each character, something like the following:

    a = [('的', 256), ('我', 128), ('了', 64), ('一', 32), ('不', 16), ('是', 8), ('你', 4), ('人', 2), ('有', 1)]

Or the lists could contain the percentage each character occurs in the total data set:

    a = [('的', 0.2), ('我', 0.1), ('了', 0.05), ('一', 0.025), ('不', 0.0125), ('是', 0.00625), ('你', 0.003125), ('人', 0.0015625), ('有, 00078125)]

----

If it is relevant, Chinese characters (and most other languages) follow Zipf's Law which states that the most frequent character will occur twice as often as the second character, third as often as the third character, and etc. ",How to quantify the difference between two lists of Chinese character frequency data?,9g4xlc,new,6,5,5,0
"I have to pick a card out of a deck 5 times, and I have to “win” at least 3 out of those 5 times. I’ll be drawing the cards at random.

For the first 3 draws, I have a 2/3 chance of drawing correctly (after each draw, the card I selected goes back in the deck)

For the last 2 draws, I have an 85% chance of drawing correctly.

Out of the 5 total draws, I just need to know what are the chances of drawing correctly at least 3 out of the 5 times? ",Can someone please help me solve this type of statistics problem?,9g3wqa,new,9,1,1,0
"Hey folks,

&#x200B;

A few days I came across a question here on calculating correlation between a categorical variable and continuous one. Inspired partially by it, I wrote a (hopefully) comprehensive on different correlation metrics - especially focusing on comparing correlation between continuous-continuous, continuous-categorical and categorical-continuous feature pairs. Would love some feedback if people have any. Thank you so much!

[https://medium.com/@outside2SDs/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365](https://medium.com/@outside2SDs/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365)",How to calculate correlations between categorical and continuous features in a comparable manner?,9g21m5,new,0,2,2,0
" 

I'm trying to understand the basics of GAMs. Wood's book ""Generalized Additive Models: an introduction with R"" (1st edition) introduces GAMs via a cubic spline basis {b\_j (x)} (see p. 122), where b\_1(x)=1, b\_2(x)=x  and 

b\_{i+2} (x) is defined via a certain function R(x,z). But there's definitely an error in the definition of R(x,z). 

&#x200B;

Could anyone suggest another simple but mathematically rigorous (and relatively comprehensive) introduction to GAMs?

Thanks in advance. ",Generalized additive models - a good introductory textbook?,9g1m2r,new,8,2,2,0
"I'm currently working on my research project where I look at changes in cells of diseased animal and give them ordinal scores (histopathology). I was thinking that it was unwise to do statistics on my project for 2 reasons, one being I have a small sample size(n=12), another being it is a preliminary study with no references. Recently my supervisor has been asking me if I can do statistics and I'm just at a dead end. The only statistics I can think of doing is one way ANOVA between the scores of different organs, but it just feels weird comparing severity in two different organs. My question is, what statistics can I actually do to make sense? ",Can I do statistics for my research (help)?,9g10l5,new,12,1,1,0
"So I am looking to conduct a 2SLS regression, as I believe to have some endogenous variable in my standard OLS regression.

My question is whether the instrumental variables used for the 2SLS must be variables that do not already exist in my standard OLS regression as controls.

In other words, if my OLS regression looks as follows:
[y1 = a+b+c+d+e+error]
and I believe variable C to be endogenous:
[c = a+e+f+error]

How would I proceed to construct my 2SLS with instrumental variables?

Maybe I also completely misunderstand the concept of 2SLS, but I would appreciate some help. For context, this is for financial research regarding M&A transactions.

",2SLS Regression,9g0mes,new,10,3,3,0
"Sorry if this is a basic question...

I'm running stats on 12 years of wildlife observations as part of my masters project. Each observation records how many male zebra, lactating female, non lactating female, and foals of various ages were seen.

I want to run a test (ANOVA?) that shows if there is a difference in the count of these groups over time, habitat type, etc. Zebra count are scale in SPSS. Year, habitat type are nominal. 

When I run an ANOVA it is counting the frequency of observations, not how many zebra from each group were counted. Observations with ""0"" zebra are counted. 

When ""0"" is not a ""missing"", SPSS says the frequency is the total # of observations, regardless of how many times 1 vs 5 vs 15 zebra were recorded.

What am I doing wrong?
Thank you so much for your help!
",ANOVA on SPSS Help,9g0kkb,new,1,0,0,0
"I have a problem where I need to run a regression but need as output the distribution of values rather than simply the point estimate. I can think of a few different ways of doing this (below) and would like to know a) which of these would be best and b) if there are any better ways of doing it. I know this would be straightforward for something like linear regression but I'd prefer answers which are model agnostic.

My approaches are:

* Discretize the continuous variable into bins and then build a classifier per bin, the predicted probabilities for each bin provide an approximation of the pdf of the target and I can then either fit this to a distribution (eg normal) or use something like a LOESS to create the distribution.
* Run quantile regression with appropriate intervals (eg at 5% intervals) and then repeat a similar process to the above (LOESS or fit a distribution)
* Train a regression model then use the residuals on a test set as an empirical estimate of the error. Once a point estimate is made then take the residuals for all values in my test set close to the point estimate and use these residuals to build the distribution.
* Using a tree based method, look to which leaf (or leaves in the case of random forest) the sample is sorted to and create a distribution from all points in a test set which are also sorted to this leaf (or leaves).",Regression to predict distribution of value rather than point estimate,9g0i3r,new,36,18,18,0
"As a follow up to [this](https://www.reddit.com/r/statistics/comments/9fum9q/should_i_take_monte_carlo_methods_or_neural) post, I was someone who studied Neural Nets when I had the choice and my dissertation used them extensively for image recognition. However a lot of people are pointing to MCM as being more useful. Any of you guys know what are some good sources for learning ? Thanks.",What are good sources for teaching myself Monte Carlo Methods?,9fwyxy,new,8,2,2,0
"My professor posted a lecture video about a normal coin toss, but the questions are on an unfair coin toss and I have no clue how to begin answering the questions. Any advise would be great. 

 **Suppose I have an unfair coin that comes up heads 70% of the time.  I flip this coin two times.**

The questions I am having trouble with are... (They seem to be so simple, but the unfair coin is really throwing me off here.)

 **What is the probability that I flip exactly one head.**

 **What is the probability that I flip at least one head.**

Thanks :) ",Unfair Coin Toss,9fwcr6,new,6,1,1,0
"I have 100 categorical variables with cardinality less than 10.
 I want to do dimension reduction
Chi Sq Test -> It will be very long process ....",Chi Sq Test,9fuzl5,new,1,0,0,0
"While reading some material for my comps, I stumbled upon an interesting fact.

Suppose Z_1,...,Z_n are independent standard normal random variables which statisfy

`[; \sum_{k = 1}^{n} a_k Z_k = D;]`

Then, the sum the Z^2 has chisquared distribution with n-1 degrees of freedom.  Moreover, if there are additional m linear constraints, the sum of the Z^2 has chisquared distribution with n - m degrees of freedom.

It seems like the name ""degrees of freedom"" has been taken (or inspired) from linear algebra.  Having m linear constraints is the same as having an m by n matrix, which can have rank at most m (this having n-m degrees of freedom).

Is anyone able to point me in the direction of a formal proof of distribution of the sum of constrained squared standard normals?",Chisquare and Sum of Constrained Normals,9fuwkl,new,2,0,0,0
Im in an M.S in Applied math program with a concentration on Statistics. I can either take Monte Carlo Methods or Neural Networks. Which one is more applicable in this day and age ? I do plan on taking a machine learning class in my final semester( spring 2020).,Should I take Monte Carlo Methods or Neural Networks?,9fum9q,new,19,12,12,0
"I mean, once you enter public toilets do you follow your own rules like for example “I am not going to the first toilet because is the most used (because is the nearest yo the access door). I was wondering whether this only happens to me, because if not, the first toilet (the nearest to the access door) is the less used then. ","In public toilets, where there are for example more than 4 toilets, which ones are the most used?",9ftwzw,new,8,0,0,0
"So, I have a bit of a mundane question, but the issue is starting to do my head in. 

I require a calculator that can deal with stats for my college course (think binompdf and friends) that doesn't have the ability to save any information (formulae and whatnot).

I used to use a TI-84, but sadly using it in my upcoming exam isn't an option. I'd prefer a graphic one of the same brand. That being said, as long as the first two conditions are met, I couldn't care less about the rest. I just can't be bothered spending ages punching in numbers when I know for a fact it can be avoided. 

Apologies if this isn't the most appropriate sub, but I would really, really appreciate any help I can get on the matter at this point. ",Calculators?,9ft27v,new,5,0,0,0
" 

Hello everyone!

I want to model a high-volatility time series of a financial asset, and I have heard of ARCH/GARCH processes. By looking at the data (I don't have it yet, just a graph) I think that it might have unit root. So it raises the question... is stationarity needed for a series to be modeled as a GARCH?

In addition, do you know if there exist any implementation of a multivariate ARCH/GARCH you could show me?

Thank you in advance!",ARCH/GARCH and Stationarity,9fskq7,new,2,1,1,0
"Can anyone give me an example of how cumulative sum works? A sample with short series (like 1,2,3,4,5) would be awesome.

&#x200B;

&#x200B;

Thank you",Explanation with example of Cumulative Sum,9frmkt,new,7,1,1,0
"Let P(A|rain)=0.05, P(A|snow)=0.15, and P(A|sleet)=0.2
							
If the probabilities of rain, snow, and sleet on any given day are 0.3, 0.4, and 0.5. respectively, what is the probability of A (Auto accident)?						

I know the answer is 0.175 but I need to know how to get to that value. Can someone please guide me on how you solve this type of problem?",Statistics Question: Probability of Auto Accident,9fq89p,new,5,1,1,0
" Hi everyone, Just like the title I have a problem about panel data sets. 

Now I have two panel data sets which one has (2008-2011) 4 years and the other one has 4 years (2012-2015). But these are two seperate panel data sets. I want to pool (add each other) these two panel data sets and get a panel data sets between the years 2008-2015. Btw these data sets have 100.000 observation(id - N). Now If I do that this panel structure will be acceptable? I already tested ,seperately and combined. The results're very close to each other and significiant. I guess it's acceptable. I am not sure enough. ",A very interesting question about panel data,9fpocm,new,0,0,0,0
"Sorry  if  my question is  pretty basilar, I am  trying to learn about  interaction between the predictors, but still I don't  understand for what  an interaction is used.

For example, when  making a  multiple regression  when the  DEPENDENT variable is  gasoline  required by a car to make 100km in 1 hour,  and  the predictors are  the    **atmospherical weather** (bad condition marked as ''B''  and good atmospherical weahter  marked as ''G'') , **the state of the road** (  variable ranging  from 1 ''optimal  condition''  to 0 ''bad condition''  )  and the **speed  of the driver who is driving the car (in KM/h)**

If  I  would make  a linear  regression I would put it like this:

Y \~ **atmospherical weather + the state of the road + speed  of the driver who is driving the car**

&#x200B;

Now I  don't  know  the usefulness of  making a  regression  making an interaction between two of the predictors.

&#x200B;

Basically  I want to  know  the usefulness of interaction when I am making  regressions.

&#x200B;

If I would  have written (I am doing it with R software) :

&#x200B;

Y \~ **atmospherical weather  : the state of the road + speed  of the driver who is driving the car**

or

Y \~ **atmospherical weather \*  the state of the road + speed  of the driver who is driving the car**

&#x200B;

What  would have been  changed  from:

Y \~ **atmospherical weather + the state of the road + speed  of the driver who is driving the car**

&#x200B;

Which  is  the difference?  What  would have the first two lines of code  explained to me  that  the last one would'nt have explained to me?  What more?

&#x200B;

Thank you, I hope  I have made it clear  for you to understand, also  just feel free  to correct me if I am doing somenthing wrongin this regression analysis (maybe I am  missing somenthing, or I have mispelled  somenthing, or I have missed  somenthing  to put in the model above)

&#x200B;

Please  forgive me, I am still at the beginning

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;

&#x200B;",Interaction between two predictors,9fp2hi,new,10,16,16,0
"Let's suppose I'm having an admission test. It has 3 different subjects, and the final grade is the average of the 3 tests. I know for a fact that the average grade (for people that pass the test) for these three subjects are x,y and z. If I grade above average in all of the subjects, am I guaranteed to pass the test?

(Asking this because there may be some people that grade very well in 2 of the subjects, but zeroed the other (let's suppose they zeroed the Z subject). They passed the test because they were excellent on the other 2 subjects, but they are going to push z down, so it might not be enough for me to grade above average on the Z subject).",Doubt about smaller averages generating a bigger average,9foo5m,new,2,0,0,0
"Hey all! I just [wrote a quick post](https://guille.site/second-law-markov.html) which you may find interesting on the connections between Markovian processes and the second law (and similar inequalities).

Please feel free to ask any questions, I'd love to have a discussion on this notion :)",Markov processes and the second law,9foctz,new,0,2,2,0
"Searching online has been unsuccessful, anyone know where I can find it?",Where can I find a proof of the density function for a Weibull distribution?,9fnz8p,new,2,1,1,0
"Hi, I'm learning time series statistic right now. I realize Arima/ar/ma is only about one time series for one subject.

What if you have multiple time series? Is that longitudinal?

Thanks.",Multiple time series observation what is this called?,9fnami,new,3,1,1,0
"Hey guys, I'm trying to come up (Or copy) an excel formula that would help me predict the outcome (And if possible the probability) of a particular team winning and if possible, by how much. In my research I came across the Poisson Distribution. 

My question is: The Poisson Distribution would not be a good tool to use because the theory assumes ""the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and [independently](https://en.wikipedia.org/wiki/Statistical_independence) of the time since the last event."" which wouldn't apply in the NFL. Because although the ""event"" is either them winning or losing"" it doesn't take into account the other team's probability of winning or losing....right?",Question about Poisson Distribution,9fn31q,new,3,1,1,0
"Hey guys first post here, I wasn't sure if I should post here or in homework help.

I'm working on a medical research project involving cost analysis for patients coming to our hospital and have been burdened with the stats (of which I know little about, but I have been doing a good bit of research).

I really just want to verify that I did things correctly before presenting the project a small poster conference at my school.

&#x200B;

I have roughly 300 patients in my project and each one has an associated cost. The project tries to find differences in the workups of each of the patients and tries to identify any factors that cause the patients to cost significantly to be different on average. 

&#x200B;

For example I would group the costs into 2 columns, one for a patients with a normal ekg and the other for patients with an abnormal ekg. So there would be 160 patients with a normal ekg and and 140 with an abnormal.

&#x200B;

I used T.tests to see if there was any significant difference the excel formula was =T.Test(Array1,Array2,2,3) 2 for 2 tailed and 3 for unequal variances. It my understanding that this formula will put out the p value, and if my p value is less that .05 then it is considered a significant difference. The Array1 would be a column with 160 rows each being the cost of a patient with a normal ekg and Array2 would be a column with 140 rows each being the cost of a patient with an abnormal ekg.

&#x200B;

I used Chi square tests as well to confirm the data was not generated randomly, since abnormal ekg is categorical meaning its either a yes or no. But I'm not sure I need to include in my presentation the chi square results? Unfortunately one of my variables I found to have a significant difference in average cost from my T tests did not have a significant Chi square.

&#x200B;

I am also really interested in the variance of each dataset, like how the variance in price of a patient with an abnormal ekg differs from a patient with a normal ekg. Because it seems that if certain variables are true then they can have use effects on the variance of the cost. For example if the patient has diabetes then the variability(variance) of their total cost is 2x that of a patient without diabetes. I have researched that determining the statistical power from variance can be very difficult, can I just compare two variances within my datasets like the variability increases 2 or 3 times depending on these factors? Also does variance essentially = variability for my uses here?

&#x200B;

Can anyone please help validate that it looks like I am approaching this correctly? I'm really kind of flying blind here and the statistics department can't help me since I'm just a student.

&#x200B;

Disclaimer: most of the variables and amount of patients are made up since I don't know how much of the project I can share and want to err on the side of caution.",I'm a med student and I really need some quick help with basic statistics for a research project,9fmntx,new,16,14,14,0
"Hi everyone,

I am trying to create a personal project in which I calculate the probability that a NBA player scores a certain X number of points given a list containing the number of points that they have scored throughout the season. 

ex: 

10 games have passed in the 2018-19 NBA season and Stephen Curry has scored {23,22,39,30,22,28,27,23,20,30} points in each game. What is percentage chance that Curry will score 24 points in the next game?

&#x200B;

I've calculated the mean and std dev for the set and tried doing z-score/t-score calculation, but I don't think that is right. Can someone please point me in what direction to go to for this project?

&#x200B;

Thank you so much :)",Calculating probability of a value given a set of data,9fmg6m,new,3,4,4,0
"I recently stumbled across this probability puzzle: [https://blackswans.io/post/22/](https://blackswans.io/post/22/)

It seems really counter-intuitive to me, so I thought I'd share!

Enjoy.","Can Anybody Solve ""The Last Banana 🍌"" Probability Puzzle?",9flkin,new,20,10,10,0
"Is social media usage interval or ordinal data?

Variable: social media usage
Attributes: low user, average user, high user, extremely high user
Values: 5-9, 10-14, 15-9, 19 above",Interval or ordinal data?,9fktjt,new,12,0,0,0
"McNemar's test only takes into account data that changed categories from first measurement to the second, and nowhere does it take into account the total number of entries (not in the test statistic formula, not in degrees of freedom). Because of that, you can increase the number of entries in matched measurements by any amount, and end up with the same significance. 

That means that if e.g. 10 out of 10 people changed their views about something after listening to a speech from disagreeing to agreeing, McNemar's test considers the speech just as effective as if 10 out of 10000 changed them, which seems ridiculous to me. Or maybe I'm just misinterpreting which research question McNemar's test is answering?

Is there a test that takes into account the total number of entries? 

Edit: I've found [this](https://i.imgur.com/KQSN0k6.jpg) in my old college formulas. It says ""t-test for proportions for small paired samples (N<30)."" For the life of me, I can't find anything like it online. This formula is also unclear to me, as using it means there's no chance of t-value being higher than 1 in any situation. Maybe in this case the standard t-distribution tables aren't used, but ones adjusted for proportions (though I've never heard of such tables).",Alternative to McNemar's test for paired proportions?,9fjh3q,new,17,8,8,0
"So I have two variables, and I created a dataframe with descriptive statistics for them.  I wanted to output a table that could be included as a subplot with other plots like histograms and scatter plots.  

So far the only thing I've found that works is the gridExtra package, the table it outputs is nice but I can't get it to fit in as a subplot. It just prints the table on top of the plots already there. 

Any ideas of what I could do? I'd prefer not to install any additional packages if I don't need to. ",R help - how to output a dataframe as a table that can fit in with subplots?,9fjayr,new,3,0,0,0
"Hello all,

Wondering if you can shed some light on a problem I have...

I have a metric ton of data of individual points of activation. To be able to describe the pattern of activation, I've fitted each point against a function (double gaussian), the parameters of these fitted curves can be then used to describe the activation over time.

So far, so fine.

However, I am struggling to find a just way of accepting and rejecting fits. Either fitting with a constrained or unconstrained model, I will get function parameters that are not realistic (eg. they indicate activity before I even started measuring), these are very easy to identify as being bad fits for the data, however I am worried how I should reject bad fits that give plausible results. Including these would be to say certain nodes are contributing to the activity, but may just be coincidence.

I have goodness of fit measurements, but I don't know what acceptable, empirical limits I can put on these. I'm under the impression that high/low RMSE doesn't directly imply the accuracy of the fit, so I was wondering if anyone here can lend a hand, because otherwise if I am overly strict on inclusion criteria, I will likely bias my results in my favour.

Thank you kindly!",Assessing good model fit?,9fijgn,new,2,0,0,0
"Originally I was trying to understand this model: 

`Attitudes ~ Site + Household Size + Gender + Educational Category`

but after running the model and checking the nominal\_test, it failed, with gender being significant. As a result, I ran a partial proportional odds model with gender as a nominal effect:

`Attitudes ~ Site + Household Size + Educational Category, Nominal = ~ Gender` 

The problem is that gender is quite important for my hypothesis, and I can't seem to infer what the direction and strength of gender's effect is. Is that possible in this model? Also, how do I generally interpret this model? I've noticed the first level in SITE, KT, is missing. For reference, please see the output:

&#x200B;

 `link  threshold nobs logLik  AIC    niter max.grad cond.H` 

 `logit flexible  227  -196.80 415.60 7(0)  2.29e-12 2.3e+03`

&#x200B;

`Coefficients:`

`Estimate Std. Error z value Pr(>|z|)`  

`SITEKT                  -0.93516    0.51111  -1.830   0.0673 .`

`SITEMA                  -0.70317    0.48145  -1.461   0.1441`  

`SITERW                  -0.18236    0.51175  -0.356   0.7216`  

`SITEYK                  -0.35549    0.49522  -0.718   0.4728`  

`SITEYP                   0.99321    0.48974   2.028   0.0426 *`

`HSIZE                    0.07139    0.05307   1.345   0.1786`  

`EDU_CATSecondary/Higher  0.33787    0.31240   1.082   0.2795`  

`---`

`Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1`

&#x200B;

`Threshold coefficients:`

`Estimate Std. Error z value`

`Neg|Neu.(Intercept)   1.2335     0.5079   2.429`

`Neu|Pos.(Intercept)   1.9837     0.5371   3.694`

`Neg|Neu.GENM         -0.8947     0.3198  -2.798`

`Neu|Pos.GENM         -0.1441     0.3920  -0.368`",[Help needed] Interpreting clm (ordinal regression) model with nominal effect in r. Possible to understand the effect direction and strength of the nominal effect?,9fgacd,new,0,2,2,0
"Here is another topic I was confronted with the other day. I would love to hear your opinions.

\---------------------------------------------------------

Researchers weighed brain weights and noted the weight in grams.

Then, they estimated brain weight based on Computed-Tomography Imaging.

&#x200B;

Hypothesis is: The brain weight estimated by the radiological method is accurate.

&#x200B;

They measured both of these weights in 25 cases.

Then, they apply a \[paired t-test\], which turned out to be non-significant \[p = 0.53\].

It was also said, that the t-test had ONE degree of freedom (this must be false, correct?)

&#x200B;

They state that brain weight estimation works well, as the \[t-test\] does not reveal significant differences between the two measured brain weights.

\-----------------------------------------------------------

i had to give my opinion about this. I thought:

Does a paired t-test really make sense here? Would it not be more accurate to use simple linear regression for this study? The correlation coefficient should give us a good indication on whther this method works?

I also thought it would make sense to look at the residuals and evaluate them.

&#x200B;

Is tihs correct? Or is their method actually good?

Again, all comments are highly appreciated.",Comparison of brain weights - Paired t-test or Simple Linear Regression?,9ffof5,new,11,13,13,0
"Hi,  I  know  that  Convolutional  networks are used  for  image recognization, but I was wondering if  they could also be used  in the  stock market as  pattern recognization. At the end  they  have hidden  layers  which  are made by predictors,  and  one output (Y)   which could outcome the probability of  a  pattern  to  realize it.  Maybe  it's a  difficult approach  to stock's market  pattern  rcognization, and  time series  suit  a lot  better,  I was  just  wondering if  CNN  can be used to forecast  price action of a stock, and how

&#x200B;",Convolutional networks,9fexo0,new,4,0,0,0
"Is there a technical term for what we're doing below? Seems to me it's just the aggregate of mean scores.

""Each team will submit a single solution to be scored. The submitted solutions will be evaluated against n number of test scenarios to test the versatility of the solutions. Each test scenario will, in turn, be run m number times to account for sampling error. The composite score of the *n x m* runs is given by:
[this equation](https://imgur.com/IlRj6YP)
",Technical term for this?,9fem32,new,4,0,0,0
"It is often said that *The Problem of Points* inspired the creation of Probability Theory.

See if you can solve it: [blackswans.io/post/20/](https://blackswans.io/post/20/).",Can You Solve the Puzzle Which Inspired Probability Theory? 😲,9fcf1u,new,4,0,0,0
"I've got 30 binary inputs, and each individual is a ""yes"" for between 1 and 15 of them. I want to see how these inputs relate to the continuous output (cost). For example it would be nice to see things like, which input combinations are common, or which input combinations have a high cost. Any ideas for relevant methods I could google and learn more about?",How can I analyze the relationship between a bunch of binary variables and a single continuous variable (cost)?,9fbsbh,new,9,10,10,0
"My question is in the game show Let’s make a deal, does anyone have the statistics of what door the deal of the day is behind the most? Thanks!",Let’s make a deal statistics (not Monty Hall Problem),9fajry,new,3,0,0,0
"seems like i did some things in my thesis where my statistics classes did not prepare me for. Maybe this discussion will help me gain some confidence into my results.    

So i did an experiment that has a within subject design. Subjects had to go through an experiment with two different conditions (lets call them A & B). within those two conditions i measured 10 'micro'-conditions for 4 times each. producing data of 4*10 for each condition A & B    

***
Now what i'am interested in are the following questions:    
* Is there any difference between A:1-A:10  - obviously same question for B    
* I have contrasts from a paper my work is based on i have to check -> is there an model describing the dependent variable based on the variables A:1 - A:10    
* Is there any difference between (A:1-A:10)-(B:1-B:10)    
***

I kinda have the feeling that a lot of methods i could use are based on between-subject design (especially profile analysis that would do the job already (as the 'micro'-conditions don't seems to have any effect anyway) or repeated measures in the same condition or single measurements in multiple conditions. So i am unsure if i should either use the mean of the subject for each 'micro' condition, solving the problem for me but loosing some variance or treat every repeated measure as a different ""subject"" (aka row in my table) where i am thinking that it would not be the right approach.

So would love to hear your thoughts guys'n gals. Especially what methods you would use.",Repeated measurement within-subject analysis with 2x10 conditions,9f98kf,new,0,2,2,0
"let's say we have a treatment and a control group, and the participants answer several true / false questions. Afterwards, since all assumptions are met, we perform student's t-test to compare if the differences between the correct answers are significantly different.

The problem with true / false questions, each participant has the 50% probability to answer the question correctly on random. Now what I'm wondering is - how ""strong"" are scientific methods, such as t-test, to detect such random answering? Generally, I think, the significant results would suggest that the difference is not due to random, but does the type of questions affect this?",True - false questions and the credibility of the results,9f8fb0,new,11,3,3,0
"Dear community,

Im working in forensic medicine but im still relatively fresh. Today I listened to a PHD defense.

To make it short. They had a group of 16 ppl taking a drug, and 16 ppl who took placebo. Then they looked at concentrations of a LOT (like 30) of metabolites, and whether these would change. Than they checked (t-test) , whether these metabolite concentrations differed between the two groups.

&#x200B;

I wondered: Is the constellation of low number of cases together with looking at 30 metabolites not a recipe for P-Hacking? Are they not provoking false positive results with this study design?

&#x200B;

Is there a way to do this study in a reasonable way? Would this be better with be a Bonferroni correction? Which p-values can be accepted then? Which other means could legitimize this study design?

&#x200B;

Help and advise are much appreciated. Im still building up my understanding of statistics.

&#x200B;

All the best,

konehta

&#x200B;

&#x200B;

EDIT:

Thanks for all your contributions, I learned a lot. And the discussions were very helpful.",Is this P-Hacking?,9f7zqr,new,56,33,33,0
"I took an intro to statistics course last year, and a friend who is writing her thesis has just asked me to help her figure out what test to use, but most of what I learned has fled my brain.

She has two groups of children (typically developing and language delay) — this, I believe, is the independent variable (or is it 2 independent variables?).  Each group did 3 tests. (The dependent variables)

She wants to know the differences between the two groups (ie: is there a significant difference), _and_ is there is a significant difference between the results of the 3 tests.

I feel like it is a MANOVA she needs, followed by a post hoc analysis, but I barely remember anything from that class, and we only covered MANOVA in one class, and I have no idea what the post hoc analysis should be... any advice? ",Trying to figure out which statistical test to use... help!,9f7z4z,new,9,0,0,0
"My background is mostly in qualitative and procedural legal analysis, but some of the questions I'm looking at now need some quant. Any suggestions appreciated! ",Are there any online courses for statistics in Empirical Legal Studies?,9f6feh,new,0,6,6,0
[https://uic.edu.hk/en/home/news/6447-biostatistics-gilead-associate-director-on-machine-learning-methods](https://uic.edu.hk/en/home/news/6447-biostatistics-gilead-associate-director-on-machine-learning-methods),Biostatistics Gilead Associate Director on Machine Learning Methods,9f645i,new,0,1,1,0
"Hi there, 

&#x200B;

Very novice in SPSS and statistics and I have a question please help. I have run two one-way anovas. One is running Location (Melbourne, Queenstown and Mumbai)/Creativity and the other is running (Manager, Factory workers, and Customer Care)/Creativity. 

&#x200B;

both results come up with statistically significant results. The post hoc for the first one reveals Melbourne has the highest mean (with the other two grouped together). The post hoc for the second one reveals Managers have the highest mean (with factory workers and customer care grouped together). The sig for both tests are p>0.05. 

&#x200B;

Do i need to correct the sig level to reduce type one error (family wise error rate)? as in do something like add their p values together then divide by two (because there are two tests). I'm confused because their p-values are really low anyway. Or, do you only do that for a t-test?

&#x200B;

Thanks from a lost statistic student",Type 1 error rate two 1-way anovas,9f5rky,new,0,1,1,0
"Hi,

&#x200B;

I want to build a classifier for 2 classes, Normal (17 samples) vs. Disease (19 samples). And with this small dataset, I divided it to 30 samples for training, 6 samples for test, using createDataPartition() of Caret package in R. After training the model with 10-fold cv using Random Forest, I got a final accuracy value of 1 (classified 6 test samples correctly), but the confidence interval of the accuracy was broad (0.5407 - 1). Does this mean the size of test samples is too small?

&#x200B;

Thanks",Broad confidence interval,9f57y7,new,5,1,1,0
"SOrry for the noob  question.   Before  to   making a linear / polinomail  regression  should  I  know  the distribution of the dependent variable (Y)?  Because  based on the  distribution  if the Y  then  I  decide which  kind   of  regression would be   better?  Sorry if  I  have said somenthing wrong, I am starting out, help  me to understand.

Thank you.

&#x200B;

2)  Question:   Let' s   assume  we have a  dataset' scatterplot

[https://math.illinoisstate.edu/day/courses/old/312/session1102.gif](https://math.illinoisstate.edu/day/courses/old/312/session1102.gif)

&#x200B;

each point  of the dataset (which is a  singular data) has a  variance of  ''0''   and  a  mean  of ''1'' ?   Why  ''1''?",Distribution of the dependent variable Y,9f4ibu,new,12,0,0,0
"Say I have a population size of 100 fruits. I take a sample of 30 from this population, and out of those 30 fruits 13 are bad. I setup the following to estimate how many are bad in my population

&#x200B;

Sample Mean: 0.4333..

St. Deviation: 0.50401

St. Error: 0.50401/sqrt(30) = 0.092018655

Lower Limit of Confidence Interval (90% Confidence): 0.43333 - (1.645 \* 0.092018655)   = 0.281959313

Estimated Number of Bad Fruit in Population: 0.281959313 \* 100 = 28

&#x200B;

Is this method a reasonable approach to answering this problem?

&#x200B;",Estimating number of bad fruits in known population,9f408y,new,23,9,9,0
"Hi Stats! 

I have tried searching on Google but I can’t find exactly what I’m looking for. This may not really be a statistics question but I thought you guys would probably know where to look.

I’m looking for a program or method to easily calculate the value of variables to create a grade or rank for a list of items. [Example of what I'm looking to do here](https://i.imgur.com/UqJT2N1.png). I want to easily input data into a table like that and be able to rank each “item” (or, “job title” in the table linked). The table will probably be 40 items with 6 “variables” (entry pay, career pay, stress level, etc in table).

This is *NOT* homework help. I am doing this for an assignment but it is not a requirement. I have been curious about this for a while now and I am just finding a way to make learning about it useful. My assignment is to take a personality quiz and the quiz will generate job titles ranked from 1 – 40. The next part is for me to choose 5 I like the most and write a summary of them. I am really interested in 11 so I want to re-rank everything myself. 

I know this is extra work for nothing but I would like to learn this anyways.

Any help appreciated!!
",Is this even a statistics question??,9f3orz,new,11,3,3,0
"Hey y'all,


I took a statistics course a few years ago so I'm a little shaky. I have access to a large database at work and am trying to analyze it to predict some future behavior. I spent some time looking for trends and feel like I found some solid ones. I also think the math I put together checks out but was struggling to explain it to folks at work so I'm now questioning myself.


Essentially, I'm a hiring manager. Here's an example of what I'm doing. Obviously I've got much larger numbers that span a few years, but just want to make sure this line of thinking is right. Could you let me know if I'm on the right track?


Last year, 100 people started applications to work with us.  
Of those 100, 60 ended up submitting applications.  
25 started their application and submitted it on the same day.  
20 started their application, worked on it for 2 days, then submitted  
10 started their application, worked on it for 4 days, then submitted.  
5 started their application, spent 10 days on it, then submitted.  


With this data, I'd predict that:  
If an application was started today, there's a 60% chance they'll submit. (60/100)  
If an application has been open for 1-2 days, there's a 46.67% chance they'll submit. (60-25)/(100-25)  
If an application has been open for 3-4 days, there's a 27.27% chance they'll submit. (35-20)/(75-20)  
If there's an application open longer than 4 days, there's an 11.11% chance they'll submit. (15-10)/(55-10)  


Again, I've got multiple years of data and a few thousand data points per year so I feel like my data's large enough to safely predict this. Vaguely remembering confidence percentages now too, does that come into play here?",Running numbers on historical data to predict behavior,9f2o49,new,0,1,1,0
"I'm having trouble fully grasping what was done in a class that I'm taking and why.

The original transform is as follows:

Output1 = LN(Daily\_Asset\_Price\_Level) / StDev\_Of\_LN\_Of\_Daily\_Asset\_Price\_Level

That much is fine, we now have a natural log of price levels which is scaled to the standard deviation. So, the movement has been converted into ""standard deviation points of log points"" as a unit (I think) \[possibly AKA ""Z-Score of Centinepers""\].

The same setup is repeated for Asset\_2.

Now we solve for the correlation and covariance of the two assets.

COV(Output1, Output2) = X

COR(Output1, Output2) = X

OR: COV=COR

I think what is happening is that we have scaled both the direction and magnitude by each asset's standard deviation. The result is that COV = COR.

With regard to the direction, e.g. CORRELATION, it seems to have no impact because it is direction without regard to magnitude.

I believe the COVARIANCE is ""wrong"" because we have altered the magnitude with different scalar values.

Can anyone explain what is happening here? What, if any, are the implications?

Also: No, this discussion does not impact my grade. I already submitted the project and it's completed. However, I think the project itself was incorrect in asking for the above equations. Also, I have obfuscated the actual terms for academic integrity concerns, e.g. I'm not posting any internal data or exam questions online.",Strange Transformation: Correlation and Covariance,9f1xyx,new,10,1,1,0
"Hi all,

I am graduating with a master's degree in Applied statistics and biostatistics this year. 

My question is: What kind of jobs should I look for? I have little experience as I only had one data analyst internship.

All data analyst positions require experience and other stuff. I know SAS and R",Jobs for new grads,9f0ojk,new,8,2,2,0
"Hey all, 

I am learning statistics and have a bunch of time series data that I am hoping to draw some conclusions from. I have one high resolution time series data set acting as the dependent variable, and I have 4 different parameters/variables (each also a high resolution time series data set) acting as the independent variables. I am hoping to show which if these 4 (independent variables) show important/significant associations or that are triggering the dependent variable. Further, also considering these IV may be confounding — so looking to show significant associations, but also ranking them, or looking at them as potentially nested data.

I am not that experienced in statistics — is it possible to use the time series data in a model (MODWT, ARMA, ARIMA, or something like this) and get an Rsquared or Pvalue from it, or do time series associations not function like that and I am thinking about it wrong? Does anyone have any advice for the best way to show significant relationships with data sets like this? Thanks in advance.",Help with/understanding correlations with time series data,9f09it,new,6,5,5,0
"I am doing a research proposal entitled ""Correlational analysis between social media usage and self-esteem"" 

I've read similar studies and they used Pearson correlation two-tailed but I can't seem to understand why exactly that kind of correlation was used. 

Can someone please explain to me why Pearson correlation will be used? And if that's correct?",Why Pearson Correlation?,9ezlpj,new,5,1,1,0
"Working on a data viz project to highlight difference in animal populations in areas that allow trophy hunting vs. places that don't. Or related topic for animal conservation (e.g. endangered species' population). Found a lot of visualizations but haven't had luck getting raw data thus far

Would appreciate an assist with any raw data sets y'all can recommend!Working on a data viz project to highlight difference in animal populations in areas that allow trophy hunting vs. places that don't. Or related topic for animal conservation (e.g. endangered species' population). Found a lot of visualizations but haven't had luck getting raw data thus far

Would appreciate an assist with any raw data sets y'all can recommend!",Looking for public datasets in poaching/trophy hunting/ivory trade,9ez8yt,new,2,1,1,0
"Hi all,

I'm currently taking my first statistics class with a particularly poor professor, so I'm doing most of my learning from the text book and from searches of youtube and other sources. Does anyone know of any strong comprehensive courses online for this subject matter?

A little background: I have a strong math background otherwise (I have a Physics degree - never had to take stats! I know, I know), and I'm working out of the Probability and Statistics for the Engineering and Sciences by Jay Devore.

Any help would be very appreciated.",Good online lectures for Engineering Statistics?,9eyh7w,new,3,13,13,0
"Hi,

The Royal Statistical Society are [looking for nominations](http://www.rss.org.uk/RSS/Get_involved/Statistic_of_the_year/RSS/Get_involved/Statistic_of_the_Year_.aspx?hkey=e5008987-fab9-4385-9110-4287e487b8d6) for the Statistic of the Year, and I thought it would be interesting to see whether people on r/statistics had any thoughts!  

I posted this on my statistics community earlier, and there are already [some great suggestions](https://blackswans.io/post/18/).",What is your Statistic of the Year?,9exwbu,new,9,20,20,0
"hello:) basic question,but  my textbook does not explain this: could anyone explain in an intuitive way Why the  "" + 1"" in the equation below is, / why it is necessary? Thank you!

Pth percentile= Value located in the ( p/100)(n +1 )th position",question on percentiles calculation,9ewlte,new,1,0,0,0
"Hi,
I'm doing an test related to click-rate on ads. For my test I have 2 equal size groups, with n=10,000.

Here's where it gets a bit interesting for me: I cannot promise all the users from the each group will be receiving the same amount of ads (or impressions), so when I look at the non distinct impressions per group after 1 week, I have that group A did 2,500 imps and group B did 5,000 imps.
For clicks, I have that A did 800 clicks and B did 1500 clicks.

I'd like to know if there's a statistically significant increase or decrease in CTR from any.

What I did up until now was a two-sided z-test (a=.95), where I get a very small p-value. Here are the things I'm concerned about:

1) I believe with unequal sample sizes I should be using another test or some correction and;

2) I also feel my p-value is low at some part because my n is relatively high. I was thinking on doing power test and solve for alpha to have a better estimate on which alpha I should use?

What do you guys think? Am I over-complicating this and my hunch feeling is wrong?

Thank you.",Help with proportion test with unequal sample size,9ev0th,new,15,3,3,0
"If all the t Tables in the world vanished, how would you recreate it?",t Tables,9euefd,new,5,0,0,0
"Hello everyone.

I'm currently working on a study where calculation of positive predictive values (PPV) and negative predictive values (NPV) for multi-stage viral infection is required.

The study is divided into 4 stages; 1 (healthy), 2 (mild), 3 (moderate) and 4 (severe).

Using SPSS or MedCalc, how can I calculate PPV and NPV for each 2 separate categories?",How to calculate Positive & Negative predictive values for multiple patient categories?,9eu5yj,new,0,3,3,0
"Hi. I'm having a problem defining the structure of an economic growth model based on institutions.  


I can't go into details about the literature behind this, but to be concise:

There are two concepts that explain long-run growth and development. Let's say concepts A and B. These are not mensurables. 

Each concept, A and B, can be explained by a set of variables, let's call them X and Y. X ""measures"" concept A, and Y ""measures"" concept B.  
X and Y are set of variables, most of them from the databank of World Bank. In total, we have around 24 variables, 217 countries. We have data for years 1996:2015, but decided to use an average of the years 1997:2000 because it's the range where most variables are available simultaneously.

We thought about using PLS path modeling for estimation. A and B would be Latent Variables (LV) and X and Y Manifest Variables (MV).

We used principal component analysis (PCA) and found out from the Factor Loadings that most of the variables are strongly correlated to the 1st Factor, while the 2nd not so much. Eigenvalues: F1 = 8.479; F2 = 1.644. 

To solve this, we decided to be use a Higher Order Model PLS path modeling. So instead of having just one layer of Latent Variables, we would create a new one. See, inside of each set of variables, X and Y, we can create new ""factors"" or ""concepts"" that can be used to explain the higher order Latent Variable. One example is the following: variables *Research and development expenditure (% of GDP)* and *Average years of schooling,* both Manifest Variables are used to explain the Factor/Concept ""Human Capital"", which is a Latent Variable. This LV Human Capital then is used to explain the Latent Variable ""A"".

  
Any tips or considerations? Is this possible? I don't really know what I'm going to use for the estimation. I see that there's a R package called SEMinR and it may be useful. Is this the best model?",Help defining statistical method for Economics research,9esvsn,new,3,9,9,0
"How I built my Own Code Repository for Rstats. Very useful in every new project.

[https://medium.com/@obedm/how-to-build-your-own-code-repository-with-an-intern-search-engine-86392df916be](https://medium.com/@obedm/how-to-build-your-own-code-repository-with-an-intern-search-engine-86392df916be)",How To Build Your Own Code Repository With an Intern Search Engine,9erqwo,new,2,9,9,0
After running a LDA (DFA) the loadings seem very large (both negative and positive). The range for these are between -500 and 500. My data plotted on the two first discriminant functions dose not exceed -3 and 3. What would cause this huge discrepancy?,LDA loadings/scaling are extremely large,9epy57,new,0,1,1,0
"Where to find data on iq as it relates to demographic data like income, education, race, etc. I'm just making a model for a statistics class and was looking for where to find these data sets. Not for anything controversial.",IQ and Demographic Data,9eprf8,new,18,5,5,0
"I'd like to decompose a monthly time series about the sales of a product into its three components (trend, seasonal and remainder). This product is being sold in k different territories, so I will decompose k different time series. When I look to the overall time series, I can see that it has positive values for every month. But when I look to the time series of each territory, I can see that there are some zero values for some months, i.e. for some months zero units of the product were sold.

Can I decompose a time series even if it has some zero values? Does the presence of the zero values affect the decomposition? Does it matter if the time series follow an additive or a multiplicative model? Should I add a constant to the time series (like 100), perform the decomposition on the transformed data and then back-transform it on the original scale? Should I decompose the quarterly data instead of the monthly data in order to make all values positive?

Thank you in advance.

&#x200B;",Time series decomposition with zero values,9epf5k,new,1,1,1,0
"Hello, I am a recent graduate with a Bachelor in Science in Health Sciences. I am currently working in clinical research and eventually want to work in the pharmaceutical industry. My Alma mater (Northeastern University) offers an MS in Bioinformatics but I have also heard that an MS in Biostats is more marketable. Can anyone shed some light in this comparison?

Question part 2: If I do a part time degree, will that reflect poorly on me to my employers? What about an online degree?

Thanks for all your help! =\]",MS in Bioinformatics or Biostatistics (Pharma),9eoger,new,16,14,14,0
"Assume we have a slightly skewed coin that's in favor of Heads over Tails.  


How would one determine the probability of each side?   


Possible algebraic manipulation of the bi-nominal probability theorem.

   


[https://www.fourmilab.ch/rpkp/experiments/statistics.html](https://www.fourmilab.ch/rpkp/experiments/statistics.html)",Simulation patterns (coin toss),9eo3te,new,1,0,0,0
"I am currently estimating a seemingly unrelated regression to test the effect of IV1 on DV1 and DV2 (errors for DV1 and DV2 are theoretically correlated), following the procedures outlined here:  

[https://stats.idre.ucla.edu/r/faq/how-can-i-perform-seemingly-unrelated-regression-in-r/](https://stats.idre.ucla.edu/r/faq/how-can-i-perform-seemingly-unrelated-regression-in-r/) 

After I run the regression, I use the Wald test to see if IV1 in model1 is equal to IV1 in model2. I only do this because the help page shows me that it is possible. In my stats classes we never learned SUR models so I guess my first question is, does it actually make sense to use the Wald test to compare the size of IV1 effect in model1 and model2? It seems weird to me that this is possible but again I don't have much experience with simultaneous models.   


My second question is does it matter that DV1 and DV2 have different scales?  I can reject the null that the two effect sizes are the same, but my scales are not the same. Should I rescale my variables or does the Wald test take that into consideration? Thanks! ",Question: Wald test and seemingly unrelated regression,9eo04e,new,2,4,4,0
"Hello,   for what is  useful  to find  centroid of a  cluster of  data  when your  analizying a dataset?

&#x200B;

I find those  examples:

[https://gis.stackexchange.com/questions/6025/find-the-centroid-of-a-cluster-of-points](https://gis.stackexchange.com/questions/6025/find-the-centroid-of-a-cluster-of-points)

 

While searching the web, solutions for finding centroids of polygons  come up rather often. What I'm interested in is finding a centroid of a  cluster of points. A weighted mean of sorts. I would appreciate it if  someone could provide some pointers, pseudo code (or even better, an R  package that has already solved this) or links of how this issue can be  tackled.

*EDIT*

Convergence has been afoot (again). iant has suggested a method to  average coordinates and use that for the centroid. This is exactly what  crossed my mind when I saw the right picture on [this web page](http://number-none.com/product/My%20Friend,%20the%20Covariance%20Body/index.html).

Here is some simple R code to draw the following figure that demonstrates this (× is the centroid):

    xcor <- rchisq(10, 3, 2) ycor <- runif(10, min = 1, max = 100) mx <- mean(xcor) my <- mean(ycor)  plot(xcor, ycor, pch = 1) points(mx, my, pch = 3) 

📷

*EDIT 2*

cluster::pam()$medoids  
 returns a medoid of a set of cluster. This is an example shamelessly stolen from @Joris Meys:

    library(cluster) df <- data.frame(X = rnorm(100, 0), Y = rpois(100, 2)) plot(df$X, df$Y) points(pam(df, 1)$medoids, pch = 16, col = ""red"") 
    

&#x200B;",For what is it useful to find a centroid of a cluster of points?,9el5i3,new,2,0,0,0
"Hey there!

Can you guys recommend any good source of mobile device usage statistic by age and country in EU? It would be appreciated if it didn't include a subscription or a payment fee.

Thanks a bunch!",European Mobile Device Usage Statistics by Age and Country (2016-2018),9el4hc,new,0,1,1,0
"Hi guys, this is my senior year in a Statistics program and I am planning to start the MS Applied Stats next year at my current institution. I know that most Stats students are trying to get into Machine Learning. Personally, I think the problems in ML are interesting, and I should definitely pick up some ML skills. However, with my exposure to Statistics so far, I think I am more into the ""inference"" side of its. I guess I like a more-traditional-statistician role. I wonder if I could achieve that goal in the setting of universities or research labs. I am interested in Biostats, and so far, I have seen job postings for statistician from Stanford University, UC San Francisco, etc. If you have some perspectives of working at universities or research labs as a statistician, please tell me something, like: day-to-day work, tools used, examples of problems, satisfaction, job prospect, working environment etc. And most importantly, what can I do to get a job like you? I am trying to get more exposures to different areas in Statistics (like Functional Data Analysis, Statistical Bioinformatics) through research with professors. Perhaps, that is a good start.

&#x200B;

Please help me if you can. Thank you so much. Any input is valued","Statisticians working at universities or research labs, what can I do to get a job like yours?",9ekc1y,new,15,10,10,0
